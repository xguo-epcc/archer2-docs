{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ARCHER2 User Documentation Warning The ARCHER2 Service is not yet available. This documentation is in development. ARCHER2 is due to commence operation in 2020, replacing the current service ARCHER. For more information on ARCHER, please visit the ARCHER web site . ARCHER2 is the next generation UK National Supercomputing Service. You can find more information on the service and the research it supports on the ARCHER2 website . The ARCHER2 Service is a world class advanced computing resource for UK researchers. ARCHER2 is provided by UKRI , EPCC , Cray (an HPE company) and the University of Edinburgh . What the documentation covers This is the documentation for the ARCHER2 service and includes: Quick Start Guide The ARCHER2 quick start guide provides the minimum information for new users or users transferring from ARCHER. ARCHER2 User and Best Practice Guide Covers all aspects of use of the ARCHER2 supercomputing service. This includes fundamentals (required by all users to use the system effectively), best practice for getting the most out of ARCHER2, and more advanced technical topics. Research Software Information on each of the centrally-installed research software packages. Software Libraries Information on the centrally-installed software libraries. Most libraries work as expected so no additional notes are required however a small number require specific documentation Data Analysis and Tools Information on data analysis tools and other useful utilities. Essential Skills This section provides information and links on essential skills required to use ARCHER2 efficiently: e.g. using Linux command line, accessing help and documentation. Contributing to the documentation The source for this documentation is publicly available in the ARCHER2 documentation Github repository so that anyone can contribute to improve the documentation for the service. Contributions can be in the form of improvements or addtions to the content and/or addtion of Issues providing suggestions for how it can be improved. Full details of how to contribute can be found in the README.md file of the repository. Credits This documentation draws on the Cirrus Tier-2 HPC Documentation , Sheffield Iceberg Documentation and the ARCHER National Supercomputing Service Documentation .","title":"Documentation overview"},{"location":"#archer2-user-documentation","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development. ARCHER2 is due to commence operation in 2020, replacing the current service ARCHER. For more information on ARCHER, please visit the ARCHER web site . ARCHER2 is the next generation UK National Supercomputing Service. You can find more information on the service and the research it supports on the ARCHER2 website . The ARCHER2 Service is a world class advanced computing resource for UK researchers. ARCHER2 is provided by UKRI , EPCC , Cray (an HPE company) and the University of Edinburgh .","title":"ARCHER2 User Documentation"},{"location":"#what-the-documentation-covers","text":"This is the documentation for the ARCHER2 service and includes: Quick Start Guide The ARCHER2 quick start guide provides the minimum information for new users or users transferring from ARCHER. ARCHER2 User and Best Practice Guide Covers all aspects of use of the ARCHER2 supercomputing service. This includes fundamentals (required by all users to use the system effectively), best practice for getting the most out of ARCHER2, and more advanced technical topics. Research Software Information on each of the centrally-installed research software packages. Software Libraries Information on the centrally-installed software libraries. Most libraries work as expected so no additional notes are required however a small number require specific documentation Data Analysis and Tools Information on data analysis tools and other useful utilities. Essential Skills This section provides information and links on essential skills required to use ARCHER2 efficiently: e.g. using Linux command line, accessing help and documentation.","title":"What the documentation covers"},{"location":"#contributing-to-the-documentation","text":"The source for this documentation is publicly available in the ARCHER2 documentation Github repository so that anyone can contribute to improve the documentation for the service. Contributions can be in the form of improvements or addtions to the content and/or addtion of Issues providing suggestions for how it can be improved. Full details of how to contribute can be found in the README.md file of the repository.","title":"Contributing to the documentation"},{"location":"#credits","text":"This documentation draws on the Cirrus Tier-2 HPC Documentation , Sheffield Iceberg Documentation and the ARCHER National Supercomputing Service Documentation .","title":"Credits"},{"location":"data-tools/","text":"Data Analysis and Tools This section provides information on each of the centrally-installed data analysis tools: versions available, how to get access, good practice for getting best performance, links to associated training and webinars, links to associated technical reports (eCSE final reports, white papers), links to instruction manuals and further information. Tip You may also find the Data analysis section of the User and Best Practice Guide useful. Note Links to the different data analysis tools will be added as soon as they are available.","title":"Data Analysis and Tools"},{"location":"data-tools/#data-analysis-and-tools","text":"This section provides information on each of the centrally-installed data analysis tools: versions available, how to get access, good practice for getting best performance, links to associated training and webinars, links to associated technical reports (eCSE final reports, white papers), links to instruction manuals and further information. Tip You may also find the Data analysis section of the User and Best Practice Guide useful. Note Links to the different data analysis tools will be added as soon as they are available.","title":"Data Analysis and Tools"},{"location":"essentials/","text":"Essential Skills This section provides information and links on essential skills required to use ARCHER2 efficiently: e.g. using Linux command line, accessing help and documentation. Note This section of the documentation will be populated soon. Tip You may also find the Essential Skills section of the ARCHER website useful.","title":"Essential Skills"},{"location":"essentials/#essential-skills","text":"This section provides information and links on essential skills required to use ARCHER2 efficiently: e.g. using Linux command line, accessing help and documentation. Note This section of the documentation will be populated soon. Tip You may also find the Essential Skills section of the ARCHER website useful.","title":"Essential Skills"},{"location":"quick-start/overview/","text":"Quickstart The ARCHER2 quickstart guides provide the minimum information for new users or users transferring from ARCHER. There are two sections available which are meant to be followed in sequence. Quickstart for users : Covers the basics of ARCHER2 useful for all users, including: applying for an account, logging in and transferring data, accessing software and running jobs. Quickstart for developers : Covers additional topics around compiling your own programs on ARCHER2, debugging and profiling. If you are not going to be compiling your own programs on ARCHER2, you do not need to follow this guide.","title":"Overview"},{"location":"quick-start/overview/#quickstart","text":"The ARCHER2 quickstart guides provide the minimum information for new users or users transferring from ARCHER. There are two sections available which are meant to be followed in sequence. Quickstart for users : Covers the basics of ARCHER2 useful for all users, including: applying for an account, logging in and transferring data, accessing software and running jobs. Quickstart for developers : Covers additional topics around compiling your own programs on ARCHER2, debugging and profiling. If you are not going to be compiling your own programs on ARCHER2, you do not need to follow this guide.","title":"Quickstart"},{"location":"quick-start/quickstart-developers/","text":"Quickstart for developers Warning The ARCHER2 Service is not yet available. This documentation is in development. This guide aims to quickly enable developers to work on ARCHER2. It assumes that you are familiar with the material in the Quickstart for users section. Compiler wrappers When compiling code on ARCHER2, you should make use of the Cray compiler wrappers. These ensure that the correct libraries and headers (for example, MPI or Cray LibSci) will be used during the compilation and linking stages. These wrappers should be accessed by providing the following compiler names: Language Wrapper name C cc C++ CC Fortran ftn This means that you should use the wrapper names whether on the command line, in build scripts, or in configure options. It could be helpful to set some or all of the following environment variables before running a build to ensure that the build tool is aware of the wrappers: export CC=cc export CXX=CC export FC=ftn export F77=ftn export F90=ftn man pages are available for each wrapper. You can also see the full set of compiler and linker options being used by passing the -craype-verbose option to the wrapper when using it. Programming environments On login to ARCHER2, the PrgEnv-cray collection will be loaded, as will a cce module. The latter makes available Cray's compilers from the Cray Compiling Environment (CCE), while the former provides the correct wrappers and support to use them. The GNU Compiler Collection (GCC) is also available. To make use of any Programming Environment, restore the correct PrgEnv collection. After doing so the compiler wrappers ( cc , CC and ftn ) will correctly call the compilers from the new suite. The default version of the corresponding compiler suite will also be loaded, but you may swap to another available version if you wish. The following table summarises the suites and associated programming environments. Suite name Module Programming environment collection CCE cce PrgEnv-cray GCC gcc PrgEnv-gnu AOCC aocc PrgEnv-aocc As an example, after logging in you may wish to use GCC as your compiler suite. Running module restore PrgEnv-gnu will replace the Cray environment with the GNU environment. It will also unload the cce module and load the default version of the gcc module; at the time of writing, this is GCC 10.1.0. If you need to use a different version of GCC, for example 9.3.0, you would follow up with module swap gcc gcc/9.3.0 . At this point you may invoke the compiler wrappers and they will correctly use Cray's libraries and tools in conjunction with GCC 9.3.0. Warning The gcc/8.3.0 module is available on ARCHER2 but cannot be used as the supporting scientific and system libraries are not available. You should not use this version of GCC. When choosing the programming environment, a big factor will likely be which compilers you have previously used for your code's development. The Cray Fortran compiler is similar to the compiler you may be familiar with from ARCHER, while the Cray C and C++ compilers provided on ARCHER2 are new versions that are now derived from Clang. The GCC suite provides gcc/g++ and gfortran. The AOCC suite provides AMD Clang/Clang++ and AMD Flang. Note The Intel compilers are not available on ARCHER2. Useful compiler options The compiler options you use will depend on both the software you are building and also on the current stage of development. The following flags should be a good starting point for reasonable performance: Compilers Optimisation flags Cray C/C++ -O2 -funroll-loops -ffast-math Cray Fortran Default options GCC -O2 -ftree-vectorize -funroll-loops -ffast-math Tip If you want to use GCC version 10 or greater to compile Fortran code, you must add the -fallow-argument-mismatch option when compiling otherwise you will see compile errors associated with MPI functions. When you are happy with your code's performance you may wish to enable more aggressive optimisations; in this case you could start using the following flags. Please note, however, that these optimisations may lead to deviations from IEEE/ISO specifications. If your code relies on strict adherence then these flags may lead to it producing incorrect output. Compilers Optimisation flags Cray C/C++ -Ofast -funroll-loops Cray Fortran -O3 -hfp3 GCC -Ofast -funroll-loops Vectorisation is enabled by the Cray Fortran compiler at -O1 and above, by Cray C and C++ at -O2 and above or when using -ftree-vectorize , and by the GCC compilers at -O3 and above or when using -ftree-vectorize . You may wish to promote default real and integer types in Fortran codes from 4 to 8 bytes. In this case, the following flags may be used: Compiler Fortran real and integer promotion flags Cray Fortran -s real64 -s integer64 gfortran -freal-4-real-8 -finteger-4-integer-8 More documentation on the compilers is available through man . The pages to read are accessed as follow: Compiler suite C C++ Fortran Cray man craycc man crayCC man crayftn GNU man gcc man g++ man gfortran Tip There are no man pages for the AOCC compilers at the moment. Linking on ARCHER2 Executables on ARCHER2 link dynamically, and the Cray Programming Environment does not currently support static linking. This is in contrast to ARCHER where the default was to build statically. If you attempt to link statically, you will see errors similar to: /usr/bin/ld: cannot find -lpmi /usr/bin/ld: cannot find -lpmi2 collect2: error: ld returned 1 exit status The compiler wrapper scripts on ARCHER link runtime libraries in using the runpath by default. This means that the paths to the runtime libraries are encoded into the executable so you do not need to load the compiler environment in your job submission scripts. Using RPATHs to link The default behaviour of a dynamically linked executable will be to allow the linker to provide the libraries it needs at runtime by searching the paths in the LD_LIBRARY_PATH environment variable. This is flexible in that it allows an executable to use newly installed library versions without rebuilding, but in some cases you may prefer to bake the paths to specific libraries into the executable, keeping them constant. While the libraries are still dynamically loaded at run time, from the end user's point of view the resulting behaviour will be similar to that of a statically compiled executable in that they will not need to concern themselves with ensuring the linker will be able to find the libraries. This is achieved by providing RPATHs to the compiler as options. To set the compiler wrappers to do this, you can set the following environment variable: export CRAY_ADD_RPATH=yes You can also provide RPATHs directly to the compilers using the -Wl,-rpath=<path-to-directory> flag, where the provided path is to the directory containing the libraries which are themselves typically specified with flags of the type -l<library-name> . Debugging tools The following debugging tools are available on ARCHER2: gdb4hpc is a command-line tool working similarly to gdb that allows users to debug parallel programs. It can launch parallel programs or attach to ones already running and allows the user to step through the execution to identify the causes of any unexpected behaviour. Available via module load gdb4hpc . valgrind4hpc is a parallel memory debugging tool that aids in detection of memory leaks and errors in parallel applications. It aggregates like errors across processes and threads to simplify debugging of parallel appliciations. Available via module load valgrind4hpc . STAT , the Stack Trace Analysis Tool, generates merged stack traces for parallel applications. It also provides visualisation tools. Available via module load cray-stat . To get started debugging on ARCHER2, you might like to use gdb4hpc. You should first of all compile your code using the -g flag to enable debugging symbols. Once compiled, load the gdb4hpc module and start it: module load gdb4hpc gdb4hpc Once inside gdb4hpc, you can start your program's execution with the launch command: dbg all> launch $my_prog{128} ./prog In this example, a job called my_prog will be launched to run the executable file prog over 128 cores on a compute node. If you run squeue in another terminal you will be able to see it running. Inside gdb4hpc you may then step through the code's execution, continue to breakpoints that you set with break , print the values of variables at these points, and perform a backtrace on the stack if the program crashes. Debugging jobs will end when you exit gdb4hpc, or you can end them yourself by running, in this example, release $my_prog . For more information on debugging parallel codes, see the documentation at ARCHER2 User and Best Practice Guide - Debugging <../user-guide/debug> . Note We will add more information on using the debugging tools once the ARCHER2 system is available. Profiling tools Profiling on ARCHER2 is provided through the Cray Performance Measurement and Analysis Tools (CrayPAT). This has a number of different components: CrayPAT the full-featured program analysis tool set. CrayPAT consists of pat_build, the utility used to instrument programs, the CrayPat run time environment, which collects the specified performance data during program execution, and pat_report, the first-level data analysis tool, used to produce text reports or export data for more sophisticated analysis CrayPAT-lite a simplified and easy-to-use version of CrayPAT that provides basic performance analysis information automatically, with a minimum of user interaction. Reveal the next-generation integrated performance analysis and code optimization tool, which enables the user to correlate performance data captured during program execution directly to the original source, and identify opportunities for further optimization. Cray PAPI components, which are support packages for those who want to access performance counters. Cray Apprentice2 the second-level data analysis tool, used to visualize, manipulate, explore, and compare sets of program performance data in a GUI environment. The above tools are made available for use by firstly loading the perftools-base module followed by either perftools (for CrayPAT, Reveal and Apprentice2) or one of the perftools-lite modules. The simplest way to get started profiling your code is with CrayPAT-lite. For example, to sample a run of a code you would load the perftools-base and perftools-lite modules, and then compile (you will receive a message that the executable is being instrumented). Performing a batch run as usual with this executable will produce a directory such as my_prog+74653-2s which can be passed to pat_report to view the results. In this example, pat_report -O calltree+src my_prog+74653-2s will produce a report containing the call tree. You can view available report keywords to be provided to the -O option by running pat_report -O -h . The available perftools-lite modules are: perftools-lite , instrumenting a basic sampling experiment. perftools-lite-events , instrumenting a tracing experiment. perftools-lite-gpu , instrumenting OpenACC and OpenMP 4 use of GPUs. perftools-lite-hbm , instrumenting for memory bandwidth usage. perftools-lite-loops , instrumenting a loop work estimate experiment. Tip For more information on profiling parallel codes, see the documentation at ARCHER2 User and Best Practice Guide - Profiling . Note We will add more information on using the profiling tools once the ARCHER2 system is available. Useful Links Links to other documentation you may find useful: ARCHER2 User and Best Practice Guide - Covers all aspects of use of the ARCHER2 service. This includes fundamentals (required by all users to use the system effectively), best practice for getting the most out of ARCHER2, and more advanced technical topics. Cray Programming Environment User Guide Cray Performance Measurement and Analysis Tools User Guide","title":"Quickstart for developers"},{"location":"quick-start/quickstart-developers/#quickstart-for-developers","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development. This guide aims to quickly enable developers to work on ARCHER2. It assumes that you are familiar with the material in the Quickstart for users section.","title":"Quickstart for developers"},{"location":"quick-start/quickstart-developers/#compiler-wrappers","text":"When compiling code on ARCHER2, you should make use of the Cray compiler wrappers. These ensure that the correct libraries and headers (for example, MPI or Cray LibSci) will be used during the compilation and linking stages. These wrappers should be accessed by providing the following compiler names: Language Wrapper name C cc C++ CC Fortran ftn This means that you should use the wrapper names whether on the command line, in build scripts, or in configure options. It could be helpful to set some or all of the following environment variables before running a build to ensure that the build tool is aware of the wrappers: export CC=cc export CXX=CC export FC=ftn export F77=ftn export F90=ftn man pages are available for each wrapper. You can also see the full set of compiler and linker options being used by passing the -craype-verbose option to the wrapper when using it.","title":"Compiler wrappers"},{"location":"quick-start/quickstart-developers/#programming-environments","text":"On login to ARCHER2, the PrgEnv-cray collection will be loaded, as will a cce module. The latter makes available Cray's compilers from the Cray Compiling Environment (CCE), while the former provides the correct wrappers and support to use them. The GNU Compiler Collection (GCC) is also available. To make use of any Programming Environment, restore the correct PrgEnv collection. After doing so the compiler wrappers ( cc , CC and ftn ) will correctly call the compilers from the new suite. The default version of the corresponding compiler suite will also be loaded, but you may swap to another available version if you wish. The following table summarises the suites and associated programming environments. Suite name Module Programming environment collection CCE cce PrgEnv-cray GCC gcc PrgEnv-gnu AOCC aocc PrgEnv-aocc As an example, after logging in you may wish to use GCC as your compiler suite. Running module restore PrgEnv-gnu will replace the Cray environment with the GNU environment. It will also unload the cce module and load the default version of the gcc module; at the time of writing, this is GCC 10.1.0. If you need to use a different version of GCC, for example 9.3.0, you would follow up with module swap gcc gcc/9.3.0 . At this point you may invoke the compiler wrappers and they will correctly use Cray's libraries and tools in conjunction with GCC 9.3.0. Warning The gcc/8.3.0 module is available on ARCHER2 but cannot be used as the supporting scientific and system libraries are not available. You should not use this version of GCC. When choosing the programming environment, a big factor will likely be which compilers you have previously used for your code's development. The Cray Fortran compiler is similar to the compiler you may be familiar with from ARCHER, while the Cray C and C++ compilers provided on ARCHER2 are new versions that are now derived from Clang. The GCC suite provides gcc/g++ and gfortran. The AOCC suite provides AMD Clang/Clang++ and AMD Flang. Note The Intel compilers are not available on ARCHER2.","title":"Programming environments"},{"location":"quick-start/quickstart-developers/#useful-compiler-options","text":"The compiler options you use will depend on both the software you are building and also on the current stage of development. The following flags should be a good starting point for reasonable performance: Compilers Optimisation flags Cray C/C++ -O2 -funroll-loops -ffast-math Cray Fortran Default options GCC -O2 -ftree-vectorize -funroll-loops -ffast-math Tip If you want to use GCC version 10 or greater to compile Fortran code, you must add the -fallow-argument-mismatch option when compiling otherwise you will see compile errors associated with MPI functions. When you are happy with your code's performance you may wish to enable more aggressive optimisations; in this case you could start using the following flags. Please note, however, that these optimisations may lead to deviations from IEEE/ISO specifications. If your code relies on strict adherence then these flags may lead to it producing incorrect output. Compilers Optimisation flags Cray C/C++ -Ofast -funroll-loops Cray Fortran -O3 -hfp3 GCC -Ofast -funroll-loops Vectorisation is enabled by the Cray Fortran compiler at -O1 and above, by Cray C and C++ at -O2 and above or when using -ftree-vectorize , and by the GCC compilers at -O3 and above or when using -ftree-vectorize . You may wish to promote default real and integer types in Fortran codes from 4 to 8 bytes. In this case, the following flags may be used: Compiler Fortran real and integer promotion flags Cray Fortran -s real64 -s integer64 gfortran -freal-4-real-8 -finteger-4-integer-8 More documentation on the compilers is available through man . The pages to read are accessed as follow: Compiler suite C C++ Fortran Cray man craycc man crayCC man crayftn GNU man gcc man g++ man gfortran Tip There are no man pages for the AOCC compilers at the moment.","title":"Useful compiler options"},{"location":"quick-start/quickstart-developers/#linking-on-archer2","text":"Executables on ARCHER2 link dynamically, and the Cray Programming Environment does not currently support static linking. This is in contrast to ARCHER where the default was to build statically. If you attempt to link statically, you will see errors similar to: /usr/bin/ld: cannot find -lpmi /usr/bin/ld: cannot find -lpmi2 collect2: error: ld returned 1 exit status The compiler wrapper scripts on ARCHER link runtime libraries in using the runpath by default. This means that the paths to the runtime libraries are encoded into the executable so you do not need to load the compiler environment in your job submission scripts.","title":"Linking on ARCHER2"},{"location":"quick-start/quickstart-developers/#using-rpaths-to-link","text":"The default behaviour of a dynamically linked executable will be to allow the linker to provide the libraries it needs at runtime by searching the paths in the LD_LIBRARY_PATH environment variable. This is flexible in that it allows an executable to use newly installed library versions without rebuilding, but in some cases you may prefer to bake the paths to specific libraries into the executable, keeping them constant. While the libraries are still dynamically loaded at run time, from the end user's point of view the resulting behaviour will be similar to that of a statically compiled executable in that they will not need to concern themselves with ensuring the linker will be able to find the libraries. This is achieved by providing RPATHs to the compiler as options. To set the compiler wrappers to do this, you can set the following environment variable: export CRAY_ADD_RPATH=yes You can also provide RPATHs directly to the compilers using the -Wl,-rpath=<path-to-directory> flag, where the provided path is to the directory containing the libraries which are themselves typically specified with flags of the type -l<library-name> .","title":"Using RPATHs to link"},{"location":"quick-start/quickstart-developers/#debugging-tools","text":"The following debugging tools are available on ARCHER2: gdb4hpc is a command-line tool working similarly to gdb that allows users to debug parallel programs. It can launch parallel programs or attach to ones already running and allows the user to step through the execution to identify the causes of any unexpected behaviour. Available via module load gdb4hpc . valgrind4hpc is a parallel memory debugging tool that aids in detection of memory leaks and errors in parallel applications. It aggregates like errors across processes and threads to simplify debugging of parallel appliciations. Available via module load valgrind4hpc . STAT , the Stack Trace Analysis Tool, generates merged stack traces for parallel applications. It also provides visualisation tools. Available via module load cray-stat . To get started debugging on ARCHER2, you might like to use gdb4hpc. You should first of all compile your code using the -g flag to enable debugging symbols. Once compiled, load the gdb4hpc module and start it: module load gdb4hpc gdb4hpc Once inside gdb4hpc, you can start your program's execution with the launch command: dbg all> launch $my_prog{128} ./prog In this example, a job called my_prog will be launched to run the executable file prog over 128 cores on a compute node. If you run squeue in another terminal you will be able to see it running. Inside gdb4hpc you may then step through the code's execution, continue to breakpoints that you set with break , print the values of variables at these points, and perform a backtrace on the stack if the program crashes. Debugging jobs will end when you exit gdb4hpc, or you can end them yourself by running, in this example, release $my_prog . For more information on debugging parallel codes, see the documentation at ARCHER2 User and Best Practice Guide - Debugging <../user-guide/debug> . Note We will add more information on using the debugging tools once the ARCHER2 system is available.","title":"Debugging tools"},{"location":"quick-start/quickstart-developers/#profiling-tools","text":"Profiling on ARCHER2 is provided through the Cray Performance Measurement and Analysis Tools (CrayPAT). This has a number of different components: CrayPAT the full-featured program analysis tool set. CrayPAT consists of pat_build, the utility used to instrument programs, the CrayPat run time environment, which collects the specified performance data during program execution, and pat_report, the first-level data analysis tool, used to produce text reports or export data for more sophisticated analysis CrayPAT-lite a simplified and easy-to-use version of CrayPAT that provides basic performance analysis information automatically, with a minimum of user interaction. Reveal the next-generation integrated performance analysis and code optimization tool, which enables the user to correlate performance data captured during program execution directly to the original source, and identify opportunities for further optimization. Cray PAPI components, which are support packages for those who want to access performance counters. Cray Apprentice2 the second-level data analysis tool, used to visualize, manipulate, explore, and compare sets of program performance data in a GUI environment. The above tools are made available for use by firstly loading the perftools-base module followed by either perftools (for CrayPAT, Reveal and Apprentice2) or one of the perftools-lite modules. The simplest way to get started profiling your code is with CrayPAT-lite. For example, to sample a run of a code you would load the perftools-base and perftools-lite modules, and then compile (you will receive a message that the executable is being instrumented). Performing a batch run as usual with this executable will produce a directory such as my_prog+74653-2s which can be passed to pat_report to view the results. In this example, pat_report -O calltree+src my_prog+74653-2s will produce a report containing the call tree. You can view available report keywords to be provided to the -O option by running pat_report -O -h . The available perftools-lite modules are: perftools-lite , instrumenting a basic sampling experiment. perftools-lite-events , instrumenting a tracing experiment. perftools-lite-gpu , instrumenting OpenACC and OpenMP 4 use of GPUs. perftools-lite-hbm , instrumenting for memory bandwidth usage. perftools-lite-loops , instrumenting a loop work estimate experiment. Tip For more information on profiling parallel codes, see the documentation at ARCHER2 User and Best Practice Guide - Profiling . Note We will add more information on using the profiling tools once the ARCHER2 system is available.","title":"Profiling tools"},{"location":"quick-start/quickstart-developers/#useful-links","text":"Links to other documentation you may find useful: ARCHER2 User and Best Practice Guide - Covers all aspects of use of the ARCHER2 service. This includes fundamentals (required by all users to use the system effectively), best practice for getting the most out of ARCHER2, and more advanced technical topics. Cray Programming Environment User Guide Cray Performance Measurement and Analysis Tools User Guide","title":"Useful Links"},{"location":"quick-start/quickstart-users/","text":"Quickstart for users Warning The ARCHER2 Service is not yet available. This documentation is in development. This guide aims to quickly enable new users to get up and running on ARCHER2. It covers the process of getting an ARCHER2 account, logging in and running your first job. Request an account on ARCHER2 Important You need to use both a password and a passphrase-protected SSH key pair to log into ARCHER2. You get the password from SAFE, but, you will also need to setup your own SSH key pair and add the public part to your account via SAFE before you will be able to log in. We cover the authentication steps below. Obtain an account on the SAFE website The first step is to sign up for an account on the ARCHER2 SAFE website. The SAFE account is used to manage all of your login accounts, allowing you to report on your usage and quotas. To do this: Go to the SAFE New User Signup Form Fill in your personal details. You can come back later and change them if you wish Click Submit You are now registered. Your SAFE password will be emailed to the email address you provided. You can then login with that email address and password. (You can change your initial SAFE password whenever you want by selecting the Change SAFE password option from the Your details menu.) Generating and adding an SSH key pair How you generate your SSH key pair depends on which operating system you use and which SSH client you use to connect to ARCHER2. We will not cover the details on generating an SSH key pair here, but detailed information on generating an SSH key pair is available in the ARCHER2 User and Best Practice Guide . Once you have generated your SSH key pair, you should add the public part to your login account using SAFE: Log into SAFE Use the menu Your details and select Update personal details Either copy and paste the public part of your SSH key into the SSH Public key box or use the button to select the public key file on your computer. Click Update to associate the public SSH key part with your SAFE account Request an ARCHER2 login account Once you have a SAFE account and an SSH key you will need to request a user account on ARCHER2 itself. To do this you will require a Project Code ; you usually obtain this from the Principle Investigator (PI) or project manager for the project you will be working on. Once you have the Project Code: Log into SAFE Use the Login accounts - Request new account menu item Select the correct project from the drop down list Select the ARCHER2 machine in the list of available machines Click Next Enter a username for the account Click Request The PI or project manager of the project will be asked to approve your request. After your request has been approved the account will be created and when this has been done you will receive an email. You can then come back to SAFE and pick up the initial single-use password for your new account. Note ARCHER2 account passwords are also sometimes referred to as LDAP passwords by the system. Generating and adding an SSH key pair How you generate your SSH key pair depends on which operating system you use and which SSH client you use to connect to ARCHER2. We will not cover the details on generating an SSH key pair here, but detailed information on this topic is available in the ARCHER2 User and Best Practice Guide . After generating your SSH key pair, add the public part to your login account using SAFE: Log into SAFE Use the menu Login accounts and select the ARCHER2 account to be associated with the SSH key On the subsequent Login account details page, click the Add Credential button Select SSH public key as the Credential Type and click Next Either copy and paste the public part of your SSH key into the SSH Public key box or use the button to select the public key file on your computer Click Add to associate the public SSH key part with your account Once you have done this, your SSH key will be added to your ARCHER2 account. Remember, you will need to use both an SSH key and password to log into ARCHER2 so you will also need to collect your initial password before you can log into ARCHER2 for the first time. We cover this next. Collecting your ARCHER2 password You should now collect your ARCHER2 password: Log into SAFE Use the Login accounts menu to select your new login account Use the View Login Account Password button to view your single-use ARCHER2 password This password is generated randomly by the software. It's best to copy-and-paste it across when you log in to ARCHER2. After you login, you will immediately be prompted to begin the process of changing your password. You should now enter the initial password again, and then you will be prompted for your new, easy-to-remember password. Your new password should conform to the ARCHER2 Password Policy . Note The View Login Account Password option within SAFE will continue to display your old initial password. Your SAFE account has no knowledge of your new machine account password. Login to ARCHER2 To log into ARCHER2 you should use the login.archer2.ac.uk address: ssh [userID]@login.archer2.ac.uk You will first be prompted for your machine account password. Once you have entered your password successfully, you will then be prompted for the passphrase associated with your SSH key pair. You need to enter both credentials correctly to be able to access ARCHER2. Tip If your SSH key pair is not stored in the default location (usually ~/.ssh/id_rsa ) on your local system, you may need to specify the path to the private part of the key wih the -i option to ssh . For example, if your key is in a file called keys/id_rsa_archer2 you would use the command ssh -i keys/id_rsa_archer2 username@login.archer2.ac.uk to log in. Tip When you first log into ARCHER2, you will be prompted to change your initial password. This is a three step process: When prompted to enter your ldap password : re-enter the password you retrieved from SAFE When prompted to enter your new password: type in a new password When prompted to re-enter the new password: re-enter the new password Your password has now been changed. Hint More information on connecting to ARCHER2 is available in the Connecting to ARCHER2 section of the User Guide. File systems and manipulating data ARCHER2 has a number of different file systems and understanding the difference between them is crucial to being able to use the system. In particular, transferring and moving data often requires a bit of thought in advance to ensure that the data is secure and in a useful form. ARCHER2 file systems are: /home : backed up for disaster recovery purposes only, data recovery for accidental deletion is not supported. NFS is available on login and service nodes. /work : not backed-up. Lustre is available on login, service and compute nodes. Top tips for managing data on ARCHER2: Do not generate huge numbers of files (>1000) in a single directory. Poor performance relating to file transfer is often due to the number of files involved in the transfer - minimise the number of files that you have to transfer by using archiving tools to improve performance. Archive directories or large numbers of files before moving them between file systems (e.g. by using commands like tar or zip ). When using tar or rsync between file systems mounted on ARCHER2 avoid the use of compression options as these can slow performance (time saved by transferring smaller compressed files is usually less than the overhead added by having to compress files on the fly). Think about automating the merging and transfer of multiple files output by software on ARCHER2 to other resources. The Data Management Guide linked below provides examples of how to automatically verify the integrity of an archive. Hint Information on best practice in managing you data is available in the Data management and transfer section of the User Guide. Accessing software Software on ARCHER2 is principally accessed through environment modules. These load and unload the desired compilers, tools and libraries through the module command and its subcommands. Some modules will be loaded by default on login, providing a default working environment; many more will be available for use but initially unloaded, allowing you to set up the environment to suit your needs. At any stage you can check which modules have been loaded by running module list Running the following command will display all environment modules available on ARCHER2, whether loaded or unloaded module avail The search field for this command may be narrowed by providing the first few characters of the module name being queried. For example, all available versions and variants of VASP may be found by running module avail vasp You will see that different versions are available for many modules. For example, vasp/5/5.4.4 and vasp/6/6.1.0 are two available versions of VASP. Furthermore, a default version may be specified; this is used if no version is provided by the user. Important VASP is licensed software, as are other software packages on ARCHER2. You must have a valid licence to use licensed software on ARCHER2. Often you will need to request access through the SAFE. More on this below. The module load and module add commands perform the same action, loading a module for use. Following the above, module load vasp/5 would load the default version of VASP 5, while module load vasp/5/5.4.4 would specifically load version 5.4.4 . A loaded module may be unloaded through the identical module unload , module remove or module delete commands, e.g. module unload vasp The above unloads whichever version of VASP is currently in the environment. Rather than issuing separate unload and load commands, versions of a module may be swapped as follows: module swap vasp vasp/5/5.4.4 Other helpful commands are: module help <modulename> which provides a short description of the module module show <modulename> which displays the contents of the modulefile Points to be aware of include: Some modules will conflict with others. A simple example would be the conflict arising when trying to load a different version of an already loaded module. When a conflict occurs, the loading process will fail and an error message will be displayed. Examination of the message and the module output (via module show ) should reveal the cause of the conflict and how to resolve it. The order in which modules are loaded can matter. Consider two modules which set the same variable to a different value. The final value would be that set by the module which loaded last. If you suspect that two modules may be interfering with one another, you can examine their contents with module show . Requesting access to licensed software Some of the software installed on ARCHER2 requires a user to have a valid licence agreed with the software owners/developers to be able to use it (for example, VASP). Although you will be able to load this software on ARCHER2, you will be barred from actually using it until your licence has been verified. You request access to licensed software through the EPCC SAFE (the web administration tool you used to apply for your account and retrieve your initial password) by being added to the appropriate Package Group . To request access to licensed software: Log in to SAFE Go to the Menu Login accounts and select the login account which requires access to the software Click New Package Group Request Select the software from the list of available packages and click Select Package Group Fill in as much information as possible about your license; at the very least provide the information requested at the top of the screen such as the licence holder's name and contact details. If you are covered by the license because the licence holder is your supervisor, for example, please state this. Click Submit Your request will then be processed by the ARCHER2 Service Desk who will confirm your license with the software owners/developers before enabling your access to the software on ARCHER2. This can take several days (depending on how quickly the software owners/developers take to respond) but you will be advised once this has been done. Create a job submission script To run a program on the ARCHER2 compute nodes you need to write a job submission script that tells the system how many compute nodes you want to reserve and for how long. You also need to use the srun command to launch your parallel executable. Hint For a more details on the Slurm scheduler on ARCHER2 and writing job submission scripts see the Running jobs on ARCHER2 section of the User and Best Practice Guide. Important Parallel jobs on ARCHER2 should be run from the /work file system as /home is not available on the compute nodes - you will see a chdir or file not found error if you try to run a job from the /home file system. Create a job submission script called submit.slurm in your space on the work file system using your favourite text editor. For example, using vim : auser@uan01:~> cd /work/t01/t01/auser auser@uan01:/work/t01/t01/auser> vim submit.slurm Tip You will need to use your project code and username to get to the correct directory. i.e. replace the t01 above with your project code and replace the username auser with your ARCHER2 username. Paste the following text into your job submission script, replacing ENTER_YOUR_BUDGET_CODE_HERE with your budget code e.g. e99-ham , ENTER_PARTITION_HERE with the partition you wish to run on (e.g standard ), and ENTER_QOS_HERE with the quality of service you want (e.g. standard ). #!/bin/bash --login #SBATCH --job-name=test_job #SBATCH --nodes=1 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=0:5:0 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the batch environment module load epcc-job-env # Load the xthi module to get access to the xthi program module load xthi # srun launches the parallel program based on the SBATCH options srun --cpu-bind=cores xthi Submit your job to the queue You submit your job to the queues using the sbatch command: auser@uan01:/work/t01/t01/auser> sbatch submit.slurm Submitted batch job 23996 The value returned is your *Job ID*. Monitoring your job You use the squeue command to examine jobs in the queue. Use: auser@uan01:/work/t01/t01/auser> squeue -u $USER To list all the jobs you have in the queue. squeue on its own lists all jobs in the queue from all users. Checking the output from the job The job submission script above should write the output to a file called slurm-<jobID>.out (i.e. if the Job ID was 23996, the file would be slurm-23996.out ), you can check the contents of this file with the cat command. If the job was successful you should see output that looks something like: auser@eslogin01:/work/t01/t01/auser> cat slurm-23996.out Hello from rank 20, thread 0, on nid00001. (core affinity = 20) Hello from rank 27, thread 0, on nid00001. (core affinity = 27) Hello from rank 23, thread 0, on nid00001. (core affinity = 23) Hello from rank 34, thread 0, on nid00001. (core affinity = 34) Hello from rank 18, thread 0, on nid00001. (core affinity = 18) Hello from rank 33, thread 0, on nid00001. (core affinity = 33) Hello from rank 19, thread 0, on nid00001. (core affinity = 19) Hello from rank 22, thread 0, on nid00001. (core affinity = 22) Hello from rank 6, thread 0, on nid00001. (core affinity = 6) Hello from rank 26, thread 0, on nid00001. (core affinity = 26) Hello from rank 31, thread 0, on nid00001. (core affinity = 31) Hello from rank 21, thread 0, on nid00001. (core affinity = 21) Hello from rank 35, thread 0, on nid00001. (core affinity = 35) Hello from rank 32, thread 0, on nid00001. (core affinity = 32) Hello from rank 28, thread 0, on nid00001. (core affinity = 28) Hello from rank 25, thread 0, on nid00001. (core affinity = 25) Hello from rank 24, thread 0, on nid00001. (core affinity = 24) Hello from rank 30, thread 0, on nid00001. (core affinity = 30) Hello from rank 29, thread 0, on nid00001. (core affinity = 29) Hello from rank 10, thread 0, on nid00001. (core affinity = 10) Hello from rank 2, thread 0, on nid00001. (core affinity = 2) Hello from rank 11, thread 0, on nid00001. (core affinity = 11) Hello from rank 0, thread 0, on nid00001. (core affinity = 0) Hello from rank 1, thread 0, on nid00001. (core affinity = 1) Hello from rank 7, thread 0, on nid00001. (core affinity = 7) Hello from rank 4, thread 0, on nid00001. (core affinity = 4) Hello from rank 3, thread 0, on nid00001. (core affinity = 3) Hello from rank 5, thread 0, on nid00001. (core affinity = 5) Hello from rank 8, thread 0, on nid00001. (core affinity = 8) Hello from rank 9, thread 0, on nid00001. (core affinity = 9) Hello from rank 12, thread 0, on nid00001. (core affinity = 12) Hello from rank 13, thread 0, on nid00001. (core affinity = 13) Hello from rank 14, thread 0, on nid00001. (core affinity = 14) Hello from rank 15, thread 0, on nid00001. (core affinity = 15) Hello from rank 16, thread 0, on nid00001. (core affinity = 16) Hello from rank 17, thread 0, on nid00001. (core affinity = 17) ... output trimmed ... If something has gone wrong, you will find any error messages in the file instead of the expected output. Acknowledging ARCHER2 You should use the following phrase to acknowledge ARCHER2 for all research outputs that were generated using the ARCHER2 service: This work used the ARCHER2 UK National Supercomputing Service (https://www.archer2.ac.uk). You should also tag outputs with the keyword ARCHER2 whenever possible. Useful Links If you plan to compile your own programs on ARCHER2, you may also want to look at quickstart-developers . Other documentation you may find useful: ARCHER2 User and Best Practice Guide : Covers all aspects of use of the ARCHER2 service. This includes fundamentals (required by all users to use the system effectively), best practice for getting the most out of ARCHER2, and more advanced technical topics. Cray Programming Environment User Guide","title":"Quickstart for users"},{"location":"quick-start/quickstart-users/#quickstart-for-users","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development. This guide aims to quickly enable new users to get up and running on ARCHER2. It covers the process of getting an ARCHER2 account, logging in and running your first job.","title":"Quickstart for users"},{"location":"quick-start/quickstart-users/#request-an-account-on-archer2","text":"Important You need to use both a password and a passphrase-protected SSH key pair to log into ARCHER2. You get the password from SAFE, but, you will also need to setup your own SSH key pair and add the public part to your account via SAFE before you will be able to log in. We cover the authentication steps below.","title":"Request an account on ARCHER2"},{"location":"quick-start/quickstart-users/#obtain-an-account-on-the-safe-website","text":"The first step is to sign up for an account on the ARCHER2 SAFE website. The SAFE account is used to manage all of your login accounts, allowing you to report on your usage and quotas. To do this: Go to the SAFE New User Signup Form Fill in your personal details. You can come back later and change them if you wish Click Submit You are now registered. Your SAFE password will be emailed to the email address you provided. You can then login with that email address and password. (You can change your initial SAFE password whenever you want by selecting the Change SAFE password option from the Your details menu.)","title":"Obtain an account on the SAFE website"},{"location":"quick-start/quickstart-users/#generating-and-adding-an-ssh-key-pair","text":"How you generate your SSH key pair depends on which operating system you use and which SSH client you use to connect to ARCHER2. We will not cover the details on generating an SSH key pair here, but detailed information on generating an SSH key pair is available in the ARCHER2 User and Best Practice Guide . Once you have generated your SSH key pair, you should add the public part to your login account using SAFE: Log into SAFE Use the menu Your details and select Update personal details Either copy and paste the public part of your SSH key into the SSH Public key box or use the button to select the public key file on your computer. Click Update to associate the public SSH key part with your SAFE account","title":"Generating and adding an SSH key pair"},{"location":"quick-start/quickstart-users/#request-an-archer2-login-account","text":"Once you have a SAFE account and an SSH key you will need to request a user account on ARCHER2 itself. To do this you will require a Project Code ; you usually obtain this from the Principle Investigator (PI) or project manager for the project you will be working on. Once you have the Project Code: Log into SAFE Use the Login accounts - Request new account menu item Select the correct project from the drop down list Select the ARCHER2 machine in the list of available machines Click Next Enter a username for the account Click Request The PI or project manager of the project will be asked to approve your request. After your request has been approved the account will be created and when this has been done you will receive an email. You can then come back to SAFE and pick up the initial single-use password for your new account. Note ARCHER2 account passwords are also sometimes referred to as LDAP passwords by the system.","title":"Request an ARCHER2 login account"},{"location":"quick-start/quickstart-users/#generating-and-adding-an-ssh-key-pair_1","text":"How you generate your SSH key pair depends on which operating system you use and which SSH client you use to connect to ARCHER2. We will not cover the details on generating an SSH key pair here, but detailed information on this topic is available in the ARCHER2 User and Best Practice Guide . After generating your SSH key pair, add the public part to your login account using SAFE: Log into SAFE Use the menu Login accounts and select the ARCHER2 account to be associated with the SSH key On the subsequent Login account details page, click the Add Credential button Select SSH public key as the Credential Type and click Next Either copy and paste the public part of your SSH key into the SSH Public key box or use the button to select the public key file on your computer Click Add to associate the public SSH key part with your account Once you have done this, your SSH key will be added to your ARCHER2 account. Remember, you will need to use both an SSH key and password to log into ARCHER2 so you will also need to collect your initial password before you can log into ARCHER2 for the first time. We cover this next.","title":"Generating and adding an SSH key pair"},{"location":"quick-start/quickstart-users/#collecting-your-archer2-password","text":"You should now collect your ARCHER2 password: Log into SAFE Use the Login accounts menu to select your new login account Use the View Login Account Password button to view your single-use ARCHER2 password This password is generated randomly by the software. It's best to copy-and-paste it across when you log in to ARCHER2. After you login, you will immediately be prompted to begin the process of changing your password. You should now enter the initial password again, and then you will be prompted for your new, easy-to-remember password. Your new password should conform to the ARCHER2 Password Policy . Note The View Login Account Password option within SAFE will continue to display your old initial password. Your SAFE account has no knowledge of your new machine account password.","title":"Collecting your ARCHER2 password"},{"location":"quick-start/quickstart-users/#login-to-archer2","text":"To log into ARCHER2 you should use the login.archer2.ac.uk address: ssh [userID]@login.archer2.ac.uk You will first be prompted for your machine account password. Once you have entered your password successfully, you will then be prompted for the passphrase associated with your SSH key pair. You need to enter both credentials correctly to be able to access ARCHER2. Tip If your SSH key pair is not stored in the default location (usually ~/.ssh/id_rsa ) on your local system, you may need to specify the path to the private part of the key wih the -i option to ssh . For example, if your key is in a file called keys/id_rsa_archer2 you would use the command ssh -i keys/id_rsa_archer2 username@login.archer2.ac.uk to log in. Tip When you first log into ARCHER2, you will be prompted to change your initial password. This is a three step process: When prompted to enter your ldap password : re-enter the password you retrieved from SAFE When prompted to enter your new password: type in a new password When prompted to re-enter the new password: re-enter the new password Your password has now been changed. Hint More information on connecting to ARCHER2 is available in the Connecting to ARCHER2 section of the User Guide.","title":"Login to ARCHER2"},{"location":"quick-start/quickstart-users/#file-systems-and-manipulating-data","text":"ARCHER2 has a number of different file systems and understanding the difference between them is crucial to being able to use the system. In particular, transferring and moving data often requires a bit of thought in advance to ensure that the data is secure and in a useful form. ARCHER2 file systems are: /home : backed up for disaster recovery purposes only, data recovery for accidental deletion is not supported. NFS is available on login and service nodes. /work : not backed-up. Lustre is available on login, service and compute nodes. Top tips for managing data on ARCHER2: Do not generate huge numbers of files (>1000) in a single directory. Poor performance relating to file transfer is often due to the number of files involved in the transfer - minimise the number of files that you have to transfer by using archiving tools to improve performance. Archive directories or large numbers of files before moving them between file systems (e.g. by using commands like tar or zip ). When using tar or rsync between file systems mounted on ARCHER2 avoid the use of compression options as these can slow performance (time saved by transferring smaller compressed files is usually less than the overhead added by having to compress files on the fly). Think about automating the merging and transfer of multiple files output by software on ARCHER2 to other resources. The Data Management Guide linked below provides examples of how to automatically verify the integrity of an archive. Hint Information on best practice in managing you data is available in the Data management and transfer section of the User Guide.","title":"File systems and manipulating data"},{"location":"quick-start/quickstart-users/#accessing-software","text":"Software on ARCHER2 is principally accessed through environment modules. These load and unload the desired compilers, tools and libraries through the module command and its subcommands. Some modules will be loaded by default on login, providing a default working environment; many more will be available for use but initially unloaded, allowing you to set up the environment to suit your needs. At any stage you can check which modules have been loaded by running module list Running the following command will display all environment modules available on ARCHER2, whether loaded or unloaded module avail The search field for this command may be narrowed by providing the first few characters of the module name being queried. For example, all available versions and variants of VASP may be found by running module avail vasp You will see that different versions are available for many modules. For example, vasp/5/5.4.4 and vasp/6/6.1.0 are two available versions of VASP. Furthermore, a default version may be specified; this is used if no version is provided by the user. Important VASP is licensed software, as are other software packages on ARCHER2. You must have a valid licence to use licensed software on ARCHER2. Often you will need to request access through the SAFE. More on this below. The module load and module add commands perform the same action, loading a module for use. Following the above, module load vasp/5 would load the default version of VASP 5, while module load vasp/5/5.4.4 would specifically load version 5.4.4 . A loaded module may be unloaded through the identical module unload , module remove or module delete commands, e.g. module unload vasp The above unloads whichever version of VASP is currently in the environment. Rather than issuing separate unload and load commands, versions of a module may be swapped as follows: module swap vasp vasp/5/5.4.4 Other helpful commands are: module help <modulename> which provides a short description of the module module show <modulename> which displays the contents of the modulefile Points to be aware of include: Some modules will conflict with others. A simple example would be the conflict arising when trying to load a different version of an already loaded module. When a conflict occurs, the loading process will fail and an error message will be displayed. Examination of the message and the module output (via module show ) should reveal the cause of the conflict and how to resolve it. The order in which modules are loaded can matter. Consider two modules which set the same variable to a different value. The final value would be that set by the module which loaded last. If you suspect that two modules may be interfering with one another, you can examine their contents with module show .","title":"Accessing software"},{"location":"quick-start/quickstart-users/#requesting-access-to-licensed-software","text":"Some of the software installed on ARCHER2 requires a user to have a valid licence agreed with the software owners/developers to be able to use it (for example, VASP). Although you will be able to load this software on ARCHER2, you will be barred from actually using it until your licence has been verified. You request access to licensed software through the EPCC SAFE (the web administration tool you used to apply for your account and retrieve your initial password) by being added to the appropriate Package Group . To request access to licensed software: Log in to SAFE Go to the Menu Login accounts and select the login account which requires access to the software Click New Package Group Request Select the software from the list of available packages and click Select Package Group Fill in as much information as possible about your license; at the very least provide the information requested at the top of the screen such as the licence holder's name and contact details. If you are covered by the license because the licence holder is your supervisor, for example, please state this. Click Submit Your request will then be processed by the ARCHER2 Service Desk who will confirm your license with the software owners/developers before enabling your access to the software on ARCHER2. This can take several days (depending on how quickly the software owners/developers take to respond) but you will be advised once this has been done.","title":"Requesting access to licensed software"},{"location":"quick-start/quickstart-users/#create-a-job-submission-script","text":"To run a program on the ARCHER2 compute nodes you need to write a job submission script that tells the system how many compute nodes you want to reserve and for how long. You also need to use the srun command to launch your parallel executable. Hint For a more details on the Slurm scheduler on ARCHER2 and writing job submission scripts see the Running jobs on ARCHER2 section of the User and Best Practice Guide. Important Parallel jobs on ARCHER2 should be run from the /work file system as /home is not available on the compute nodes - you will see a chdir or file not found error if you try to run a job from the /home file system. Create a job submission script called submit.slurm in your space on the work file system using your favourite text editor. For example, using vim : auser@uan01:~> cd /work/t01/t01/auser auser@uan01:/work/t01/t01/auser> vim submit.slurm Tip You will need to use your project code and username to get to the correct directory. i.e. replace the t01 above with your project code and replace the username auser with your ARCHER2 username. Paste the following text into your job submission script, replacing ENTER_YOUR_BUDGET_CODE_HERE with your budget code e.g. e99-ham , ENTER_PARTITION_HERE with the partition you wish to run on (e.g standard ), and ENTER_QOS_HERE with the quality of service you want (e.g. standard ). #!/bin/bash --login #SBATCH --job-name=test_job #SBATCH --nodes=1 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=0:5:0 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the batch environment module load epcc-job-env # Load the xthi module to get access to the xthi program module load xthi # srun launches the parallel program based on the SBATCH options srun --cpu-bind=cores xthi","title":"Create a job submission script"},{"location":"quick-start/quickstart-users/#submit-your-job-to-the-queue","text":"You submit your job to the queues using the sbatch command: auser@uan01:/work/t01/t01/auser> sbatch submit.slurm Submitted batch job 23996 The value returned is your *Job ID*.","title":"Submit your job to the queue"},{"location":"quick-start/quickstart-users/#monitoring-your-job","text":"You use the squeue command to examine jobs in the queue. Use: auser@uan01:/work/t01/t01/auser> squeue -u $USER To list all the jobs you have in the queue. squeue on its own lists all jobs in the queue from all users.","title":"Monitoring your job"},{"location":"quick-start/quickstart-users/#checking-the-output-from-the-job","text":"The job submission script above should write the output to a file called slurm-<jobID>.out (i.e. if the Job ID was 23996, the file would be slurm-23996.out ), you can check the contents of this file with the cat command. If the job was successful you should see output that looks something like: auser@eslogin01:/work/t01/t01/auser> cat slurm-23996.out Hello from rank 20, thread 0, on nid00001. (core affinity = 20) Hello from rank 27, thread 0, on nid00001. (core affinity = 27) Hello from rank 23, thread 0, on nid00001. (core affinity = 23) Hello from rank 34, thread 0, on nid00001. (core affinity = 34) Hello from rank 18, thread 0, on nid00001. (core affinity = 18) Hello from rank 33, thread 0, on nid00001. (core affinity = 33) Hello from rank 19, thread 0, on nid00001. (core affinity = 19) Hello from rank 22, thread 0, on nid00001. (core affinity = 22) Hello from rank 6, thread 0, on nid00001. (core affinity = 6) Hello from rank 26, thread 0, on nid00001. (core affinity = 26) Hello from rank 31, thread 0, on nid00001. (core affinity = 31) Hello from rank 21, thread 0, on nid00001. (core affinity = 21) Hello from rank 35, thread 0, on nid00001. (core affinity = 35) Hello from rank 32, thread 0, on nid00001. (core affinity = 32) Hello from rank 28, thread 0, on nid00001. (core affinity = 28) Hello from rank 25, thread 0, on nid00001. (core affinity = 25) Hello from rank 24, thread 0, on nid00001. (core affinity = 24) Hello from rank 30, thread 0, on nid00001. (core affinity = 30) Hello from rank 29, thread 0, on nid00001. (core affinity = 29) Hello from rank 10, thread 0, on nid00001. (core affinity = 10) Hello from rank 2, thread 0, on nid00001. (core affinity = 2) Hello from rank 11, thread 0, on nid00001. (core affinity = 11) Hello from rank 0, thread 0, on nid00001. (core affinity = 0) Hello from rank 1, thread 0, on nid00001. (core affinity = 1) Hello from rank 7, thread 0, on nid00001. (core affinity = 7) Hello from rank 4, thread 0, on nid00001. (core affinity = 4) Hello from rank 3, thread 0, on nid00001. (core affinity = 3) Hello from rank 5, thread 0, on nid00001. (core affinity = 5) Hello from rank 8, thread 0, on nid00001. (core affinity = 8) Hello from rank 9, thread 0, on nid00001. (core affinity = 9) Hello from rank 12, thread 0, on nid00001. (core affinity = 12) Hello from rank 13, thread 0, on nid00001. (core affinity = 13) Hello from rank 14, thread 0, on nid00001. (core affinity = 14) Hello from rank 15, thread 0, on nid00001. (core affinity = 15) Hello from rank 16, thread 0, on nid00001. (core affinity = 16) Hello from rank 17, thread 0, on nid00001. (core affinity = 17) ... output trimmed ... If something has gone wrong, you will find any error messages in the file instead of the expected output.","title":"Checking the output from the job"},{"location":"quick-start/quickstart-users/#acknowledging-archer2","text":"You should use the following phrase to acknowledge ARCHER2 for all research outputs that were generated using the ARCHER2 service: This work used the ARCHER2 UK National Supercomputing Service (https://www.archer2.ac.uk). You should also tag outputs with the keyword ARCHER2 whenever possible.","title":"Acknowledging ARCHER2"},{"location":"quick-start/quickstart-users/#useful-links","text":"If you plan to compile your own programs on ARCHER2, you may also want to look at quickstart-developers . Other documentation you may find useful: ARCHER2 User and Best Practice Guide : Covers all aspects of use of the ARCHER2 service. This includes fundamentals (required by all users to use the system effectively), best practice for getting the most out of ARCHER2, and more advanced technical topics. Cray Programming Environment User Guide","title":"Useful Links"},{"location":"research-software/","text":"Research Software Information on each of the centrally-installed Research Software packages, versions available, how to get access, example job submission scripts, good practice for getting best performance, links to associated training and webinars, links to associated technical reports (eCSE final reports, white papers), links to instruction manuals and further information. Centrally supported packages ARCHER2 provides a number of research software packages as centrally supported packages . Many of these packages are free to use, but others require a license (which you, or your research group, need to supply). The centrally supported package will usually be the current stable release, to include major releases and significant updates. We will usually not maintain older versions and versions no longer supported by the developers of the package. The following sections provide details on access to each of the centrally supported packages: CASTEP Chemshell Code_Saturne CP2K ELK FEniCS GROMACS LAMMPS MITgcm Met Office Unified Model NAMD Nektar++ NEMO NWChem ONETEP OpenFOAM Quantum Espresso VASP Not on the list? If the code you are interested in is not in the above list, we may still be able to help you install your own version, either individually, or as a project. Please contact the Service Desk .","title":"Overview"},{"location":"research-software/#research-software","text":"Information on each of the centrally-installed Research Software packages, versions available, how to get access, example job submission scripts, good practice for getting best performance, links to associated training and webinars, links to associated technical reports (eCSE final reports, white papers), links to instruction manuals and further information.","title":"Research Software"},{"location":"research-software/#centrally-supported-packages","text":"ARCHER2 provides a number of research software packages as centrally supported packages . Many of these packages are free to use, but others require a license (which you, or your research group, need to supply). The centrally supported package will usually be the current stable release, to include major releases and significant updates. We will usually not maintain older versions and versions no longer supported by the developers of the package. The following sections provide details on access to each of the centrally supported packages: CASTEP Chemshell Code_Saturne CP2K ELK FEniCS GROMACS LAMMPS MITgcm Met Office Unified Model NAMD Nektar++ NEMO NWChem ONETEP OpenFOAM Quantum Espresso VASP","title":"Centrally supported packages"},{"location":"research-software/#not-on-the-list","text":"If the code you are interested in is not in the above list, we may still be able to help you install your own version, either individually, or as a project. Please contact the Service Desk .","title":"Not on the list?"},{"location":"research-software/castep/castep/","text":"CASTEP Warning The ARCHER2 Service is not yet available. This documentation is in development. CASTEP is a leading code for calculating the properties of materials from first principles. Using density functional theory, it can simulate a wide range of properties of materials proprieties including energetics, structure at the atomic level, vibrational properties, electronic response properties etc. In particular it has a wide range of spectroscopic features that link directly to experiment, such as infra-red and Raman spectroscopies, NMR, and core level spectra. Useful Links CASTEP User Guides CASTEP Tutorials CASTEP Licensing Using CASTEP on ARCHER2 CASTEP is only available to users who have a valid CASTEP licence. If you have a CASTEP licence and wish to have access to CASTEP on ARCHER2, please make a request via the SAFE, see: How to request access to package groups Please have your license details to hand. Running parallel CASTEP jobs The following script will run a CASTEP job using 2 nodes (256 cores). it assumes that the input files have the file stem text_calc . #!/bin/bash # Request 2 nodes with 128 MPI tasks per node for 20 minutes #SBATCH --job-name=CASTEP #SBATCH --nodes=2 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the batch environment module load epcc-job-env # Load the CASTEP module, avoid any unintentional OpenMP threading by # setting OMP_NUM_THREADS, and launch the code. module load castep export OMP_NUM_THREADS=1 srun -cpu-bind=cores castep.mpi test_calc Compiling CASTEP The latest instructions for building CASTEP on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for CASTEP on GitHub","title":"CASTEP"},{"location":"research-software/castep/castep/#castep","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development. CASTEP is a leading code for calculating the properties of materials from first principles. Using density functional theory, it can simulate a wide range of properties of materials proprieties including energetics, structure at the atomic level, vibrational properties, electronic response properties etc. In particular it has a wide range of spectroscopic features that link directly to experiment, such as infra-red and Raman spectroscopies, NMR, and core level spectra.","title":"CASTEP"},{"location":"research-software/castep/castep/#useful-links","text":"CASTEP User Guides CASTEP Tutorials CASTEP Licensing","title":"Useful Links"},{"location":"research-software/castep/castep/#using-castep-on-archer2","text":"CASTEP is only available to users who have a valid CASTEP licence. If you have a CASTEP licence and wish to have access to CASTEP on ARCHER2, please make a request via the SAFE, see: How to request access to package groups Please have your license details to hand.","title":"Using CASTEP on ARCHER2"},{"location":"research-software/castep/castep/#running-parallel-castep-jobs","text":"The following script will run a CASTEP job using 2 nodes (256 cores). it assumes that the input files have the file stem text_calc . #!/bin/bash # Request 2 nodes with 128 MPI tasks per node for 20 minutes #SBATCH --job-name=CASTEP #SBATCH --nodes=2 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the batch environment module load epcc-job-env # Load the CASTEP module, avoid any unintentional OpenMP threading by # setting OMP_NUM_THREADS, and launch the code. module load castep export OMP_NUM_THREADS=1 srun -cpu-bind=cores castep.mpi test_calc","title":"Running parallel CASTEP jobs"},{"location":"research-software/castep/castep/#compiling-castep","text":"The latest instructions for building CASTEP on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for CASTEP on GitHub","title":"Compiling CASTEP"},{"location":"research-software/chemshell/chemshell/","text":"ChemShell Warning The ARCHER2 Service is not yet available. This documentation is in development. ChemShell is a script-based chemistry code focusing on hybrid QM/MM calculations with support for standard quantum chemical or force field calculations. There are two versions: an older Tcl-based version Tcl-ChemShell and a more recent python-based version Py-ChemShell. The advice from https://www.chemshell.org/licence on the difference is: We regard Py-ChemShell 19.0 as suitable for production calculations on materials systems, although you will find its feature set is more limited than Tcl-ChemShell. We do not currently recommend Py-ChemShell for calculations on biological systems, as automated import of biomolecular force fields is scheduled for a future release. Useful Links ChemShell home page https://www.chemshell.org ChemShell documentation https://www.chemshell.org/documentation ChemShell forums https://www.chemshell.org/forum Using ChemShell on ARCHER2 Warning ChemShell is not yet available on ARCHER2 The python-based version of ChemShell is open-source and is freely available to all users on ARCHER2. The older version of Tcl-based ChemShell requires a license. Users with a valid license should request access via the ARCHER2 SAFE. Running parallel ChemShell jobs Note Information on running ChemShell and example scripts will be added soon.","title":"Chemshell"},{"location":"research-software/chemshell/chemshell/#chemshell","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development. ChemShell is a script-based chemistry code focusing on hybrid QM/MM calculations with support for standard quantum chemical or force field calculations. There are two versions: an older Tcl-based version Tcl-ChemShell and a more recent python-based version Py-ChemShell. The advice from https://www.chemshell.org/licence on the difference is: We regard Py-ChemShell 19.0 as suitable for production calculations on materials systems, although you will find its feature set is more limited than Tcl-ChemShell. We do not currently recommend Py-ChemShell for calculations on biological systems, as automated import of biomolecular force fields is scheduled for a future release.","title":"ChemShell"},{"location":"research-software/chemshell/chemshell/#useful-links","text":"ChemShell home page https://www.chemshell.org ChemShell documentation https://www.chemshell.org/documentation ChemShell forums https://www.chemshell.org/forum","title":"Useful Links"},{"location":"research-software/chemshell/chemshell/#using-chemshell-on-archer2","text":"Warning ChemShell is not yet available on ARCHER2 The python-based version of ChemShell is open-source and is freely available to all users on ARCHER2. The older version of Tcl-based ChemShell requires a license. Users with a valid license should request access via the ARCHER2 SAFE.","title":"Using ChemShell on ARCHER2"},{"location":"research-software/chemshell/chemshell/#running-parallel-chemshell-jobs","text":"Note Information on running ChemShell and example scripts will be added soon.","title":"Running parallel ChemShell jobs"},{"location":"research-software/code-saturne/code-saturne/","text":"Code_Saturne Warning The ARCHER2 Service is not yet available. This documentation is in development. Code_Saturne solves the Navier-Stokes equations for 2D, 2D-axisymmetric and 3D flows, steady or unsteady, laminar or turbulent, incompressible or weakly dilatable, isothermal or not, with scalar transport if required. Several turbulence models are available, from Reynolds-averaged models to large-eddy simulation (LES) models. In addition, a number of specific physical models are also available as \"modules\": gas, coal and heavy-fuel oil combustion, semi-transparent radiative transfer, particle-tracking with Lagrangian modeling, Joule effect, electrics arcs, weakly compressible flows, atmospheric flows, rotor/stator interaction for hydraulic machines. Useful Links Code_Saturne home page https://www.code-saturne.org/cms/ Code_Saturne user guides https://www.code-saturne.org/cms/documentation/guides Code_Saturne users' forum https://www.code-saturne.org/forum/ Using Code_Saturne on ARCHER2 Code_Saturne is released under the GNU General Public Licence v2 and so is freely available to all users on ARCHER2. You can load Code_Saturne 6.0.5 for use by running the following command: module load code_saturne/6.0.5-gcc10 Running parallel Code_Saturne jobs After setting up a case it should be initialized by running the following command from the case directory, where setup.xml is the input file: code_saturne run --initialize --param setup.xml This will create a directory named for the current date and time (e.g. 20201019-1636) inside the RESU directory. Inside the new directory will be a script named run_solver . You may alter this to resemble the script below, or you may wish to simply create a new one with the contents shown. If you wish to alter the existing run_solver script you will need to add all the #SBATCH options shown to set the job name, size and so on. You should also add the two module commands, and srun --cpu-bind=cores as well as the --mpi option to the line executing ./cs_solver to ensure parallel execution on the compute nodes. The export LD_LIBRARY_PATH=... and cd commands are redundant and may be retained or removed. This script will run an MPI-only Code_Saturne job over 4 nodes (128 x 4 = 512 cores) for a maximum of 20 minutes. #!/bin/bash #SBATCH --export=none #SBATCH --job-name=CSExample #SBATCH --time=0:20:0 #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the batch environment module load epcc-job-env module load code_saturne # Prevent threading. export OMP_NUM_THREADS = 1 # Run solver. srun --cpu-bind = cores ./cs_solver --mpi $@ The script can then be submitted to the batch system with sbatch . Compiling Code_Saturne The latest instructions for building Code_Saturne on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for Code_Saturne on GitHub","title":"Code_Saturne"},{"location":"research-software/code-saturne/code-saturne/#code_saturne","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development. Code_Saturne solves the Navier-Stokes equations for 2D, 2D-axisymmetric and 3D flows, steady or unsteady, laminar or turbulent, incompressible or weakly dilatable, isothermal or not, with scalar transport if required. Several turbulence models are available, from Reynolds-averaged models to large-eddy simulation (LES) models. In addition, a number of specific physical models are also available as \"modules\": gas, coal and heavy-fuel oil combustion, semi-transparent radiative transfer, particle-tracking with Lagrangian modeling, Joule effect, electrics arcs, weakly compressible flows, atmospheric flows, rotor/stator interaction for hydraulic machines.","title":"Code_Saturne"},{"location":"research-software/code-saturne/code-saturne/#useful-links","text":"Code_Saturne home page https://www.code-saturne.org/cms/ Code_Saturne user guides https://www.code-saturne.org/cms/documentation/guides Code_Saturne users' forum https://www.code-saturne.org/forum/","title":"Useful Links"},{"location":"research-software/code-saturne/code-saturne/#using-code_saturne-on-archer2","text":"Code_Saturne is released under the GNU General Public Licence v2 and so is freely available to all users on ARCHER2. You can load Code_Saturne 6.0.5 for use by running the following command: module load code_saturne/6.0.5-gcc10","title":"Using Code_Saturne on ARCHER2"},{"location":"research-software/code-saturne/code-saturne/#running-parallel-code_saturne-jobs","text":"After setting up a case it should be initialized by running the following command from the case directory, where setup.xml is the input file: code_saturne run --initialize --param setup.xml This will create a directory named for the current date and time (e.g. 20201019-1636) inside the RESU directory. Inside the new directory will be a script named run_solver . You may alter this to resemble the script below, or you may wish to simply create a new one with the contents shown. If you wish to alter the existing run_solver script you will need to add all the #SBATCH options shown to set the job name, size and so on. You should also add the two module commands, and srun --cpu-bind=cores as well as the --mpi option to the line executing ./cs_solver to ensure parallel execution on the compute nodes. The export LD_LIBRARY_PATH=... and cd commands are redundant and may be retained or removed. This script will run an MPI-only Code_Saturne job over 4 nodes (128 x 4 = 512 cores) for a maximum of 20 minutes. #!/bin/bash #SBATCH --export=none #SBATCH --job-name=CSExample #SBATCH --time=0:20:0 #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the batch environment module load epcc-job-env module load code_saturne # Prevent threading. export OMP_NUM_THREADS = 1 # Run solver. srun --cpu-bind = cores ./cs_solver --mpi $@ The script can then be submitted to the batch system with sbatch .","title":"Running parallel Code_Saturne jobs"},{"location":"research-software/code-saturne/code-saturne/#compiling-code_saturne","text":"The latest instructions for building Code_Saturne on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for Code_Saturne on GitHub","title":"Compiling Code_Saturne"},{"location":"research-software/cp2k/cp2k/","text":"CP2K CP2K is a quantum chemistry and solid state physics software package that can perform atomistic simulations of solid state, liquid, molecular, periodic, material, crystal, and biological systems. CP2K provides a general framework for different modelling methods such as DFT using the mixed Gaussian and plane waves approaches GPW and GAPW. Supported theory levels include DFTB, LDA, GGA, MP2, RPA, semi-empirical methods (AM1, PM3, PM6, RM1, MNDO), and classical force fields (AMBER, CHARMM). CP2K can do simulations of molecular dynamics, metadynamics, Monte Carlo, Ehrenfest dynamics, vibrational analysis, core level spectroscopy, energy minimisation, and transition state optimisation using NEB or dimer method. Useful links CP2K Reference Manual CP2K HOWTOs CP2K FAQs Using CP2K on ARCHER2 CP2K is available through the cp2k module. MPI only cp2k.popt and MPI/OpenMP Hybrid cp2k.psmp binaries are available. For ARCHER2, CP2K has been compiled with the following optional features: FFTW for fast Fourier transforms, libint to enable methods including Hartree-Fock exchange, libxsmm for efficient small matrix multiplications, libxc to provide a wider choice of exchange-correlation functionals, ELPA for improved performance of matrix diagonalisation, PLUMED to allow enhanced sampling methods, and SIRIUS for plane wave computations. See CP2K compile instructions for a full list of optional features. If there is an optional feature not available, and which you would like, please contact the Service Desk . Experts may also wish to compile their own versions of the code (see below for instructions). Running parallel CP2K jobs MPI only jobs To run CP2K using MPI only, load the cp2k module and use the cp2k.popt executable. For example, the following script will run a CP2K job using 4 nodes (128x4 cores): #!/bin/bash # Request 4 nodes using 128 cores per node for 128 MPI tasks per node. #SBATCH --job-name=CP2K_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the batch environment module load epcc-job-env module load cp2k export OMP_NUM_THREADS=1 srun cp2k.popt -i MYINPUT.inp MPI/OpenMP hybrid jobs To run CP2K using MPI and OpenMP, load the cp2k module and use the cp2k.psmp executable. #!/bin/bash # Request 4 nodes with 16 MPI tasks per node each using 8 threads; # note this means 128 MPI tasks in total. # Remember to replace [budget code] below with your account code, # e.g. '--account=t01'. #SBATCH --job-name=CP2K_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=16 #SBATCH --cpus-per-task=8 #SBATCH --time=00:20:00 #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the relevant CP2K module # Ensure OMP_NUM_THREADS is consistent with cpus-per-task above # Launch the executable module -s restore /etc/cray-pe.d/PrgEnv-gnu module load cp2k export OMP_NUM_THREADS=8 srun cp2k.psmp -i MYINPUT.inp Compiling CP2K The latest instructions for building CP2K on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for CP2K on GitHub","title":"CP2K"},{"location":"research-software/cp2k/cp2k/#cp2k","text":"CP2K is a quantum chemistry and solid state physics software package that can perform atomistic simulations of solid state, liquid, molecular, periodic, material, crystal, and biological systems. CP2K provides a general framework for different modelling methods such as DFT using the mixed Gaussian and plane waves approaches GPW and GAPW. Supported theory levels include DFTB, LDA, GGA, MP2, RPA, semi-empirical methods (AM1, PM3, PM6, RM1, MNDO), and classical force fields (AMBER, CHARMM). CP2K can do simulations of molecular dynamics, metadynamics, Monte Carlo, Ehrenfest dynamics, vibrational analysis, core level spectroscopy, energy minimisation, and transition state optimisation using NEB or dimer method.","title":"CP2K"},{"location":"research-software/cp2k/cp2k/#useful-links","text":"CP2K Reference Manual CP2K HOWTOs CP2K FAQs","title":"Useful links"},{"location":"research-software/cp2k/cp2k/#using-cp2k-on-archer2","text":"CP2K is available through the cp2k module. MPI only cp2k.popt and MPI/OpenMP Hybrid cp2k.psmp binaries are available. For ARCHER2, CP2K has been compiled with the following optional features: FFTW for fast Fourier transforms, libint to enable methods including Hartree-Fock exchange, libxsmm for efficient small matrix multiplications, libxc to provide a wider choice of exchange-correlation functionals, ELPA for improved performance of matrix diagonalisation, PLUMED to allow enhanced sampling methods, and SIRIUS for plane wave computations. See CP2K compile instructions for a full list of optional features. If there is an optional feature not available, and which you would like, please contact the Service Desk . Experts may also wish to compile their own versions of the code (see below for instructions).","title":"Using CP2K on ARCHER2"},{"location":"research-software/cp2k/cp2k/#running-parallel-cp2k-jobs","text":"","title":"Running parallel CP2K jobs"},{"location":"research-software/cp2k/cp2k/#mpi-only-jobs","text":"To run CP2K using MPI only, load the cp2k module and use the cp2k.popt executable. For example, the following script will run a CP2K job using 4 nodes (128x4 cores): #!/bin/bash # Request 4 nodes using 128 cores per node for 128 MPI tasks per node. #SBATCH --job-name=CP2K_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the batch environment module load epcc-job-env module load cp2k export OMP_NUM_THREADS=1 srun cp2k.popt -i MYINPUT.inp","title":"MPI only jobs"},{"location":"research-software/cp2k/cp2k/#mpiopenmp-hybrid-jobs","text":"To run CP2K using MPI and OpenMP, load the cp2k module and use the cp2k.psmp executable. #!/bin/bash # Request 4 nodes with 16 MPI tasks per node each using 8 threads; # note this means 128 MPI tasks in total. # Remember to replace [budget code] below with your account code, # e.g. '--account=t01'. #SBATCH --job-name=CP2K_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=16 #SBATCH --cpus-per-task=8 #SBATCH --time=00:20:00 #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Load the relevant CP2K module # Ensure OMP_NUM_THREADS is consistent with cpus-per-task above # Launch the executable module -s restore /etc/cray-pe.d/PrgEnv-gnu module load cp2k export OMP_NUM_THREADS=8 srun cp2k.psmp -i MYINPUT.inp","title":"MPI/OpenMP hybrid jobs"},{"location":"research-software/cp2k/cp2k/#compiling-cp2k","text":"The latest instructions for building CP2K on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for CP2K on GitHub","title":"Compiling CP2K"},{"location":"research-software/elk/elk/","text":"ELK Warning The ARCHER2 Service is not yet available. This documentation is in development. ELK is an all-electron full-potential linearised augmented-plane wave (FP-LAPW) code with many advanced features. It was written originally at Karl-Franzens-Universitt Graz as a milestone of the EXCITING EU Research and Training Network. Useful Links ELK home page http://elk.sourceforge.net ELK documentation http://elk.sourceforge.net/#documentation Using ELK on ARCHER2 ELK is freely available to all users on ARCHER2. Running parallel ELK jobs Example MPI ELK job The following script will run an ELK job on 4 nodes (512 cores). #!/bin/bash # Request 512 MPI tasks (4 nodes at 128 tasks per node) with a # maximum wall clock time limit of 20 minutes. #SBATCH --job-name=elk_job #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the batch environment module load epcc-job-env module load elk srun --cpu-bind=cores elk Example mixed MPI/OpenMP ELK job The following script will run an ELK job on 4 nodes, using 8 OpenMP threads and 16 MPI tasks per node. #!/bin/bash # Request 4 nodes (using 8 threads and 16 MPI tasks per node) with a # maximum wall clock time limit of 20 minutes. # Replace [budget code] with your account code. #SBATCH --job-name=elk_job #SBATCH --nodes=4 #SBATCH --tasks-per-node=16 #SBATCH --cpus-per-task=8 #SBATCH --time=00:20:00 #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard export OMP_NUM_THREADS=8 # Load the elk module # Launch the executable # Input filename elk.in module -s restore /etc/cray-pe.d/PrgEnv-gnu module load elk srun elk Compiling ELK The latest instructions for building ELK on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for ELK on GitHub","title":"ELK"},{"location":"research-software/elk/elk/#elk","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development. ELK is an all-electron full-potential linearised augmented-plane wave (FP-LAPW) code with many advanced features. It was written originally at Karl-Franzens-Universitt Graz as a milestone of the EXCITING EU Research and Training Network.","title":"ELK"},{"location":"research-software/elk/elk/#useful-links","text":"ELK home page http://elk.sourceforge.net ELK documentation http://elk.sourceforge.net/#documentation","title":"Useful Links"},{"location":"research-software/elk/elk/#using-elk-on-archer2","text":"ELK is freely available to all users on ARCHER2.","title":"Using ELK on ARCHER2"},{"location":"research-software/elk/elk/#running-parallel-elk-jobs","text":"","title":"Running parallel ELK jobs"},{"location":"research-software/elk/elk/#example-mpi-elk-job","text":"The following script will run an ELK job on 4 nodes (512 cores). #!/bin/bash # Request 512 MPI tasks (4 nodes at 128 tasks per node) with a # maximum wall clock time limit of 20 minutes. #SBATCH --job-name=elk_job #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the batch environment module load epcc-job-env module load elk srun --cpu-bind=cores elk","title":"Example MPI ELK job"},{"location":"research-software/elk/elk/#example-mixed-mpiopenmp-elk-job","text":"The following script will run an ELK job on 4 nodes, using 8 OpenMP threads and 16 MPI tasks per node. #!/bin/bash # Request 4 nodes (using 8 threads and 16 MPI tasks per node) with a # maximum wall clock time limit of 20 minutes. # Replace [budget code] with your account code. #SBATCH --job-name=elk_job #SBATCH --nodes=4 #SBATCH --tasks-per-node=16 #SBATCH --cpus-per-task=8 #SBATCH --time=00:20:00 #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard export OMP_NUM_THREADS=8 # Load the elk module # Launch the executable # Input filename elk.in module -s restore /etc/cray-pe.d/PrgEnv-gnu module load elk srun elk","title":"Example mixed MPI/OpenMP ELK job"},{"location":"research-software/elk/elk/#compiling-elk","text":"The latest instructions for building ELK on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for ELK on GitHub","title":"Compiling ELK"},{"location":"research-software/fenics/fenics/","text":"FEniCS Warning The ARCHER2 Service is not yet available. This documentation is in development. FEniCS is an open-source (LGPLv3) computing platform for solving partial differential equations (PDEs). FEniCS enables users to translate scientific models into efficient finite element code. With the high-level Python and C++ interfaces to FEniCS, it is easy to get started, but FEniCS also offers powerful capabilities for more experienced programmers. Useful Links FEniCS home page https://fenicsproject.org FEniCS documentation https://fenicsproject.org/documentation/ Using FEniCS on ARCHER2 Warning FEniCS is not yet available on ARCHER2","title":"FEniCS"},{"location":"research-software/fenics/fenics/#fenics","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development. FEniCS is an open-source (LGPLv3) computing platform for solving partial differential equations (PDEs). FEniCS enables users to translate scientific models into efficient finite element code. With the high-level Python and C++ interfaces to FEniCS, it is easy to get started, but FEniCS also offers powerful capabilities for more experienced programmers.","title":"FEniCS"},{"location":"research-software/fenics/fenics/#useful-links","text":"FEniCS home page https://fenicsproject.org FEniCS documentation https://fenicsproject.org/documentation/","title":"Useful Links"},{"location":"research-software/fenics/fenics/#using-fenics-on-archer2","text":"Warning FEniCS is not yet available on ARCHER2","title":"Using FEniCS on ARCHER2"},{"location":"research-software/gromacs/gromacs/","text":"GROMACS Warning The ARCHER2 Service is not yet available. This documentation is in development. GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. It is primarily designed for biochemical molecules like proteins, lipids and nucleic acids that have a lot of complicated bonded interactions, but since GROMACS is extremely fast at calculating the nonbonded interactions (that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers. Useful Links GROMACS User Guides GROMACS Tutorials Using GROMACS on ARCHER2 GROMACS is Open Source software and is freely available to all users. Two versions are available: Parallel MPI/OpenMP, single precision: gmx_mpi Parallel MPI/OpenMP, double precision: gmx_mpi_d Running parallel GROMACS jobs Running MPI only jobs The following script will run a GROMACS MD job using 4 nodes (128x4 cores) with pure MPI. #!/bin/bash #SBATCH --job-name=mdrun_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the batch environment module load epcc-job-env module load gromacs export OMP_NUM_THREADS=1 srun --cpu-bind=cores gmx_mpi mdrun -s test_calc.tpr Running hybrid MPI/OpenMP jobs The following script will run a GROMACS MD job using 4 nodes (128x4 cores) with 6 MPI processes per node (24 MPI processes in total) and 6 OpenMP threads per MPI process. #!/bin/bash #SBATCH --job-name=mdrun_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=16 #SBATCH --cpus-per-task=8 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the batch environment module load epcc-job-env module load gromacs export OMP_NUM_THREADS=8 srun --hint=nomultithread --distribution=block:block gmx_mpi mdrun -s test_calc.tpr Compiling Gromacs The latest instructions for building GROMACS on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for GROMACS on GitHub","title":"GROMACS"},{"location":"research-software/gromacs/gromacs/#gromacs","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development. GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. It is primarily designed for biochemical molecules like proteins, lipids and nucleic acids that have a lot of complicated bonded interactions, but since GROMACS is extremely fast at calculating the nonbonded interactions (that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers.","title":"GROMACS"},{"location":"research-software/gromacs/gromacs/#useful-links","text":"GROMACS User Guides GROMACS Tutorials","title":"Useful Links"},{"location":"research-software/gromacs/gromacs/#using-gromacs-on-archer2","text":"GROMACS is Open Source software and is freely available to all users. Two versions are available: Parallel MPI/OpenMP, single precision: gmx_mpi Parallel MPI/OpenMP, double precision: gmx_mpi_d","title":"Using GROMACS on ARCHER2"},{"location":"research-software/gromacs/gromacs/#running-parallel-gromacs-jobs","text":"","title":"Running parallel GROMACS jobs"},{"location":"research-software/gromacs/gromacs/#running-mpi-only-jobs","text":"The following script will run a GROMACS MD job using 4 nodes (128x4 cores) with pure MPI. #!/bin/bash #SBATCH --job-name=mdrun_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the batch environment module load epcc-job-env module load gromacs export OMP_NUM_THREADS=1 srun --cpu-bind=cores gmx_mpi mdrun -s test_calc.tpr","title":"Running MPI only jobs"},{"location":"research-software/gromacs/gromacs/#running-hybrid-mpiopenmp-jobs","text":"The following script will run a GROMACS MD job using 4 nodes (128x4 cores) with 6 MPI processes per node (24 MPI processes in total) and 6 OpenMP threads per MPI process. #!/bin/bash #SBATCH --job-name=mdrun_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=16 #SBATCH --cpus-per-task=8 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the batch environment module load epcc-job-env module load gromacs export OMP_NUM_THREADS=8 srun --hint=nomultithread --distribution=block:block gmx_mpi mdrun -s test_calc.tpr","title":"Running hybrid MPI/OpenMP jobs"},{"location":"research-software/gromacs/gromacs/#compiling-gromacs","text":"The latest instructions for building GROMACS on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for GROMACS on GitHub","title":"Compiling Gromacs"},{"location":"research-software/lammps/lammps/","text":"LAMMPS Warning The ARCHER2 Service is not yet available. This documentation is in development. LAMMPS , is a classical molecular dynamics code, (\"Large-scale Atomic/Molecular Massively Parallel Simulator\"). LAMMPS has potentials for solid-state materials (metals, semiconductors) and soft matter (biomolecules, polymers) and coarse-grained or mesoscopic systems. It can be used to model atoms or, more generically, as a parallel particle simulator at the atomic, mesoscopic, or continuum scale. Useful Links LAMMPS Documentation https://lammps.sandia.gov/doc/Manual.html LAMMPS Mailing list details https://lammps.sandia.gov/mail.html Using LAMMPS on ARCHER2 LAMMPS is freely available to all ARCHER2 users. The centrally installed version of LAMMPS is compiled with all the standard packages included: ASPHERE , BODY , CLASS2 , COLLOID , COMPRESS , CORESHELL , DIPOLE , GRANULAR , KSPACE , MANYBODY , MC , MISC , MOLECULE , OPT , PERI , QEQ , REPLICA , RIGID , SHOCK , SNAP , SRD . We do not install any USER packages. If you are interested in a USER package, we would encourage you to try to compile your own version and we can help out if necessary (see below). Running parallel LAMMPS jobs LAMMPS can exploit multiple nodes on ARCHER2 and will generally be run in exclusive mode using more than one node. For example, the following script will run a LAMMPS MD job using 4 nodes (128x4 cores) with MPI only. #!/bin/bash #SBATCH --job-name=lammps_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env module load lammps srun --cpu-bind=cores lmp -i in.test -o out.test Compiling LAMMPS The large range of optional packages available for LAMMPS, and opportunity for extensibility, may mean that it is convenient for users to compile their own copy. In practice, LAMMPS is relatively easy to compile, so we encourage users to have a go. Compilation instructions for LAMMPS on ARCHER2 can be found on GitHub: Build instructions for LAMMPS on GitHub","title":"LAMMPS"},{"location":"research-software/lammps/lammps/#lammps","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development. LAMMPS , is a classical molecular dynamics code, (\"Large-scale Atomic/Molecular Massively Parallel Simulator\"). LAMMPS has potentials for solid-state materials (metals, semiconductors) and soft matter (biomolecules, polymers) and coarse-grained or mesoscopic systems. It can be used to model atoms or, more generically, as a parallel particle simulator at the atomic, mesoscopic, or continuum scale.","title":"LAMMPS"},{"location":"research-software/lammps/lammps/#useful-links","text":"LAMMPS Documentation https://lammps.sandia.gov/doc/Manual.html LAMMPS Mailing list details https://lammps.sandia.gov/mail.html","title":"Useful Links"},{"location":"research-software/lammps/lammps/#using-lammps-on-archer2","text":"LAMMPS is freely available to all ARCHER2 users. The centrally installed version of LAMMPS is compiled with all the standard packages included: ASPHERE , BODY , CLASS2 , COLLOID , COMPRESS , CORESHELL , DIPOLE , GRANULAR , KSPACE , MANYBODY , MC , MISC , MOLECULE , OPT , PERI , QEQ , REPLICA , RIGID , SHOCK , SNAP , SRD . We do not install any USER packages. If you are interested in a USER package, we would encourage you to try to compile your own version and we can help out if necessary (see below).","title":"Using LAMMPS on ARCHER2"},{"location":"research-software/lammps/lammps/#running-parallel-lammps-jobs","text":"LAMMPS can exploit multiple nodes on ARCHER2 and will generally be run in exclusive mode using more than one node. For example, the following script will run a LAMMPS MD job using 4 nodes (128x4 cores) with MPI only. #!/bin/bash #SBATCH --job-name=lammps_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env module load lammps srun --cpu-bind=cores lmp -i in.test -o out.test","title":"Running parallel LAMMPS jobs"},{"location":"research-software/lammps/lammps/#compiling-lammps","text":"The large range of optional packages available for LAMMPS, and opportunity for extensibility, may mean that it is convenient for users to compile their own copy. In practice, LAMMPS is relatively easy to compile, so we encourage users to have a go. Compilation instructions for LAMMPS on ARCHER2 can be found on GitHub: Build instructions for LAMMPS on GitHub","title":"Compiling LAMMPS"},{"location":"research-software/mitgcm/mitgcm/","text":"MITgcm Warning The ARCHER2 Service is not yet available. This documentation is in development. The Massachusetts Institute of Technology General Circulation Model (MITgcm) is a numerical model designed for study of the atmosphere, ocean, and climate. MITgcm's flexible non-hydrostatic formulation enables it to simulate fluid phenomena over a wide range of scales; its adjoint capabilities enable it to be applied to sensitivity questions and to parameter and state estimation problems. By employing fluid equation isomorphisms, a single dynamical kernel can be used to simulate flow of both the atmosphere and ocean. Useful Links MITgcm home page http://mitgcm.org MITgcm documentation https://mitgcm.readthedocs.io/en/latest/ Building MITgcm on ARCHER2 MITgcm is not available via a module on ARCHER2 as users will build their own executables specific to the problem they are working on. However, we do provide an optfile which will allow genmake2 to create Makefiles which will work on ARCHER2. You can obtain the MITgcm source code from the developers by cloning from the GitHub repository with the command git clone https://github.com/MITgcm/MITgcm.git You should then copy the ARCHER2 optfile into the MITgcm directories: cp /work/y07/shared/mitgcm/optfile/linux_amd64_gfortran_archer2 MITgcm/tools/build_options/ When you are building your code with this optfile, use the GNU environment with module restore PrgEnv-gnu You should also set the following environment variables. MITGCM_ROOTDIR is used to locate the source code and should point to the top MITgcm directory. Optionally, adding the MITgcm tools directory to your PATH environment variable makes it easier to use tools such as genmake2, and the MITGCM_OPT environment variable makes it easier to refer to pass the optfile to genmake2. export MITGCM_ROOTDIR=/path/to/MITgcm export PATH=$MITGCM_ROOTDIR/tools:$PATH export MITGCM_OPT=$MITGCM_ROOTDIR/tools/build_options/linux_amd64_gfortran_archer2 When using genmake2 to create the Makefile, you will need to specify the optfile to use. Other commonly used options might be to use extra source code with the -mods option, and to enable MPI with -mpi . You might then run a command that resembles the following: genmake2 -mods /path/to/additional/source -mpi -optfile $MITGCM_OPT You can read about the full set of options available to genmake2 by running genmake2 -help Finally, you may then build your executable with the make depend , make commands. Running MITgcm on ARCHER2 Once you have built your executable you can write a script like the following which will allow it to run on the ARCHER2 compute nodes. This example would run a pure MPI MITgcm simulation over 2 nodes of 128 cores each for up to one hour. #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=MITgcm-simulation #SBATCH --time=1:0:0 #SBATCH --nodes=2 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env # Set the number of threads to 1 # This prevents any threaded system libraries from automatically # using threading. export OMP_NUM_THREADS=1 # Launch the parallel job # Using 256 MPI processes and 128 MPI processes per node # srun picks up the distribution from the sbatch options srun --cpu-bind=cores ./mitgcmuv","title":"MITgcm"},{"location":"research-software/mitgcm/mitgcm/#mitgcm","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development. The Massachusetts Institute of Technology General Circulation Model (MITgcm) is a numerical model designed for study of the atmosphere, ocean, and climate. MITgcm's flexible non-hydrostatic formulation enables it to simulate fluid phenomena over a wide range of scales; its adjoint capabilities enable it to be applied to sensitivity questions and to parameter and state estimation problems. By employing fluid equation isomorphisms, a single dynamical kernel can be used to simulate flow of both the atmosphere and ocean.","title":"MITgcm"},{"location":"research-software/mitgcm/mitgcm/#useful-links","text":"MITgcm home page http://mitgcm.org MITgcm documentation https://mitgcm.readthedocs.io/en/latest/","title":"Useful Links"},{"location":"research-software/mitgcm/mitgcm/#building-mitgcm-on-archer2","text":"MITgcm is not available via a module on ARCHER2 as users will build their own executables specific to the problem they are working on. However, we do provide an optfile which will allow genmake2 to create Makefiles which will work on ARCHER2. You can obtain the MITgcm source code from the developers by cloning from the GitHub repository with the command git clone https://github.com/MITgcm/MITgcm.git You should then copy the ARCHER2 optfile into the MITgcm directories: cp /work/y07/shared/mitgcm/optfile/linux_amd64_gfortran_archer2 MITgcm/tools/build_options/ When you are building your code with this optfile, use the GNU environment with module restore PrgEnv-gnu You should also set the following environment variables. MITGCM_ROOTDIR is used to locate the source code and should point to the top MITgcm directory. Optionally, adding the MITgcm tools directory to your PATH environment variable makes it easier to use tools such as genmake2, and the MITGCM_OPT environment variable makes it easier to refer to pass the optfile to genmake2. export MITGCM_ROOTDIR=/path/to/MITgcm export PATH=$MITGCM_ROOTDIR/tools:$PATH export MITGCM_OPT=$MITGCM_ROOTDIR/tools/build_options/linux_amd64_gfortran_archer2 When using genmake2 to create the Makefile, you will need to specify the optfile to use. Other commonly used options might be to use extra source code with the -mods option, and to enable MPI with -mpi . You might then run a command that resembles the following: genmake2 -mods /path/to/additional/source -mpi -optfile $MITGCM_OPT You can read about the full set of options available to genmake2 by running genmake2 -help Finally, you may then build your executable with the make depend , make commands.","title":"Building MITgcm on ARCHER2"},{"location":"research-software/mitgcm/mitgcm/#running-mitgcm-on-archer2","text":"Once you have built your executable you can write a script like the following which will allow it to run on the ARCHER2 compute nodes. This example would run a pure MPI MITgcm simulation over 2 nodes of 128 cores each for up to one hour. #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=MITgcm-simulation #SBATCH --time=1:0:0 #SBATCH --nodes=2 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env # Set the number of threads to 1 # This prevents any threaded system libraries from automatically # using threading. export OMP_NUM_THREADS=1 # Launch the parallel job # Using 256 MPI processes and 128 MPI processes per node # srun picks up the distribution from the sbatch options srun --cpu-bind=cores ./mitgcmuv","title":"Running MITgcm on ARCHER2"},{"location":"research-software/mo-unified-model/mo-unified-model/","text":"Met Office Unified Model Warning The ARCHER2 Service is not yet available. This documentation is in development. The Met Office Unified Model (\"the UM\") is a numerical model of the atmosphere used for both weather and climate applications. It is often rcoupled to the NEMO ocean model using the OASIS couplign framework to provide a full Earth system model. Useful Links Met Office Unified Model home page https://www.metoffice.gov.uk/research/approach/modelling-systems/unified-model/index Using the UM Information on using the UM is provided by the NCAS Computational Modelling Service (CMS) .","title":"Met Office Unified Model"},{"location":"research-software/mo-unified-model/mo-unified-model/#met-office-unified-model","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development. The Met Office Unified Model (\"the UM\") is a numerical model of the atmosphere used for both weather and climate applications. It is often rcoupled to the NEMO ocean model using the OASIS couplign framework to provide a full Earth system model.","title":"Met Office Unified Model"},{"location":"research-software/mo-unified-model/mo-unified-model/#useful-links","text":"Met Office Unified Model home page https://www.metoffice.gov.uk/research/approach/modelling-systems/unified-model/index","title":"Useful Links"},{"location":"research-software/mo-unified-model/mo-unified-model/#using-the-um","text":"Information on using the UM is provided by the NCAS Computational Modelling Service (CMS) .","title":"Using the UM"},{"location":"research-software/namd/namd/","text":"NAMD Warning The ARCHER2 Service is not yet available. This documentation is in development. NAMD , recipient of a 2002 Gordon Bell Award and a 2012 Sidney Fernbach Award, is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. Based on Charm++ parallel objects, NAMD scales to hundreds of cores for typical simulations and beyond 500,000 cores for the largest simulations. NAMD uses the popular molecular graphics program VMD for simulation setup and trajectory analysis, but is also file-compatible with AMBER, CHARMM, and X-PLOR. Useful Links NAMD User Guide NAMD Tutorials Using NAMD on ARCHER2 NAMD is freely available to all ARCHER2 users. Running parallel NAMD jobs The following script will run a NAMD MD job using 4 nodes (128x4 cores) with MPI. #!/bin/bash # Request four nodes to run a job of 512 MPI tasks with 128 MPI # tasks per node, here for maximum time 20 minutes. #SBATCH --job-name=namd_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-core=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env module load namd srun --cpu-bind=cores namd2 input.namd","title":"NAMD"},{"location":"research-software/namd/namd/#namd","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development. NAMD , recipient of a 2002 Gordon Bell Award and a 2012 Sidney Fernbach Award, is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. Based on Charm++ parallel objects, NAMD scales to hundreds of cores for typical simulations and beyond 500,000 cores for the largest simulations. NAMD uses the popular molecular graphics program VMD for simulation setup and trajectory analysis, but is also file-compatible with AMBER, CHARMM, and X-PLOR.","title":"NAMD"},{"location":"research-software/namd/namd/#useful-links","text":"NAMD User Guide NAMD Tutorials","title":"Useful Links"},{"location":"research-software/namd/namd/#using-namd-on-archer2","text":"NAMD is freely available to all ARCHER2 users.","title":"Using NAMD on ARCHER2"},{"location":"research-software/namd/namd/#running-parallel-namd-jobs","text":"The following script will run a NAMD MD job using 4 nodes (128x4 cores) with MPI. #!/bin/bash # Request four nodes to run a job of 512 MPI tasks with 128 MPI # tasks per node, here for maximum time 20 minutes. #SBATCH --job-name=namd_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-core=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env module load namd srun --cpu-bind=cores namd2 input.namd","title":"Running parallel NAMD jobs"},{"location":"research-software/nektarplusplus/nektarplusplus/","text":"NEKTAR++ Warning The ARCHER2 Service is not yet available. This documentation is in development. Nektar++ is a tensor product based finite element package designed to allow one to construct efficient classical low polynomial order h -type solvers (where h is the size of the finite element) as well as higher p -order piecewise polynomial order solvers. The Nektar++ framework comes with a number of solvers and also allows one to construct a variety of new solvers. Users can therefore use Nektar++ just to run simulations, or to extend and/or develop new functionality. Useful Links Nektar++ home page https://www.nektar.info Nektar++ tutorials https://www.nektar.info/community/tutorials/ Nektar gitlab repository https://gitlab.nektar.info/nektar Using Nektar++ on ARCHER2 Warning Nektar++ is not yet available on ARCHER2. Nektar++ is released under an MIT license and is available to all users on ARCHER2. Where can I get help? Specific issues with Nektar++ itself might be submitted to the issue tracker at the Nektar++ gitlab repository (see link above). More general questions might also be directed to the Nektar-users mailing list https://mailman.ic.ac.uk/mailman/listinfo/nektar-users . Issues specific to the use or behaviour of Netkar++ on ARCHER2 should be sent to the Service Desk. Running parallel Nektar++ jobs Note We will add information about running Nektar++ and example job submission scripts once the software is available.","title":"Nektar++"},{"location":"research-software/nektarplusplus/nektarplusplus/#nektar","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development. Nektar++ is a tensor product based finite element package designed to allow one to construct efficient classical low polynomial order h -type solvers (where h is the size of the finite element) as well as higher p -order piecewise polynomial order solvers. The Nektar++ framework comes with a number of solvers and also allows one to construct a variety of new solvers. Users can therefore use Nektar++ just to run simulations, or to extend and/or develop new functionality.","title":"NEKTAR++"},{"location":"research-software/nektarplusplus/nektarplusplus/#useful-links","text":"Nektar++ home page https://www.nektar.info Nektar++ tutorials https://www.nektar.info/community/tutorials/ Nektar gitlab repository https://gitlab.nektar.info/nektar","title":"Useful Links"},{"location":"research-software/nektarplusplus/nektarplusplus/#using-nektar-on-archer2","text":"Warning Nektar++ is not yet available on ARCHER2. Nektar++ is released under an MIT license and is available to all users on ARCHER2.","title":"Using Nektar++ on ARCHER2"},{"location":"research-software/nektarplusplus/nektarplusplus/#where-can-i-get-help","text":"Specific issues with Nektar++ itself might be submitted to the issue tracker at the Nektar++ gitlab repository (see link above). More general questions might also be directed to the Nektar-users mailing list https://mailman.ic.ac.uk/mailman/listinfo/nektar-users . Issues specific to the use or behaviour of Netkar++ on ARCHER2 should be sent to the Service Desk.","title":"Where can I get help?"},{"location":"research-software/nektarplusplus/nektarplusplus/#running-parallel-nektar-jobs","text":"Note We will add information about running Nektar++ and example job submission scripts once the software is available.","title":"Running parallel Nektar++ jobs"},{"location":"research-software/nemo/nemo/","text":"NEMO Warning The ARCHER2 Service is not yet available. This documentation is in development. NEMO (Nucleus for European Modelling of the Ocean) is a state-of-the-art framework for research activities and forecasting services in ocean and climate sciences, developed in a sustainable way by a European consortium. Useful Links The NEMO home page https://www.nemo-ocean.eu NEMO documentation https://forge.ipsl.jussieu.fr/nemo/chrome/site/doc/NEMO/guide/html/NEMO_guide.html NEMO users' area http://forge.ipsl.jussieu.fr/nemo/wiki/Users Using NEMO on ARCHER2 Warning A central install of NEMO is not yet available on ARCHER2. NEMO is released under a CeCILL license and if freely available to all users on ARCHER2. Running parallel NEMO jobs Note We will add information on running NEMO on ARCHER2 and job submission scripts once the software is available.","title":"NEMO"},{"location":"research-software/nemo/nemo/#nemo","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development. NEMO (Nucleus for European Modelling of the Ocean) is a state-of-the-art framework for research activities and forecasting services in ocean and climate sciences, developed in a sustainable way by a European consortium.","title":"NEMO"},{"location":"research-software/nemo/nemo/#useful-links","text":"The NEMO home page https://www.nemo-ocean.eu NEMO documentation https://forge.ipsl.jussieu.fr/nemo/chrome/site/doc/NEMO/guide/html/NEMO_guide.html NEMO users' area http://forge.ipsl.jussieu.fr/nemo/wiki/Users","title":"Useful Links"},{"location":"research-software/nemo/nemo/#using-nemo-on-archer2","text":"Warning A central install of NEMO is not yet available on ARCHER2. NEMO is released under a CeCILL license and if freely available to all users on ARCHER2.","title":"Using NEMO on ARCHER2"},{"location":"research-software/nemo/nemo/#running-parallel-nemo-jobs","text":"Note We will add information on running NEMO on ARCHER2 and job submission scripts once the software is available.","title":"Running parallel NEMO jobs"},{"location":"research-software/nwchem/nwchem/","text":"NWChem Warning The ARCHER2 Service is not yet available. This documentation is in development. NWChem aims to provide its users with computational chemistry tools that are scalable both in their ability to treat large scientific computational chemistry problems efficiently, and in their use of available parallel computing resources from high-performance parallel supercomputers to conventional workstation clusters. The NWChem software can handle: biomolecules, nanostructures, and solid-state system; from quantum to classical, and all combinations; Gaussian basis functions or plane-waves; scaling from one to thousands of processors; properties and relativity. Useful Links NWChem home page https://nwchemgit.github.io/ NWChem documentation https://nwchemgit.github.io/Home.html NWChem forum https://nwchemgit.github.io/Forum.html Using NWChem on ARCHER2 NWChem is released under an Educational Community License (ECL 2.0) and is freely available to all users on ARCHER2. Where can I get help? If you have problems accessing or running NWChem on ARCHER2, please contact the Service Desk. General questions on the use of NWChem might also be directed to the NWChem forum (see link above). More experienced users with detailed technical issues on NWChem should consider submitting them to the NWChem github issue tracker https://github.com/nwchemgit/nwchem/issues . Running NWChem jobs The following script will run a NWChem job using 2 nodes (256 cores) in the standard partition. It assumes that the input file is called test\\_calc.nw . #!/bin/bash # Request 2 nodes with 128 MPI tasks per node for 20 minutes # Replace [budget code] below with your account code, # e.g. '--account=t01' #SBATCH --job-name=CASTEP #SBATCH --nodes=2 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env # Load the NWChem module, avoid any unintentional OpenMP threading by # setting OMP_NUM_THREADS, and launch the code. module load nwchem export OMP_NUM_THREADS=1 srun -cpu-bind=cores nwchem test_calc Compiling NWChem The latest instructions for building NWChem on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for NWChem on GitHub","title":"NWChem"},{"location":"research-software/nwchem/nwchem/#nwchem","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development. NWChem aims to provide its users with computational chemistry tools that are scalable both in their ability to treat large scientific computational chemistry problems efficiently, and in their use of available parallel computing resources from high-performance parallel supercomputers to conventional workstation clusters. The NWChem software can handle: biomolecules, nanostructures, and solid-state system; from quantum to classical, and all combinations; Gaussian basis functions or plane-waves; scaling from one to thousands of processors; properties and relativity.","title":"NWChem"},{"location":"research-software/nwchem/nwchem/#useful-links","text":"NWChem home page https://nwchemgit.github.io/ NWChem documentation https://nwchemgit.github.io/Home.html NWChem forum https://nwchemgit.github.io/Forum.html","title":"Useful Links"},{"location":"research-software/nwchem/nwchem/#using-nwchem-on-archer2","text":"NWChem is released under an Educational Community License (ECL 2.0) and is freely available to all users on ARCHER2.","title":"Using NWChem on ARCHER2"},{"location":"research-software/nwchem/nwchem/#where-can-i-get-help","text":"If you have problems accessing or running NWChem on ARCHER2, please contact the Service Desk. General questions on the use of NWChem might also be directed to the NWChem forum (see link above). More experienced users with detailed technical issues on NWChem should consider submitting them to the NWChem github issue tracker https://github.com/nwchemgit/nwchem/issues .","title":"Where can I get help?"},{"location":"research-software/nwchem/nwchem/#running-nwchem-jobs","text":"The following script will run a NWChem job using 2 nodes (256 cores) in the standard partition. It assumes that the input file is called test\\_calc.nw . #!/bin/bash # Request 2 nodes with 128 MPI tasks per node for 20 minutes # Replace [budget code] below with your account code, # e.g. '--account=t01' #SBATCH --job-name=CASTEP #SBATCH --nodes=2 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env # Load the NWChem module, avoid any unintentional OpenMP threading by # setting OMP_NUM_THREADS, and launch the code. module load nwchem export OMP_NUM_THREADS=1 srun -cpu-bind=cores nwchem test_calc","title":"Running NWChem jobs"},{"location":"research-software/nwchem/nwchem/#compiling-nwchem","text":"The latest instructions for building NWChem on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for NWChem on GitHub","title":"Compiling NWChem"},{"location":"research-software/onetep/onetep/","text":"ONETEP Warning The ARCHER2 Service is not yet available. This documentation is in development. ONETEP (Order-N Electronic Total Energy Package) is a linear-scaling code for quantum-mechanical calculations based on density-functional theory. Useful Links ONETEP home page https://www.onetep.org ONETEP tutorials https://www.onetep.org/Main/Tutorials ONETEP documentation https://www.onetep.org/Main/Documentation Using ONETEP on ARCHER2 ONETEP is only available to users who have a valid ONETEP licence. If you have a ONETEP licence and wish to have access to ONETEP on ARCHER2, please make a request via the SAFE, see: How to request access to package groups Please have your license details to hand. Running parallel ONETEP jobs The following script will run a ONETEP job using 2 nodes (256 cores). It assumes that the input file is called text_calc.dat . #!/bin/bash # Request 2 nodes with 128 MPI tasks per node for 20 minutes # Replace [budget code] below with your account code, # e.g. '--account=t01' #SBATCH --job-name=ONETEP #SBATCH --nodes=2 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env # Load the ONETEP module module load onetep # Make sure that the stack settings are correct ulimit -s unlimited export OMP_STACKSIZE=64M export OMP_NUM_THREADS=1 # Launch the executable srun --cpu-bind=cores onetep.archer2 test_calc > test_calc.out Hints and Tips See the information in the ONETEP documentation, in particular the information on stack sizes: https://www.onetep.org/Main/RunningONETEP Compiling ONETEP The latest instructions for building CASTEP on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for ONETEP on GitHub","title":"ONETEP"},{"location":"research-software/onetep/onetep/#onetep","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development. ONETEP (Order-N Electronic Total Energy Package) is a linear-scaling code for quantum-mechanical calculations based on density-functional theory.","title":"ONETEP"},{"location":"research-software/onetep/onetep/#useful-links","text":"ONETEP home page https://www.onetep.org ONETEP tutorials https://www.onetep.org/Main/Tutorials ONETEP documentation https://www.onetep.org/Main/Documentation","title":"Useful Links"},{"location":"research-software/onetep/onetep/#using-onetep-on-archer2","text":"ONETEP is only available to users who have a valid ONETEP licence. If you have a ONETEP licence and wish to have access to ONETEP on ARCHER2, please make a request via the SAFE, see: How to request access to package groups Please have your license details to hand.","title":"Using ONETEP on ARCHER2"},{"location":"research-software/onetep/onetep/#running-parallel-onetep-jobs","text":"The following script will run a ONETEP job using 2 nodes (256 cores). It assumes that the input file is called text_calc.dat . #!/bin/bash # Request 2 nodes with 128 MPI tasks per node for 20 minutes # Replace [budget code] below with your account code, # e.g. '--account=t01' #SBATCH --job-name=ONETEP #SBATCH --nodes=2 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env # Load the ONETEP module module load onetep # Make sure that the stack settings are correct ulimit -s unlimited export OMP_STACKSIZE=64M export OMP_NUM_THREADS=1 # Launch the executable srun --cpu-bind=cores onetep.archer2 test_calc > test_calc.out","title":"Running parallel ONETEP jobs"},{"location":"research-software/onetep/onetep/#hints-and-tips","text":"See the information in the ONETEP documentation, in particular the information on stack sizes: https://www.onetep.org/Main/RunningONETEP","title":"Hints and Tips"},{"location":"research-software/onetep/onetep/#compiling-onetep","text":"The latest instructions for building CASTEP on ARCHER2 may be found in the GitHub repository of build instructions: Build instructions for ONETEP on GitHub","title":"Compiling ONETEP"},{"location":"research-software/openfoam/openfoam/","text":"OpenFOAM Warning The ARCHER2 Service is not yet available. This documentation is in development. OpenFOAM is an open-source toolbox for computational fluid dynamics. OpenFOAM consists of generic tools to simulate complex physics for a variety of fields of interest, from fluid flows involving chemical reactions, turbulence and heat transfer, to solid dynamics, electromagnetism and the pricing of financial options. The core technology of OpenFOAM is a flexible set of modules written in C++. These are used to build solvers and utilities to perform pre-processing and post-processing tasks ranging from simple data manipulation to visualisation and mesh processing. There are a number of different flavours of the OpenFOAM package with slightly different histories, and slightly different features. The two most common are distributed by openfoam.org and openfoam.com. Useful Links OpenFOAM website (.org) https://openfoam.org OpenFOAM documentation https://cfd.direct/openfoam/user-guide/ OpenFOAM website (.com) https://www.openfoam.com OpenFOAM documentation https://www.openfoam.com/documentation/ Using OpenFOAM on ARCHER2 OpenFOAM is released under a GPL v3 license and is freely available to all users on ARCHER2. auser@uan01:> module avail openfoam --------------- /work/y07/shared/archer2-modules/modulefiles-cse ---------- openfoam/com/v2006 openfoam/org/v8.20200901 Versions from openfoam.org are typically v8.0 etc and there is typically one release per year (in June; with a patch release in September). Versions from openfoam.com are e.g., v2006 (to be read as 2020 June) and there are typically two releases a year (one in June, and one in December). To use OpenFOAM on ARCHER2 you should first load the OpenFOAM module, e.g.,: user@uan01:> module -s restore PrgEnv-gnu user@uan01:> module load openfoam/com/v2006 The module defines only the base installation directory via the environment variable FOAM_INSTALL_DIR . After loading the module you need to source the etc/bashrc file provided by OpenFOAM, e.g.,: user@uan01:> source ${FOAM_INSTALL_DIR}/etc/bashrc You should then be able to use OpenFOAM. The above commands will also need to be added to any job/batch submission scripts you want to use to run OpenFOAM. Note that all the centrally installed versions of OpenFOAM are compiled under PrgEnv-gnu . Running parallel OpenFOAM jobs While it is possible to run limited OpenFOAM pre-processing and post-processing activities on the front end, we request all significant work is submitted to the queue system. Please remember that the front end is a shared resource. A typical SLURM job submission script for OpenFOAM is given here. This would request 4 nodes to run with 128 MPI tasks per node (a total of 512 MPI tasks). Each MPI task is allocated one core ( --cpus-per-task=1 ). #!/bin/bash #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:10:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env # Load the appropriate modules and source the OpenFOAM bashrc file # The first line makes PrgEnv-gnu available on the back end nodes. module -s restore /etc/cray-pe.d/PrgEnv-gnu module load openfoam/org/v8.20200901 source ${FOAM_INSTALL_DIR}/etc/bashrc # Run OpenFOAM work srun --cpu-bind=cores interFoam -parallel Compiling OpenFOAM If you want to compile your own version of OpenFOAM, instructions are available for ARCHER2 at https://github.com/hpc-uk/build-instructions/tree/main/OpenFOAM","title":"OpenFOAM"},{"location":"research-software/openfoam/openfoam/#openfoam","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development. OpenFOAM is an open-source toolbox for computational fluid dynamics. OpenFOAM consists of generic tools to simulate complex physics for a variety of fields of interest, from fluid flows involving chemical reactions, turbulence and heat transfer, to solid dynamics, electromagnetism and the pricing of financial options. The core technology of OpenFOAM is a flexible set of modules written in C++. These are used to build solvers and utilities to perform pre-processing and post-processing tasks ranging from simple data manipulation to visualisation and mesh processing. There are a number of different flavours of the OpenFOAM package with slightly different histories, and slightly different features. The two most common are distributed by openfoam.org and openfoam.com.","title":"OpenFOAM"},{"location":"research-software/openfoam/openfoam/#useful-links","text":"OpenFOAM website (.org) https://openfoam.org OpenFOAM documentation https://cfd.direct/openfoam/user-guide/ OpenFOAM website (.com) https://www.openfoam.com OpenFOAM documentation https://www.openfoam.com/documentation/","title":"Useful Links"},{"location":"research-software/openfoam/openfoam/#using-openfoam-on-archer2","text":"OpenFOAM is released under a GPL v3 license and is freely available to all users on ARCHER2. auser@uan01:> module avail openfoam --------------- /work/y07/shared/archer2-modules/modulefiles-cse ---------- openfoam/com/v2006 openfoam/org/v8.20200901 Versions from openfoam.org are typically v8.0 etc and there is typically one release per year (in June; with a patch release in September). Versions from openfoam.com are e.g., v2006 (to be read as 2020 June) and there are typically two releases a year (one in June, and one in December). To use OpenFOAM on ARCHER2 you should first load the OpenFOAM module, e.g.,: user@uan01:> module -s restore PrgEnv-gnu user@uan01:> module load openfoam/com/v2006 The module defines only the base installation directory via the environment variable FOAM_INSTALL_DIR . After loading the module you need to source the etc/bashrc file provided by OpenFOAM, e.g.,: user@uan01:> source ${FOAM_INSTALL_DIR}/etc/bashrc You should then be able to use OpenFOAM. The above commands will also need to be added to any job/batch submission scripts you want to use to run OpenFOAM. Note that all the centrally installed versions of OpenFOAM are compiled under PrgEnv-gnu .","title":"Using OpenFOAM on ARCHER2"},{"location":"research-software/openfoam/openfoam/#running-parallel-openfoam-jobs","text":"While it is possible to run limited OpenFOAM pre-processing and post-processing activities on the front end, we request all significant work is submitted to the queue system. Please remember that the front end is a shared resource. A typical SLURM job submission script for OpenFOAM is given here. This would request 4 nodes to run with 128 MPI tasks per node (a total of 512 MPI tasks). Each MPI task is allocated one core ( --cpus-per-task=1 ). #!/bin/bash #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:10:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env # Load the appropriate modules and source the OpenFOAM bashrc file # The first line makes PrgEnv-gnu available on the back end nodes. module -s restore /etc/cray-pe.d/PrgEnv-gnu module load openfoam/org/v8.20200901 source ${FOAM_INSTALL_DIR}/etc/bashrc # Run OpenFOAM work srun --cpu-bind=cores interFoam -parallel","title":"Running parallel OpenFOAM jobs"},{"location":"research-software/openfoam/openfoam/#compiling-openfoam","text":"If you want to compile your own version of OpenFOAM, instructions are available for ARCHER2 at https://github.com/hpc-uk/build-instructions/tree/main/OpenFOAM","title":"Compiling OpenFOAM"},{"location":"research-software/qe/qe/","text":"Quantum Espresso Warning The ARCHER2 Service is not yet available. This documentation is in development. Quantum Espresso (QE) is an integrated suite of open-source computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials. Useful Links Quantum Espresso home page Quantum Espresso User Guides Quantum Espresso Tutorials Using QE on ARCHER2 QE is released under a GPL v2 license and is freely available to all ARCHER2 users. Running parallel QE jobs For example, the following script will run a QE pw.x job using 4 nodes (128x4 cores). #!/bin/bash # Request 4 nodes to run a 512 MPI task job with 128 MPI tasks per node. # The maximum walltime limit is set to be 20 minutes. #SBATCH --job-name=qe_test #SBATCH --nodes=4 #SBATCH --ntasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env # Load the relevant Quantum Espresso module module load quantum-espresso srun pw.x < test_calc.in Hints and tips The QE module is set to load up the default QE-provided pseudo-potentials. If you wish to use non-default pseudo-potentials, you will need to change the ESPRESSO_PSEUDO variable to point to the directory you wish. This can be done by adding the following line after the module is loaded export ESPRESSO_PSEUDO /path/to/pseudo_potentials Compiling QE The latest instructions for building QE on ARCHER2 can be found in the GitHub repository of build instructions: Build instructions for Quantum Espresso","title":"Quantum Espresso"},{"location":"research-software/qe/qe/#quantum-espresso","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development. Quantum Espresso (QE) is an integrated suite of open-source computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials.","title":"Quantum Espresso"},{"location":"research-software/qe/qe/#useful-links","text":"Quantum Espresso home page Quantum Espresso User Guides Quantum Espresso Tutorials","title":"Useful Links"},{"location":"research-software/qe/qe/#using-qe-on-archer2","text":"QE is released under a GPL v2 license and is freely available to all ARCHER2 users.","title":"Using QE on ARCHER2"},{"location":"research-software/qe/qe/#running-parallel-qe-jobs","text":"For example, the following script will run a QE pw.x job using 4 nodes (128x4 cores). #!/bin/bash # Request 4 nodes to run a 512 MPI task job with 128 MPI tasks per node. # The maximum walltime limit is set to be 20 minutes. #SBATCH --job-name=qe_test #SBATCH --nodes=4 #SBATCH --ntasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env # Load the relevant Quantum Espresso module module load quantum-espresso srun pw.x < test_calc.in","title":"Running parallel QE jobs"},{"location":"research-software/qe/qe/#hints-and-tips","text":"The QE module is set to load up the default QE-provided pseudo-potentials. If you wish to use non-default pseudo-potentials, you will need to change the ESPRESSO_PSEUDO variable to point to the directory you wish. This can be done by adding the following line after the module is loaded export ESPRESSO_PSEUDO /path/to/pseudo_potentials","title":"Hints and tips"},{"location":"research-software/qe/qe/#compiling-qe","text":"The latest instructions for building QE on ARCHER2 can be found in the GitHub repository of build instructions: Build instructions for Quantum Espresso","title":"Compiling QE"},{"location":"research-software/vasp/vasp/","text":"VASP Warning The ARCHER2 Service is not yet available. This documentation is in development. The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles. VASP computes an approximate solution to the many-body Schr\u00f6dinger equation, either within density functional theory (DFT), solving the Kohn-Sham equations, or within the Hartree-Fock (HF) approximation, solving the Roothaan equations. Hybrid functionals that mix the Hartree-Fock approach with density functional theory are implemented as well. Furthermore, Green's functions methods (GW quasiparticles, and ACFDT-RPA) and many-body perturbation theory (2nd-order M\u00f8ller-Plesset) are available in VASP. In VASP, central quantities, like the one-electron orbitals, the electronic charge density, and the local potential are expressed in plane wave basis sets. The interactions between the electrons and ions are described using norm-conserving or ultrasoft pseudopotentials, or the projector-augmented-wave method. To determine the electronic ground state, VASP makes use of efficient iterative matrix diagonalisation techniques, like the residual minimisation method with direct inversion of the iterative subspace (RMM-DIIS) or blocked Davidson algorithms. These are coupled to highly efficient Broyden and Pulay density mixing schemes to speed up the self-consistency cycle. Useful Links VASP Manual VASP wiki VASP Licensing Using VASP on ARCHER2 VASP is only available to users who have a valid VASP licence. If you have a VASP 5 or 6 licence and wish to have access to VASP on ARCHER2, please make a request via the SAFE, see: How to request access to package groups Please have your license details to hand. Note Both VASP 5 and VASP 6 are available on ARCHER2. You generally need a different licence for each of these versions. Running parallel VASP jobs To access VASP you should load the appropriate vasp module in your job submission scripts. VASP 5 To load the default version of VASP 5, you would use: module load vasp/5 Once loaded, the executables are called: vasp_std - Multiple k-point version vasp_gam - GAMMA-point only version vasp_ncl - Non-collinear version Once the module has been loaded, you can access the LDA and PBE pseudopotentials for VASP on ARCHER2 at: $VASP_PSPOT_DIR The following script will run a VASP job using 2 nodes (128x2, 256 total cores). #!/bin/bash # Request 2 nodes (256 MPI tasks at 128 tasks per node) for 20 minutes. #SBATCH --job-name=VASP_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env # Load the VASP module, avoid any unintentional OpenMP threading by # setting OMP_NUM_THREADS, and launch the code. export OMP_NUM_THREADS=1 module load vasp/5 srun --cpu-bind=cores vasp_std VASP 6 To load the default version of VASP 6, you would use: module load vasp/6 Once loaded, the executables are called: vasp_std - Multiple k-point version vasp_gam - GAMMA-point only version vasp_ncl - Non-collinear version Once the module has been loaded, you can access the LDA and PBE pseudopotentials for VASP on ARCHER2 at: $VASP_PSPOT_DIR The following script will run a VASP job using 2 nodes (128x2, 256 total cores) using only MPI ranks and no OpenMP threading. Tip VASP 6 can make use of OpenMP threads in addition to running with pure MPI. We will add notes on performance and use of threading in VASP as information becomes available. #!/bin/bash # Request 2 nodes (256 MPI tasks at 128 tasks per node) for 20 minutes. #SBATCH --job-name=VASP_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env # Load the VASP module, avoid any unintentional OpenMP threading by # setting OMP_NUM_THREADS, and launch the code. export OMP_NUM_THREADS=1 module load vasp/6 srun --cpu-bind=cores vasp_std Compiling VASP on ARCHER2 If you wish to compile your own version of VASP on ARCHER2 (either VASP 5 or VASP 6) you can find information on how we compiled the central versions in the build instructions GitHub repository. See: Build instructions for VASP on GitHub","title":"VASP"},{"location":"research-software/vasp/vasp/#vasp","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development. The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles. VASP computes an approximate solution to the many-body Schr\u00f6dinger equation, either within density functional theory (DFT), solving the Kohn-Sham equations, or within the Hartree-Fock (HF) approximation, solving the Roothaan equations. Hybrid functionals that mix the Hartree-Fock approach with density functional theory are implemented as well. Furthermore, Green's functions methods (GW quasiparticles, and ACFDT-RPA) and many-body perturbation theory (2nd-order M\u00f8ller-Plesset) are available in VASP. In VASP, central quantities, like the one-electron orbitals, the electronic charge density, and the local potential are expressed in plane wave basis sets. The interactions between the electrons and ions are described using norm-conserving or ultrasoft pseudopotentials, or the projector-augmented-wave method. To determine the electronic ground state, VASP makes use of efficient iterative matrix diagonalisation techniques, like the residual minimisation method with direct inversion of the iterative subspace (RMM-DIIS) or blocked Davidson algorithms. These are coupled to highly efficient Broyden and Pulay density mixing schemes to speed up the self-consistency cycle.","title":"VASP"},{"location":"research-software/vasp/vasp/#useful-links","text":"VASP Manual VASP wiki VASP Licensing","title":"Useful Links"},{"location":"research-software/vasp/vasp/#using-vasp-on-archer2","text":"VASP is only available to users who have a valid VASP licence. If you have a VASP 5 or 6 licence and wish to have access to VASP on ARCHER2, please make a request via the SAFE, see: How to request access to package groups Please have your license details to hand. Note Both VASP 5 and VASP 6 are available on ARCHER2. You generally need a different licence for each of these versions.","title":"Using VASP on ARCHER2"},{"location":"research-software/vasp/vasp/#running-parallel-vasp-jobs","text":"To access VASP you should load the appropriate vasp module in your job submission scripts.","title":"Running parallel VASP jobs"},{"location":"research-software/vasp/vasp/#vasp-5","text":"To load the default version of VASP 5, you would use: module load vasp/5 Once loaded, the executables are called: vasp_std - Multiple k-point version vasp_gam - GAMMA-point only version vasp_ncl - Non-collinear version Once the module has been loaded, you can access the LDA and PBE pseudopotentials for VASP on ARCHER2 at: $VASP_PSPOT_DIR The following script will run a VASP job using 2 nodes (128x2, 256 total cores). #!/bin/bash # Request 2 nodes (256 MPI tasks at 128 tasks per node) for 20 minutes. #SBATCH --job-name=VASP_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env # Load the VASP module, avoid any unintentional OpenMP threading by # setting OMP_NUM_THREADS, and launch the code. export OMP_NUM_THREADS=1 module load vasp/5 srun --cpu-bind=cores vasp_std","title":"VASP 5"},{"location":"research-software/vasp/vasp/#vasp-6","text":"To load the default version of VASP 6, you would use: module load vasp/6 Once loaded, the executables are called: vasp_std - Multiple k-point version vasp_gam - GAMMA-point only version vasp_ncl - Non-collinear version Once the module has been loaded, you can access the LDA and PBE pseudopotentials for VASP on ARCHER2 at: $VASP_PSPOT_DIR The following script will run a VASP job using 2 nodes (128x2, 256 total cores) using only MPI ranks and no OpenMP threading. Tip VASP 6 can make use of OpenMP threads in addition to running with pure MPI. We will add notes on performance and use of threading in VASP as information becomes available. #!/bin/bash # Request 2 nodes (256 MPI tasks at 128 tasks per node) for 20 minutes. #SBATCH --job-name=VASP_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env # Load the VASP module, avoid any unintentional OpenMP threading by # setting OMP_NUM_THREADS, and launch the code. export OMP_NUM_THREADS=1 module load vasp/6 srun --cpu-bind=cores vasp_std","title":"VASP 6"},{"location":"research-software/vasp/vasp/#compiling-vasp-on-archer2","text":"If you wish to compile your own version of VASP on ARCHER2 (either VASP 5 or VASP 6) you can find information on how we compiled the central versions in the build instructions GitHub repository. See: Build instructions for VASP on GitHub","title":"Compiling VASP on ARCHER2"},{"location":"software-libraries/","text":"Software Libraries This section will provide information on each of the centrally-installed software libraries including: versions available, how to get access, good practice for getting best performance, links to associated training and webinars, links to associated technical reports (eCSE final reports, white papers), links to instruction manuals and further information. This section currently covers the following libraries: HPE Cray LibSci: BLAS, LAPACK, ScaLAPACK FFTW HDF5 NetCDF","title":"Overview"},{"location":"software-libraries/#software-libraries","text":"This section will provide information on each of the centrally-installed software libraries including: versions available, how to get access, good practice for getting best performance, links to associated training and webinars, links to associated technical reports (eCSE final reports, white papers), links to instruction manuals and further information. This section currently covers the following libraries: HPE Cray LibSci: BLAS, LAPACK, ScaLAPACK FFTW HDF5 NetCDF","title":"Software Libraries"},{"location":"software-libraries/fftw/","text":"FFTW Provides: FFTW v3 Access: module load cray-fftw FFTW is a C subroutine library (which includes a Fortran interface) for computing the discrete Fourier transform (DFT) in one or more dimensions, of arbitrary input size, and of both real and complex data (as well as of even/odd data, i.e. the discrete cosine/sine transforms or DCT/DST). Only the version 3 interface is available on ARCHER2.","title":"FFTW"},{"location":"software-libraries/fftw/#fftw","text":"Provides: FFTW v3 Access: module load cray-fftw FFTW is a C subroutine library (which includes a Fortran interface) for computing the discrete Fourier transform (DFT) in one or more dimensions, of arbitrary input size, and of both real and complex data (as well as of even/odd data, i.e. the discrete cosine/sine transforms or DCT/DST). Only the version 3 interface is available on ARCHER2.","title":"FFTW"},{"location":"software-libraries/hdf5/","text":"HDF5 Provides: HDF5 Access: module load cray-hdf5 (serial version) module load cray-hdf5-parallel (MPI parallel version) Hierarchical Data Format HDF5 libraries. Both serial and parallel versions are available on ARCHER2.","title":"HDF5"},{"location":"software-libraries/hdf5/#hdf5","text":"Provides: HDF5 Access: module load cray-hdf5 (serial version) module load cray-hdf5-parallel (MPI parallel version) Hierarchical Data Format HDF5 libraries. Both serial and parallel versions are available on ARCHER2.","title":"HDF5"},{"location":"software-libraries/libsci/","text":"HPE Cray LibSci Provides: BLAS, LAPACK, CBLAS, LAPACKE, BLACS, ScaLAPACK Access: module load cray-libsci (note: loaded by default for all users) Cray scientific libraries, available for all compiler choices provides access to the Fortran BLAS and LAPACK interface for basic linear algebra, the corresponding C interfaces CBLAS and LAPACKE , and BLACS and ScaLAPACK for parallel linear algebra. Type man intro_libsci for further details.","title":"HPE Cray LibSci: BLAS, LAPACK, ScaLAPACK"},{"location":"software-libraries/libsci/#hpe-cray-libsci","text":"Provides: BLAS, LAPACK, CBLAS, LAPACKE, BLACS, ScaLAPACK Access: module load cray-libsci (note: loaded by default for all users) Cray scientific libraries, available for all compiler choices provides access to the Fortran BLAS and LAPACK interface for basic linear algebra, the corresponding C interfaces CBLAS and LAPACKE , and BLACS and ScaLAPACK for parallel linear algebra. Type man intro_libsci for further details.","title":"HPE Cray LibSci"},{"location":"software-libraries/netcdf/","text":"NetCDF Provides: NetCDF, PnetCDF Access: module load cray-hdf5 (serial version) module load cray-hdf5-parallel (MPI parallel version) Network Common Data Form NetCDF , NetCDF built on top of parallel HDF5 and parallel NetCDF (usually referred to as PnetCDF ) are available.","title":"NetCDF"},{"location":"software-libraries/netcdf/#netcdf","text":"Provides: NetCDF, PnetCDF Access: module load cray-hdf5 (serial version) module load cray-hdf5-parallel (MPI parallel version) Network Common Data Form NetCDF , NetCDF built on top of parallel HDF5 and parallel NetCDF (usually referred to as PnetCDF ) are available.","title":"NetCDF"},{"location":"user-guide/","text":"User and Best Practice Guide The ARCHER2 User and Best Practice Guide covers all aspects of use of the ARCHER2 service. This includes fundamentals (required by all users to use the system effectively), best practice for getting the most out of ARCHER2 and more advanced technical topics. The User and Best Practice Guide contains the following sections: Connecting to ARCHER2 Data management and transfer Software environment Running jobs on ARCHER2 I/O and file systems Application development environment Containers Using Python Data analysis Debugging Profiling Performance tuning","title":"Overview"},{"location":"user-guide/#user-and-best-practice-guide","text":"The ARCHER2 User and Best Practice Guide covers all aspects of use of the ARCHER2 service. This includes fundamentals (required by all users to use the system effectively), best practice for getting the most out of ARCHER2 and more advanced technical topics. The User and Best Practice Guide contains the following sections: Connecting to ARCHER2 Data management and transfer Software environment Running jobs on ARCHER2 I/O and file systems Application development environment Containers Using Python Data analysis Debugging Profiling Performance tuning","title":"User and Best Practice Guide"},{"location":"user-guide/analysis/","text":"Data analysis Warning The ARCHER2 Service is not yet available. This documentation is in development. How to analyse data on ARCHER2, including use of pre- and post-processing.","title":"Data analysis"},{"location":"user-guide/analysis/#data-analysis","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development. How to analyse data on ARCHER2, including use of pre- and post-processing.","title":"Data analysis"},{"location":"user-guide/connecting/","text":"Connecting to ARCHER2 On the ARCHER2 system, interactive access can be achieved via SSH, either directly from a command line terminal or using an SSH client. In addition data can be transferred to and from the ARCHER2 system using scp from the command line or by using a file transfer client. This section covers the basic connection methods. Before following the process below, we assume you have setup an account on ARCHER2 through the EPCC SAFE. Documentation on how to do this can be found at: SAFE Guide for Users Access credentials To access ARCHER2, you need to use two credentials: your password and an SSH key pair protected by a passphrase. You can find more detailed instructions on how to set up your credentials to access ARCHER2 from Windows, macOS and Linux below. SSH Key Pairs You will need to generate an SSH key pair protected by a passphrase to access ARCHER2. Using a terminal (the command line), set up a key pair that contains your e-mail address and enter a passphrase you will use to unlock the key: $ ssh-keygen -t rsa -C \"your@email.com\" ... -bash-4.1$ ssh-keygen -t rsa -C \"your@email.com\" Generating public/private rsa key pair. Enter file in which to save the key (/Home/user/.ssh/id_rsa): [Enter] Enter passphrase (empty for no passphrase): [Passphrase] Enter same passphrase again: [Passphrase] Your identification has been saved in /Home/user/.ssh/id_rsa. Your public key has been saved in /Home/user/.ssh/id_rsa.pub. The key fingerprint is: 03:d4:c4:6d:58:0a:e2:4a:f8:73:9a:e8:e3:07:16:c8 your@email.com The key's randomart image is: +--[ RSA 2048]----+ | . ...+o++++. | | . . . =o.. | |+ . . .......o o | |oE . . | |o = . S | |. +.+ . | |. oo | |. . | | .. | +-----------------+ (remember to replace \"your@email.com\" with your e-mail address). Upload public part of key pair to SAFE You should now upload the public part of your SSH key pair to the SAFE by following the instructions at: Login to SAFE . Then: Go to the Menu Login accounts and select the ARCHER2 account you want to add the SSH key to On the subsequent Login account details page click the Add Credential button Select SSH public key as the Credential Type and click Next Either copy and paste the public part of your SSH key into the SSH Public key box or use the button to select the public key file on your computer. Click Add to associate the public SSH key part with your account Once you have done this, your SSH key will be added to your ARCHER2 account. Remember, you will need to use both an SSH key and password to log into ARCHER2 so you will also need to collect your initial password before you can log into ARCHER2. We cover this next. Initial passwords The SAFE web interface is used to provide your initial password for logging onto ARCHER2 (see the SAFE Documentation for more details on requesting accounts and picking up passwords). Note You may now change your password on the ARCHER2 machine itself using the passwd command or when you are prompted the first time you login. This change will not be reflected in the SAFE. If you forget your password, you should use the SAFE to request a new one-shot password. SSH Clients Interaction with ARCHER2 is done remotely, over an encrypted communication channel, Secure Shell version 2 (SSH-2). This allows command-line access to one of the login nodes of a ARCHER2, from which you can run commands or use a command-line text editor to edit files. SSH can also be used to run graphical programs such as GUI text editors and debuggers when used in conjunction with an X client. Logging in from Linux and MacOS Linux distributions and MacOS each come installed with a terminal application that can be use for SSH access to the login nodes. Linux users will have different terminals depending on their distribution and window manager (e.g. GNOME Terminal in GNOME, Konsole in KDE). Consult your Linux distribution's documentation for details on how to load a terminal. MacOS users can use the Terminal application, located in the Utilities folder within the Applications folder. You can use the following command from the terminal window to login into ARCHER2: ssh username@login.archer2.ac.uk You will first be prompted for your machine account password. Once you have entered your password successfully, you will then be prompted for the passphrase associated with your SSH key pair. You need to enter both credentials correctly to be able to access ARCHER2. Tip If your SSH key pair is not stored in the default location (usually ~/.ssh/id_rsa ) on your local system, you may need to specify the path to the private part of the key wih the -i option to ssh . For example, if your key is in a file called keys/id_rsa_ARCHER2 you would use the command ssh -i keys/id_rsa_ARCHER2 username@login.archer2.ac.uk to log in. Tip When you first log into ARCHER2, you will be prompted to change your initial password. This is a three step process: When promoted to enter your ldap password : Re-enter the password you retrieved from SAFE When prompted to enter your new password: type in a new password When prompted to re-enter the new password: re-enter the new password Your password has now been changed To allow remote programs, especially graphical applications to control your local display, such as being able to open up a new GUI window (such as for a debugger), use: ssh -X username@login.archer2.ac.uk Some sites recommend using the -Y flag. While this can fix some compatibility issues, the -X flag is more secure. Current MacOS systems do not have an X window system. Users should install the XQuartz package to allow for SSH with X11 forwarding on MacOS systems: XQuartz website Logging in from Windows using MobaXterm A typical Windows installation will not include a terminal client, though there are various clients available. We recommend all our Windows users to download and install MobaXterm to access ARCHER2. It is very easy to use and includes an integrated X server with SSH client to run any graphical applications on ARCHER2. You can download MobaXterm Home Edition (Installer Edition) from the following link: Install MobaXterm Double-click the downloaded Microsoft Installer file (.msi), and the Windows wizard will automatically guides you through the installation process. Note, you might need to have administrator rights to install on some Windows OS. Also make sure to check whether Windows Firewall hasn't blocked any features of this program after installation. Start MobaXterm using, for example, the icon added to the Start menu during the installation process. Use your ARCHER2 username, the login host should be set to login.archer2.ac.uk . If you would like to run any small remote GUI applications, then make sure to use -X option along with the ssh command (see above) to enable X11 forwarding, which allows you to run graphical clients on your local X server. Making access more convenient using the SSH configuration file Typing in the full command to login or transfer data to ARCHER2 can become tedious as it often has to be repeated many times. You can use the SSH configuration file, usually located on your local machine at .ssh/config to make things a bit more convenient. Each remote site (or group of sites) can have an entry in this file which may look something like: Host archer2 HostName login.archer2.ac.uk User username (remember to replace username with your actual username!). The Host archer2 line defines a short name for the entry. In this case, instead of typing ssh username@login.archer2.ac.uk to access the ARCHER2 login nodes, you could use ssh archer2 instead. The remaining lines define the options for the archer2 host. Hostname login.archer2.ac.uk - defines the full address of the host User username - defines the username to use by default for this host (replace username with your own username on the remote host) Now you can use SSH to access ARCHER2 without needing to enter your username or the full hostname every time: $ ssh archer2 You can set up as many of these entries as you need in your local configuration file. Other options are available. See the ssh_config man page (or man ssh_config on any machine with SSH installed) for a description of the SSH configuration file. You may find the IdentityFile option useful if you have to manage multiple SSH key pairs for different systems as this allows you to specify which SSH key to use for each system. Bug There is a known bug with Windows ssh-agent. If you get the error message: Warning: agent returned different signature type ssh-rsa (expected rsa-sha2-512) , you will need to either specify the path to your ssh key in the command line (using the -i option as described above) or add the path to your SSH config file by using the IdentityFile option. SSH debugging tips If you find you are unable to connect via SSH there are a number of ways you can try and diagnose the issue. Some of these are collected below - if you are having difficulties connecting we suggest trying these before contacting the ARCHER2 service desk. Can you connect to the login node? Try the command ping -c 3 login.archer2.ac.uk . If you successfully connect to the login node, the output should include: --- login.dyn.archer2.ac.uk ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 38ms (the ping time '38ms' is not important). If not all packets are received there could be a problem with your internet connection, or the login node could be unavailable. Password If you are having trouble entering your password consider using a password manager, from which you can copy and paste it. This will also help you generate a secure password. If you need to reset your password, instructions for doing so can be found in the SAFE documentation Windows users please note that Ctrl+V does not work to paste in to PuTTY, MobaXterm, or PowerShell. Instead use Shift+Ins to paste. Alternatively, right-click and select 'Paste' in PuTTY and MobaXterm, or simply right-click to paste in PowerShell. SSH key If you get the error message Permission denied (publickey) this can indicate a problem with your SSH key. Some things to check: Have you uploaded the key to SAFE? Please note that if the same key is reuploaded SAFE will not map the \"new\" key to ARCHER2. If for some reason this is required, please delete the key first, then reupload. Is ssh using the correct key? You can check which keys are being found and offered by ssh using ssh -vvv . If your private key has a non-default name you can use the -i flag to provide it to ssh, i.e. ssh -i path/to/key username@login.archer2.ac.uk . Are you entering the passphrase correctly? You will be asked for your private key's passphrase first. If you enter it incorrectly you will usually be asked to enter it again, and usually up to three times in total, after which ssh will fail with Permission denied (publickey) . If you would like to confirm your passphrase without attempting to connect, you can use ssh-keygen -y -f /path/to/private/key . If successful, this command will print the corresponding public key. You can also use this to check it is the one uploaded to SAFE. Are permissions correct on the ssh key? One common issue is that the permissions are incorrect on the either the key file, or the directory it's contained in. On Linux/MacOS for example, if your private keys are held in ~/.ssh/ you can check this with ls -al ~/.ssh . This should give something similar to the following output: $ ls -al ~/.ssh/ drwx------. 2 user group 48 Jul 15 20:24 . drwx------. 12 user group 4096 Oct 13 12:11 .. -rw-------. 1 user group 113 Jul 15 20:23 authorized_keys -rw-------. 1 user group 12686 Jul 15 20:23 id_rsa -rw-r--r--. 1 user group 2785 Jul 15 20:23 id_rsa.pub -rw-r--r--. 1 user group 1967 Oct 13 14:11 known_hosts The important section here is the string of letters and dashes at the start, for the lines ending in . , id_rsa , and id_rsa.pub , which indicate permissions on the containing directory, private key, and public key respectively. If your permissions are not correct, they can be set with chmod . Consult the table below for the relevant chmod command. On Windows, permissions are handled differently but can be set by right-clicking on the file and selecting Properties > Security > Advanced. The user, SYSTEM, and Administrators should have Full control , and no other permissions should exist for both public and private key files, and the containing folder. Target Permissions chmod Code Directory drwx------ 700 Private Key -rw------- 600 Public Key -rw-r--r-- 644 chmod can be used to set permissions on the target in the following way: chmod <code> <target> . So for example to set correct permissions on the private key file id_rsa_ARCHER2 one would use the command chmod 600 id_rsa_ARCHER2 . Tip Unix file permissions can be understood in the following way. There are three groups that can have file permissions: (owning) users , (owning) groups , and others . The available permissions are read , write , and execute . The first character indicates whether the target is a file - , or directory d . The next three characters indicate the owning user's permissions. The first character is r if they have read permission, - if they don't, the second character is w if they have write permission, - if they don't, the third character is x if they have execute permission, - if they don't. This pattern is then repeated for group , and other permissions. For example the pattern -rw-r--r-- indicates that the owning user can read and write the file, members of the owning group can read it, and anyone else can also read it. The chmod codes are constructed by treating the user, group, and owner permission strings as binary numbers, then converting them to decimal. For example the permission string -rwx------ becomes 111 000 000 -> 700 . SSH verbose output Verbose debugging output from ssh can be very useful for diagnosing the issue. In particular, it can be used to distinguish between problems with the SSH key and password - further details are given below. To enable verbose output add the -vvv flag to your SSH command. For example: ssh -vvv username@login.archer2.ac.uk The output is lengthy, but somewhere in there you should see lines similar to the following: debug1: Next authentication method: keyboard-interactive debug2: userauth_kbdint debug3: send packet: type 50 debug2: we sent a keyboard-interactive packet, wait for reply debug3: receive packet: type 60 debug2: input_userauth_info_req debug2: input_userauth_info_req: num_prompts 1 Password: debug3: send packet: type 61 debug3: receive packet: type 60 debug2: input_userauth_info_req debug2: input_userauth_info_req: num_prompts 0 debug3: send packet: type 61 debug3: receive packet: type 51 Authenticated with partial success. debug1: Authentications that can continue: publickey,password If you do not see the Password: prompt you may have connection issues, or there could be a problem with the ARCHER2 login nodes. If you do not see Authenticated with partial success it means your password was not accepted. You will be asked to re-enter your password, usually two more times before the connection will be rejected. Consider the suggestions under Password above. If you do see Authenticated with partial success , it means your password was accepted, and your SSH key will now be checked. You should next see something similiar to: debug1: Next authentication method: publickey debug1: Offering public key: RSA SHA256:<key_hash> <path_to_private_key> debug3: send_pubkey_test debug3: send packet: type 50 debug2: we sent a publickey packet, wait for reply debug3: receive packet: type 60 debug1: Server accepts key: pkalg rsa-sha2-512 blen 2071 debug2: input_userauth_pk_ok: fp SHA256:<key_hash> debug3: sign_and_send_pubkey: RSA SHA256:<key_hash> Enter passphrase for key '<path_to_private_key>': debug3: send packet: type 50 debug3: receive packet: type 52 debug1: Authentication succeeded (publickey). Most importantly, you can see which files ssh has checked for private keys, and you can see if any key is accepted. The line Authenticated succeeded indicates that the SSH key has been accepted. By default ssh will go through a list of standard private key files, as well as any you have specified with -i or a config file. This is fine, as long as one of the files mentioned is the one that matches the public key uploaded to SAFE. If your SSH key passphrase is incorrect, you will be asked to try again up to three times in total, before being disconnected with Permission denied (publickey) . If you enter your passphrase correctly, but still see this error message, please consider the advice under SSH key above. The equivalent information can be obtained in PuTTY or MobaXterm by enabling all logging in settings.","title":"Connecting to ARCHER2"},{"location":"user-guide/connecting/#connecting-to-archer2","text":"On the ARCHER2 system, interactive access can be achieved via SSH, either directly from a command line terminal or using an SSH client. In addition data can be transferred to and from the ARCHER2 system using scp from the command line or by using a file transfer client. This section covers the basic connection methods. Before following the process below, we assume you have setup an account on ARCHER2 through the EPCC SAFE. Documentation on how to do this can be found at: SAFE Guide for Users","title":"Connecting to ARCHER2"},{"location":"user-guide/connecting/#access-credentials","text":"To access ARCHER2, you need to use two credentials: your password and an SSH key pair protected by a passphrase. You can find more detailed instructions on how to set up your credentials to access ARCHER2 from Windows, macOS and Linux below.","title":"Access credentials"},{"location":"user-guide/connecting/#ssh-key-pairs","text":"You will need to generate an SSH key pair protected by a passphrase to access ARCHER2. Using a terminal (the command line), set up a key pair that contains your e-mail address and enter a passphrase you will use to unlock the key: $ ssh-keygen -t rsa -C \"your@email.com\" ... -bash-4.1$ ssh-keygen -t rsa -C \"your@email.com\" Generating public/private rsa key pair. Enter file in which to save the key (/Home/user/.ssh/id_rsa): [Enter] Enter passphrase (empty for no passphrase): [Passphrase] Enter same passphrase again: [Passphrase] Your identification has been saved in /Home/user/.ssh/id_rsa. Your public key has been saved in /Home/user/.ssh/id_rsa.pub. The key fingerprint is: 03:d4:c4:6d:58:0a:e2:4a:f8:73:9a:e8:e3:07:16:c8 your@email.com The key's randomart image is: +--[ RSA 2048]----+ | . ...+o++++. | | . . . =o.. | |+ . . .......o o | |oE . . | |o = . S | |. +.+ . | |. oo | |. . | | .. | +-----------------+ (remember to replace \"your@email.com\" with your e-mail address).","title":"SSH Key Pairs"},{"location":"user-guide/connecting/#upload-public-part-of-key-pair-to-safe","text":"You should now upload the public part of your SSH key pair to the SAFE by following the instructions at: Login to SAFE . Then: Go to the Menu Login accounts and select the ARCHER2 account you want to add the SSH key to On the subsequent Login account details page click the Add Credential button Select SSH public key as the Credential Type and click Next Either copy and paste the public part of your SSH key into the SSH Public key box or use the button to select the public key file on your computer. Click Add to associate the public SSH key part with your account Once you have done this, your SSH key will be added to your ARCHER2 account. Remember, you will need to use both an SSH key and password to log into ARCHER2 so you will also need to collect your initial password before you can log into ARCHER2. We cover this next.","title":"Upload public part of key pair to SAFE"},{"location":"user-guide/connecting/#initial-passwords","text":"The SAFE web interface is used to provide your initial password for logging onto ARCHER2 (see the SAFE Documentation for more details on requesting accounts and picking up passwords). Note You may now change your password on the ARCHER2 machine itself using the passwd command or when you are prompted the first time you login. This change will not be reflected in the SAFE. If you forget your password, you should use the SAFE to request a new one-shot password.","title":"Initial passwords"},{"location":"user-guide/connecting/#ssh-clients","text":"Interaction with ARCHER2 is done remotely, over an encrypted communication channel, Secure Shell version 2 (SSH-2). This allows command-line access to one of the login nodes of a ARCHER2, from which you can run commands or use a command-line text editor to edit files. SSH can also be used to run graphical programs such as GUI text editors and debuggers when used in conjunction with an X client.","title":"SSH Clients"},{"location":"user-guide/connecting/#logging-in-from-linux-and-macos","text":"Linux distributions and MacOS each come installed with a terminal application that can be use for SSH access to the login nodes. Linux users will have different terminals depending on their distribution and window manager (e.g. GNOME Terminal in GNOME, Konsole in KDE). Consult your Linux distribution's documentation for details on how to load a terminal. MacOS users can use the Terminal application, located in the Utilities folder within the Applications folder. You can use the following command from the terminal window to login into ARCHER2: ssh username@login.archer2.ac.uk You will first be prompted for your machine account password. Once you have entered your password successfully, you will then be prompted for the passphrase associated with your SSH key pair. You need to enter both credentials correctly to be able to access ARCHER2. Tip If your SSH key pair is not stored in the default location (usually ~/.ssh/id_rsa ) on your local system, you may need to specify the path to the private part of the key wih the -i option to ssh . For example, if your key is in a file called keys/id_rsa_ARCHER2 you would use the command ssh -i keys/id_rsa_ARCHER2 username@login.archer2.ac.uk to log in. Tip When you first log into ARCHER2, you will be prompted to change your initial password. This is a three step process: When promoted to enter your ldap password : Re-enter the password you retrieved from SAFE When prompted to enter your new password: type in a new password When prompted to re-enter the new password: re-enter the new password Your password has now been changed To allow remote programs, especially graphical applications to control your local display, such as being able to open up a new GUI window (such as for a debugger), use: ssh -X username@login.archer2.ac.uk Some sites recommend using the -Y flag. While this can fix some compatibility issues, the -X flag is more secure. Current MacOS systems do not have an X window system. Users should install the XQuartz package to allow for SSH with X11 forwarding on MacOS systems: XQuartz website","title":"Logging in from Linux and MacOS"},{"location":"user-guide/connecting/#logging-in-from-windows-using-mobaxterm","text":"A typical Windows installation will not include a terminal client, though there are various clients available. We recommend all our Windows users to download and install MobaXterm to access ARCHER2. It is very easy to use and includes an integrated X server with SSH client to run any graphical applications on ARCHER2. You can download MobaXterm Home Edition (Installer Edition) from the following link: Install MobaXterm Double-click the downloaded Microsoft Installer file (.msi), and the Windows wizard will automatically guides you through the installation process. Note, you might need to have administrator rights to install on some Windows OS. Also make sure to check whether Windows Firewall hasn't blocked any features of this program after installation. Start MobaXterm using, for example, the icon added to the Start menu during the installation process. Use your ARCHER2 username, the login host should be set to login.archer2.ac.uk . If you would like to run any small remote GUI applications, then make sure to use -X option along with the ssh command (see above) to enable X11 forwarding, which allows you to run graphical clients on your local X server.","title":"Logging in from Windows using MobaXterm"},{"location":"user-guide/connecting/#making-access-more-convenient-using-the-ssh-configuration-file","text":"Typing in the full command to login or transfer data to ARCHER2 can become tedious as it often has to be repeated many times. You can use the SSH configuration file, usually located on your local machine at .ssh/config to make things a bit more convenient. Each remote site (or group of sites) can have an entry in this file which may look something like: Host archer2 HostName login.archer2.ac.uk User username (remember to replace username with your actual username!). The Host archer2 line defines a short name for the entry. In this case, instead of typing ssh username@login.archer2.ac.uk to access the ARCHER2 login nodes, you could use ssh archer2 instead. The remaining lines define the options for the archer2 host. Hostname login.archer2.ac.uk - defines the full address of the host User username - defines the username to use by default for this host (replace username with your own username on the remote host) Now you can use SSH to access ARCHER2 without needing to enter your username or the full hostname every time: $ ssh archer2 You can set up as many of these entries as you need in your local configuration file. Other options are available. See the ssh_config man page (or man ssh_config on any machine with SSH installed) for a description of the SSH configuration file. You may find the IdentityFile option useful if you have to manage multiple SSH key pairs for different systems as this allows you to specify which SSH key to use for each system. Bug There is a known bug with Windows ssh-agent. If you get the error message: Warning: agent returned different signature type ssh-rsa (expected rsa-sha2-512) , you will need to either specify the path to your ssh key in the command line (using the -i option as described above) or add the path to your SSH config file by using the IdentityFile option.","title":"Making access more convenient using the SSH configuration file"},{"location":"user-guide/connecting/#ssh-debugging-tips","text":"If you find you are unable to connect via SSH there are a number of ways you can try and diagnose the issue. Some of these are collected below - if you are having difficulties connecting we suggest trying these before contacting the ARCHER2 service desk.","title":"SSH debugging tips"},{"location":"user-guide/connecting/#can-you-connect-to-the-login-node","text":"Try the command ping -c 3 login.archer2.ac.uk . If you successfully connect to the login node, the output should include: --- login.dyn.archer2.ac.uk ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 38ms (the ping time '38ms' is not important). If not all packets are received there could be a problem with your internet connection, or the login node could be unavailable.","title":"Can you connect to the login node?"},{"location":"user-guide/connecting/#password","text":"If you are having trouble entering your password consider using a password manager, from which you can copy and paste it. This will also help you generate a secure password. If you need to reset your password, instructions for doing so can be found in the SAFE documentation Windows users please note that Ctrl+V does not work to paste in to PuTTY, MobaXterm, or PowerShell. Instead use Shift+Ins to paste. Alternatively, right-click and select 'Paste' in PuTTY and MobaXterm, or simply right-click to paste in PowerShell.","title":"Password"},{"location":"user-guide/connecting/#ssh-key","text":"If you get the error message Permission denied (publickey) this can indicate a problem with your SSH key. Some things to check: Have you uploaded the key to SAFE? Please note that if the same key is reuploaded SAFE will not map the \"new\" key to ARCHER2. If for some reason this is required, please delete the key first, then reupload. Is ssh using the correct key? You can check which keys are being found and offered by ssh using ssh -vvv . If your private key has a non-default name you can use the -i flag to provide it to ssh, i.e. ssh -i path/to/key username@login.archer2.ac.uk . Are you entering the passphrase correctly? You will be asked for your private key's passphrase first. If you enter it incorrectly you will usually be asked to enter it again, and usually up to three times in total, after which ssh will fail with Permission denied (publickey) . If you would like to confirm your passphrase without attempting to connect, you can use ssh-keygen -y -f /path/to/private/key . If successful, this command will print the corresponding public key. You can also use this to check it is the one uploaded to SAFE. Are permissions correct on the ssh key? One common issue is that the permissions are incorrect on the either the key file, or the directory it's contained in. On Linux/MacOS for example, if your private keys are held in ~/.ssh/ you can check this with ls -al ~/.ssh . This should give something similar to the following output: $ ls -al ~/.ssh/ drwx------. 2 user group 48 Jul 15 20:24 . drwx------. 12 user group 4096 Oct 13 12:11 .. -rw-------. 1 user group 113 Jul 15 20:23 authorized_keys -rw-------. 1 user group 12686 Jul 15 20:23 id_rsa -rw-r--r--. 1 user group 2785 Jul 15 20:23 id_rsa.pub -rw-r--r--. 1 user group 1967 Oct 13 14:11 known_hosts The important section here is the string of letters and dashes at the start, for the lines ending in . , id_rsa , and id_rsa.pub , which indicate permissions on the containing directory, private key, and public key respectively. If your permissions are not correct, they can be set with chmod . Consult the table below for the relevant chmod command. On Windows, permissions are handled differently but can be set by right-clicking on the file and selecting Properties > Security > Advanced. The user, SYSTEM, and Administrators should have Full control , and no other permissions should exist for both public and private key files, and the containing folder. Target Permissions chmod Code Directory drwx------ 700 Private Key -rw------- 600 Public Key -rw-r--r-- 644 chmod can be used to set permissions on the target in the following way: chmod <code> <target> . So for example to set correct permissions on the private key file id_rsa_ARCHER2 one would use the command chmod 600 id_rsa_ARCHER2 . Tip Unix file permissions can be understood in the following way. There are three groups that can have file permissions: (owning) users , (owning) groups , and others . The available permissions are read , write , and execute . The first character indicates whether the target is a file - , or directory d . The next three characters indicate the owning user's permissions. The first character is r if they have read permission, - if they don't, the second character is w if they have write permission, - if they don't, the third character is x if they have execute permission, - if they don't. This pattern is then repeated for group , and other permissions. For example the pattern -rw-r--r-- indicates that the owning user can read and write the file, members of the owning group can read it, and anyone else can also read it. The chmod codes are constructed by treating the user, group, and owner permission strings as binary numbers, then converting them to decimal. For example the permission string -rwx------ becomes 111 000 000 -> 700 .","title":"SSH key"},{"location":"user-guide/connecting/#ssh-verbose-output","text":"Verbose debugging output from ssh can be very useful for diagnosing the issue. In particular, it can be used to distinguish between problems with the SSH key and password - further details are given below. To enable verbose output add the -vvv flag to your SSH command. For example: ssh -vvv username@login.archer2.ac.uk The output is lengthy, but somewhere in there you should see lines similar to the following: debug1: Next authentication method: keyboard-interactive debug2: userauth_kbdint debug3: send packet: type 50 debug2: we sent a keyboard-interactive packet, wait for reply debug3: receive packet: type 60 debug2: input_userauth_info_req debug2: input_userauth_info_req: num_prompts 1 Password: debug3: send packet: type 61 debug3: receive packet: type 60 debug2: input_userauth_info_req debug2: input_userauth_info_req: num_prompts 0 debug3: send packet: type 61 debug3: receive packet: type 51 Authenticated with partial success. debug1: Authentications that can continue: publickey,password If you do not see the Password: prompt you may have connection issues, or there could be a problem with the ARCHER2 login nodes. If you do not see Authenticated with partial success it means your password was not accepted. You will be asked to re-enter your password, usually two more times before the connection will be rejected. Consider the suggestions under Password above. If you do see Authenticated with partial success , it means your password was accepted, and your SSH key will now be checked. You should next see something similiar to: debug1: Next authentication method: publickey debug1: Offering public key: RSA SHA256:<key_hash> <path_to_private_key> debug3: send_pubkey_test debug3: send packet: type 50 debug2: we sent a publickey packet, wait for reply debug3: receive packet: type 60 debug1: Server accepts key: pkalg rsa-sha2-512 blen 2071 debug2: input_userauth_pk_ok: fp SHA256:<key_hash> debug3: sign_and_send_pubkey: RSA SHA256:<key_hash> Enter passphrase for key '<path_to_private_key>': debug3: send packet: type 50 debug3: receive packet: type 52 debug1: Authentication succeeded (publickey). Most importantly, you can see which files ssh has checked for private keys, and you can see if any key is accepted. The line Authenticated succeeded indicates that the SSH key has been accepted. By default ssh will go through a list of standard private key files, as well as any you have specified with -i or a config file. This is fine, as long as one of the files mentioned is the one that matches the public key uploaded to SAFE. If your SSH key passphrase is incorrect, you will be asked to try again up to three times in total, before being disconnected with Permission denied (publickey) . If you enter your passphrase correctly, but still see this error message, please consider the advice under SSH key above. The equivalent information can be obtained in PuTTY or MobaXterm by enabling all logging in settings.","title":"SSH verbose output"},{"location":"user-guide/containers/","text":"Containers Warning The ARCHER2 Service is not yet available. This documentation is in development. This page was originally based on the documentation at the University of Sheffield HPC service Designed around the notion of mobility of compute and reproducible science, Singularity enables users to have full control of their operating system environment. This means that a non-privileged user can \"swap out\" the Linux operating system and environment on the host for a Linux OS and environment that they control. So if the host system is running CentOS Linux but your application runs in Ubuntu Linux with a particular software stack; you can create an Ubuntu image, install your software into that image, copy the image to another host (e.g. ARCHER2), and run your application on that host in it\u2019s native Ubuntu environment. Singularity also allows you to leverage the resources of whatever host you are on. This includes high-speed interconnects (i.e. Slingshot on ARCHER2), file systems (i.e. /lustre on ARCHER2) and potentially other resources. Note Singularity only supports Linux containers. You cannot create images that use Windows or macOS (this is a restriction of the containerisation model rather than Singularity). Useful Links Singularity website Singularity documentation About Singularity Containers (Images) Similar to Docker, a Singularity container (or, more commonly, image ) is a self-contained software stack. As Singularity does not require a root-level daemon to run its images (as is required by Docker) it is suitable for use on multi-user HPC systems such as ARCHER2. Within the container/image, you have exactly the same permissions as you do in a standard login session on the system. In practice, this means that an image created on your local machine with all your research software installed for local development will also run on ARCHER2. Pre-built images (such as those on DockerHub or SingularityHub ) can simply be downloaded and used on ARCHER2 (or anywhere else Singularity is installed). Creating and modifying images requires root permission and so must be done on a system where you have such access (in practice, this is usually within a virtual machine on your laptop/workstation). Using Singularity Images on ARCHER2 Singularity images can be used on ARCHER2 in a number of ways, including: Interactively on the login nodes Interactively on compute nodes As serial processes within a non-interactive batch script As parallel processes within a non-interactive batch script We provide information on each of these scenarios (apart from the parallel use where we are still preparing the documentation) below. First, we describe briefly how to get existing images onto ARCHER2 so you can use them. Getting existing images onto ARCHER2 Singularity images are files, so, if you already have an image file, you can use scp to copy the file to ARCHER2 as you would with any other file. If you wish to get a file from one of the container image repositories then Singularity allows you to do this from ARCHER2 itself. For example, to retrieve an image from SingularityHub on Cirrus we can simply issue a Singularity command to pull the image. [user@archer2-login0 ~]$ singularity pull hello-world.sif shub://vsoch/hello-world The image located at the shub URI is written to a Singularity Image File (SIF) called hello-world.sif . Interactive use on the login nodes Once you have an image file, using it on the login nodes in an interactive way is extremely simple: you use the singularity shell command. Using the image we built in the example above: [user@archer2-login0 ~]$ singularity shell lolcow.simg Singularity: Invoking an interactive shell within container... Singularity lolcow.simg:~> Within a Singularity image your home directory will be available. The directory with centrally-installed software ( /lustre/sw ) is also available in images by default. !!! note that the module command will not work in images unless you have installed the required software and configured the environment correctly; we describe how to do this below. Once you have finished using your image, you can return to the ARCHER2 login node command line with the exit command: Singularity lolcow.simg:~> exit exit [user@archer2-login0 ~]$ Interactive use on the compute nodes The process for using an image interactively on the compute nodes is very similar to that for using them on the login nodes. The only difference is that you have to submit an interactive serial job to get interactive access to the compute node first. For example, to reserve a full node for you to work on interactively you would use: auser@uan01:/work/t01/t01/auser> srun --nodes=1 --exclusive --time=00:20:00 --account=[] \\ --partition=standard --qos=standard --pty /bin/bash ...wait until job starts... auser@nid00001:/work/t01/t01/auser> Note that the prompt has changed to show you are on a compute node. Now you can use the image in the same way as on the login node. auser@nid00001:/work/t01/t01/auser> singularity shell lolcow.simg Singularity: Invoking an interactive shell within container... Singularity lolcow.simg:~> exit exit auser@nid00001:/work/t01/t01/auser> exit auser@uan01:/work/t01/t01/auser> Note We used exit to leave the interactive image shell and then exit again to leave the interactive job on the compute node. Serial processes within a non-interactive batch script You can also use Singularity images within a non-interactive batch script as you would any other command. If your image contains a runscript then you can use singularity run to execute the runscript in the job. You can also use singularity exec to execute arbitrary commands (or scripts) within the image. An example job submission script to run a serial job that executes the runscript within the lolcow.simg image that we built previously on an ARCHER2 login node would be as follows. #!/bin/bash --login # Slurm job options (name, compute nodes, job time) #SBATCH --job-name=simgtest #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=00:10:00 #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the batch environment module load epcc-job-env # Run the serial executable singularity run $HOME/lolcow.simg You submit this in the usual way and the standard output and error should be written to slurm-... , where the output filename ends with the job number. Creating Your Own Singularity Images As we saw above, you can create Singularity images by importing from DockerHub or Singularity Hub on ARCHER2 itself. If you wish to create your own custom image then you must install Singularity on a system where you have root (or administrator) privileges - often your own laptop or workstation. We provide links below to instructions on how to install Singularity locally and then cover what options you need to include in a Singularity recipe file to create images that can run on ARCHER2 and access the software development modules. (This can be useful if you want to create a custom environment but still want to compile and link against libraries that you only have access to on ARCHER2 such as the Intel compilers, HPE MPI libraries, etc.) Installing Singularity on Your Local Machine You will need Singularity installed on your machine in order to locally run, create and modify images. How you install Singularity on your laptop/workstation depends on the operating system you are using. If you are using Windows or macOS, the simplest solution is to use Vagrant to give you an easy to use virtual environment with Linux and Singularity installed. The Singularity website has instructions on how to use this method to install Singularity: Installing Singularity on macOS with Vagrant Installing Singularity on Windows with Vagrant If you are using Linux then you can usually install Singularity directly, see: Installing Singularity on Linux","title":"Containers"},{"location":"user-guide/containers/#containers","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development. This page was originally based on the documentation at the University of Sheffield HPC service Designed around the notion of mobility of compute and reproducible science, Singularity enables users to have full control of their operating system environment. This means that a non-privileged user can \"swap out\" the Linux operating system and environment on the host for a Linux OS and environment that they control. So if the host system is running CentOS Linux but your application runs in Ubuntu Linux with a particular software stack; you can create an Ubuntu image, install your software into that image, copy the image to another host (e.g. ARCHER2), and run your application on that host in it\u2019s native Ubuntu environment. Singularity also allows you to leverage the resources of whatever host you are on. This includes high-speed interconnects (i.e. Slingshot on ARCHER2), file systems (i.e. /lustre on ARCHER2) and potentially other resources. Note Singularity only supports Linux containers. You cannot create images that use Windows or macOS (this is a restriction of the containerisation model rather than Singularity).","title":"Containers"},{"location":"user-guide/containers/#useful-links","text":"Singularity website Singularity documentation","title":"Useful Links"},{"location":"user-guide/containers/#about-singularity-containers-images","text":"Similar to Docker, a Singularity container (or, more commonly, image ) is a self-contained software stack. As Singularity does not require a root-level daemon to run its images (as is required by Docker) it is suitable for use on multi-user HPC systems such as ARCHER2. Within the container/image, you have exactly the same permissions as you do in a standard login session on the system. In practice, this means that an image created on your local machine with all your research software installed for local development will also run on ARCHER2. Pre-built images (such as those on DockerHub or SingularityHub ) can simply be downloaded and used on ARCHER2 (or anywhere else Singularity is installed). Creating and modifying images requires root permission and so must be done on a system where you have such access (in practice, this is usually within a virtual machine on your laptop/workstation).","title":"About Singularity Containers (Images)"},{"location":"user-guide/containers/#using-singularity-images-on-archer2","text":"Singularity images can be used on ARCHER2 in a number of ways, including: Interactively on the login nodes Interactively on compute nodes As serial processes within a non-interactive batch script As parallel processes within a non-interactive batch script We provide information on each of these scenarios (apart from the parallel use where we are still preparing the documentation) below. First, we describe briefly how to get existing images onto ARCHER2 so you can use them.","title":"Using Singularity Images on ARCHER2"},{"location":"user-guide/containers/#getting-existing-images-onto-archer2","text":"Singularity images are files, so, if you already have an image file, you can use scp to copy the file to ARCHER2 as you would with any other file. If you wish to get a file from one of the container image repositories then Singularity allows you to do this from ARCHER2 itself. For example, to retrieve an image from SingularityHub on Cirrus we can simply issue a Singularity command to pull the image. [user@archer2-login0 ~]$ singularity pull hello-world.sif shub://vsoch/hello-world The image located at the shub URI is written to a Singularity Image File (SIF) called hello-world.sif .","title":"Getting existing images onto ARCHER2"},{"location":"user-guide/containers/#interactive-use-on-the-login-nodes","text":"Once you have an image file, using it on the login nodes in an interactive way is extremely simple: you use the singularity shell command. Using the image we built in the example above: [user@archer2-login0 ~]$ singularity shell lolcow.simg Singularity: Invoking an interactive shell within container... Singularity lolcow.simg:~> Within a Singularity image your home directory will be available. The directory with centrally-installed software ( /lustre/sw ) is also available in images by default. !!! note that the module command will not work in images unless you have installed the required software and configured the environment correctly; we describe how to do this below. Once you have finished using your image, you can return to the ARCHER2 login node command line with the exit command: Singularity lolcow.simg:~> exit exit [user@archer2-login0 ~]$","title":"Interactive use on the login nodes"},{"location":"user-guide/containers/#interactive-use-on-the-compute-nodes","text":"The process for using an image interactively on the compute nodes is very similar to that for using them on the login nodes. The only difference is that you have to submit an interactive serial job to get interactive access to the compute node first. For example, to reserve a full node for you to work on interactively you would use: auser@uan01:/work/t01/t01/auser> srun --nodes=1 --exclusive --time=00:20:00 --account=[] \\ --partition=standard --qos=standard --pty /bin/bash ...wait until job starts... auser@nid00001:/work/t01/t01/auser> Note that the prompt has changed to show you are on a compute node. Now you can use the image in the same way as on the login node. auser@nid00001:/work/t01/t01/auser> singularity shell lolcow.simg Singularity: Invoking an interactive shell within container... Singularity lolcow.simg:~> exit exit auser@nid00001:/work/t01/t01/auser> exit auser@uan01:/work/t01/t01/auser> Note We used exit to leave the interactive image shell and then exit again to leave the interactive job on the compute node.","title":"Interactive use on the compute nodes"},{"location":"user-guide/containers/#serial-processes-within-a-non-interactive-batch-script","text":"You can also use Singularity images within a non-interactive batch script as you would any other command. If your image contains a runscript then you can use singularity run to execute the runscript in the job. You can also use singularity exec to execute arbitrary commands (or scripts) within the image. An example job submission script to run a serial job that executes the runscript within the lolcow.simg image that we built previously on an ARCHER2 login node would be as follows. #!/bin/bash --login # Slurm job options (name, compute nodes, job time) #SBATCH --job-name=simgtest #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=00:10:00 #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the batch environment module load epcc-job-env # Run the serial executable singularity run $HOME/lolcow.simg You submit this in the usual way and the standard output and error should be written to slurm-... , where the output filename ends with the job number.","title":"Serial processes within a non-interactive batch script"},{"location":"user-guide/containers/#creating-your-own-singularity-images","text":"As we saw above, you can create Singularity images by importing from DockerHub or Singularity Hub on ARCHER2 itself. If you wish to create your own custom image then you must install Singularity on a system where you have root (or administrator) privileges - often your own laptop or workstation. We provide links below to instructions on how to install Singularity locally and then cover what options you need to include in a Singularity recipe file to create images that can run on ARCHER2 and access the software development modules. (This can be useful if you want to create a custom environment but still want to compile and link against libraries that you only have access to on ARCHER2 such as the Intel compilers, HPE MPI libraries, etc.)","title":"Creating Your Own Singularity Images"},{"location":"user-guide/containers/#installing-singularity-on-your-local-machine","text":"You will need Singularity installed on your machine in order to locally run, create and modify images. How you install Singularity on your laptop/workstation depends on the operating system you are using. If you are using Windows or macOS, the simplest solution is to use Vagrant to give you an easy to use virtual environment with Linux and Singularity installed. The Singularity website has instructions on how to use this method to install Singularity: Installing Singularity on macOS with Vagrant Installing Singularity on Windows with Vagrant If you are using Linux then you can usually install Singularity directly, see: Installing Singularity on Linux","title":"Installing Singularity on Your Local Machine"},{"location":"user-guide/data/","text":"Data management and transfer Warning The ARCHER2 Service is not yet available. This documentation is in development. This section covers best practice and tools for data management on ARCHER2. Tip If you have any questions on data management and transfer please do not hesitate to contact the ARCHER2 service desk at support@archer2.ac.uk . Useful resources and links Harry Mangalam's guide on How to transfer large amounts of data via network . This provides lots of useful advice on transferring data though we now recommend using Globus Online, rather than GridFTP directly. Data management We strongly recommend that you give some thought to how you use the various data storage facilities that are part of the ARCHER2 service. This will not only allow you to use the machine more effectively but also to ensure that your valuable data is protected. ARCHER2 storage The ARCHER2 service, like many HPC systems has a complex structure. There are a number of different data storage types available to users: Home file systems Work file systems Each type of storage has different characteristics and policies, and is suitable for different types of use. There are also two different types of node available to users: Login nodes Compute nodes Each type of node sees a different combination of the storage types. Home file systems There are four independent home file-systems. Every project has an allocation on one of the four. You do not need to know which one your project uses as your projects space can always be accessed via the path /home/project-code . Each home file-system is approximately 60TB in size and is implemented using standard Network Attached Storage (NAS) technology. This means that these disks are not particularly high performance but are well suited to standard operations like compilation and file editing. These file systems are visible from the ARCHER2 login nodes and the pre-/post-processing nodes. The home file systems are fully backed up . Full backups are taken weekly with incremental backups added every day in between. Backups are kept for disaster recovery purposes only. If you have accidentally lost data from a backed-up file-system and have no other way of recovering the data then contact us as quickly as possible but we may be unable to assist. These file-systems are a good location to keep source-code, copies of scripts and compiled binaries. Small amounts of important data can also be copied here for safe keeping though the file systems are not fast enough to manipulate large datasets effectively. Warning Files with filenames that contain non-ascii characters and/or non-printable characters cannot be backed up using our automated process and so will be omitted from all backups. Work file systems There is one work file-systems: /work 3.4 PB Every project has an allocation on the file system. This is a high-performance, Lustre parallel file system. It is designed to support data in large files. The performance for data stored in large numbers of small files is probably not going to be as good. This is the only file system that is available on the compute nodes so all data read or written by jobs running on the compute nodes has to be hosted here. Warning There are no backups of any data on the work file system. You should not rely on these file systems for long term storage. Ideally, this file system should only contain data that is: actively in use; recently generated and in the process of being saved elsewhere; or being made ready for up-coming work. In practice it may be convenient to keep copies of datasets on the work file system that you know will be needed at a later date. However, make sure that important data is always backed up elsewhere and that your work would not be significantly impacted if the data on the work file system was lost. Large data sets can be moved to the RDF storage or transferred off the ARCHER2 service entirely. If you have data on the work file system that you are not going to need in the future please delete it. Archiving and data transfer Data transfer speed may be limited by many different factors so the best data transfer mechanism to use depends on the type of data being transferred and where the data is going. Disk speed - The ARCHER2 /work file system is highly parallel, consisting of a very large number of high performance disk drives. This allows it to support a very high data bandwidth. Unless the remote system has a similar parallel file-system you may find your transfer speed limited by disk performance. Meta-data performance - Meta-data operations such as opening and closing files or listing the owner or size of a file are much less parallel than read/write operations. If your data consists of a very large number of small files you may find your transfer speed is limited by meta-data operations. Meta-data operations performed by other users of the system will interact strongly with those you perform so reducing the number of such operations you use, may reduce variability in your IO timings. Network speed - Data transfer performance can be limited by network speed. More importantly it is limited by the slowest section of the network between source and destination. Firewall speed - Most modern networks are protected by some form of firewall that filters out malicious traffic. This filtering has some overhead and can result in a reduction in data transfer performance. The needs of a general purpose network that hosts email/web-servers and desktop machines are quite different from a research network that needs to support high volume data transfers. If you are trying to transfer data to or from a host on a general purpose network you may find the firewall for that network will limit the transfer rate you can achieve. The method you use to transfer data to/from ARCHER2 will depend on how much you want to transfer and where to. The methods we cover in this guide are: scp/sftp/rsync - These are the simplest methods of transferring data and can be used up to moderate amounts of data. If you are transferring data to your workstation/laptop then this is the method you will use. Before discussing specific data transfer methods, we cover archiving which is an essential process for transferring data efficiently. Archiving If you have related data that consists of a large number of small files it is strongly recommended to pack the files into a larger \"archive\" file for ease of transfer and manipulation. A single large file makes more efficient use of the file system and is easier to move and copy and transfer because significantly fewer meta-data operations are required. Archive files can be created using tools like tar and zip . tar The tar command packs files into a \"tape archive\" format. The command has general form: tar [options] [file(s)] Common options include: -c create a new archive -v verbosely list files processed -W verify the archive after writing -l confirm all file hard links are included in the archive -f use an archive file (for historical reasons, tar writes its output to stdout by default rather than a file). Putting these together: tar -cvWlf mydata.tar mydata will create and verify an archive. To extract files from a tar file, the option -x is used. For example: tar -xf mydata.tar will recover the contents of mydata.tar to the current working directory. To verify an existing tar file against a set of data, the -d (diff) option can be used. By default, no output will be given if a verification succeeds and an example of a failed verification follows: $> tar -df mydata.tar mydata/* mydata/damaged_file: Mod time differs mydata/damaged_file: Size differs Note tar files do not store checksums with their data, requiring the original data to be present during verification. Tip Further information on using tar can be found in the tar manual (accessed via man tar or at man tar ). zip The zip file format is widely used for archiving files and is supported by most major operating systems. The utility to create zip files can be run from the command line as: zip [options] mydata.zip [file(s)] Common options are: -r used to zip up a directory -# where \"#\" represents a digit ranging from 0 to 9 to specify compression level, 0 being the least and 9 the most. Default compression is -6 but we recommend using -0 to speed up the archiving process. Together: zip -0r mydata.zip mydata will create an archive. Note Unlike tar, zip files do not preserve hard links. File data will be copied on archive creation, e.g. an uncompressed zip archive of a 100MB file and a hard link to that file will be approximately 200MB in size. This makes zip an unsuitable format if you wish to precisely reproduce the file system layout. The corresponding unzip command is used to extract data from the archive. The simplest use case is: unzip mydata.zip which recovers the contents of the archive to the current working directory. Files in a zip archive are stored with a CRC checksum to help detect data loss. unzip provides options for verifying this checksum against the stored files. The relevant flag is -t and is used as follows: $> unzip -t mydata.zip Archive: mydata.zip testing: mydata/ OK testing: mydata/file OK No errors detected in compressed data of mydata.zip. Tip Further information on using zip can be found in the zip manual (accessed via man zip or at man zip ). Data transfer via SSH The easiest way of transferring data to/from ARCHER2 is to use one of the standard programs based on the SSH protocol such as scp , sftp or rsync . These all use the same underlying mechanism (SSH) as you normally use to log-in to ARCHER2. So, once the the command has been executed via the command line, you will be prompted for your password for the specified account on the remote machine (ARCHER2 in this case). To avoid having to type in your password multiple times you can set up a SSH key pair and use an SSH agent as documented in the User Guide at connecting . SSH data transfer performance considerations The SSH protocol encrypts all traffic it sends. This means that file transfer using SSH consumes a relatively large amount of CPU time at both ends of the transfer (for encryption and decryption). The ARCHER2 login nodes have fairly fast processors that can sustain about 100 MB/s transfer. The encryption algorithm used is negotiated between the SSH client and the SSH server. There are command line flags that allow you to specify a preference for which encryption algorithm should be used. You may be able to improve transfer speeds by requesting a different algorithm than the default. The aes128-ctr or aes256-ctr algorithms are well supported and fast as they are implemented in hardware. These are not usually the default choice when using scp so you will need to manually specify them. A single SSH based transfer will usually not be able to saturate the available network bandwidth or the available disk bandwidth so you may see an overall improvement by running several data transfer operations in parallel. To reduce metadata interactions it is a good idea to overlap transfers of files from different directories. In addition, you should consider the following when transferring data: Only transfer those files that are required. Consider which data you really need to keep. Combine lots of small files into a single tar archive, to reduce the overheads associated in initiating many separate data transfers (over SSH, each file counts as an individual transfer). Compress data before transferring it, e.g. using gzip . scp The scp command creates a copy of a file, or if given the -r flag, a directory either from a local machine onto a remote machine or from a remote machine onto a local machine. For example, to transfer files to ARCHER2 from a local machine: scp [options] source user@login.archer2.ac.uk:[destination] (Remember to replace user with your ARCHER2 username in the example above.) In the above example, the [destination] is optional, as when left out scp will copy the source into your home directory. Also, the source should be the absolute path of the file/directory being copied or the command should be executed in the directory containing the source file/directory. If you want to request a different encryption algorithm add the -c [algorithm-name] flag to the scp options. For example, to use the (usually faster) arcfour encryption algorithm you would use: scp [options] -c aes128-ctr source user@login.archer2.ac.uk:[destination] (Remember to replace user with your ARCHER2 username in the example above.) rsync The rsync command can also transfer data between hosts using a ssh connection. It creates a copy of a file or, if given the -r flag, a directory at the given destination, similar to scp above. Given the -a option rsync can also make exact copies (including permissions), this is referred to as mirroring . In this case the rsync command is executed with ssh to create the copy on a remote machine. To transfer files to ARCHER2 using rsync with ssh the command has the form: rsync [options] -e ssh source user@login.archer2.ac.uk:[destination] (Remember to replace user with your ARCHER2 username in the example above.) In the above example, the [destination] is optional, as when left out rsync will copy the source into your home directory. Also the source should be the absolute path of the file/directory being copied or the command should be executed in the directory containing the source file/directory. Additional flags can be specified for the underlying ssh command by using a quoted string as the argument of the -e flag. e.g. rsync [options] -e \"ssh -c arcfour\" source user@login.archer2.ac.uk:[destination] (Remember to replace user with your ARCHER2 username in the example above.) Tip Further information on using rsync can be found in the rsync manual (accessed via man rsync or at man rsync ). SSH data transfer example Here we have a short example demonstrating transfer of data directly from ARCHER to ARCHER2. The first step will be to set up an SSH key for access to ARCHER2 directly from ARCHER. First log in to ARCHER, and generate a new SSH key. To do this we use the following command: ssh-keygen -b 4096 -C \"otbz01@archer -> otbz19@archer2\" -f ~/.ssh/id_RSA_A2 This generates a new 4096 bit RSA SSH key with the comment otbz01@archer -> otbz19archer2 , and stores the private key in the file ~/.ssh/id_RSA_A2 , and the public key in a corresponding .pub file. During key generation process we are asked to enter a passphrase. SSH keys should always be passphrase protected, but this is especially important when they are kept on a publicly accessible machine such as ARCHER. Please do not leave the passphrase blank when setting up your SSH key. Next we must add the new public key to ARCHER2 through SAFE. We can either open the id_RSA_A2.pub file in our preferred text editor on ARCHER, or use cat id_RSA_A2.pub to output the file contents to screen. Either way we should carefully copy the full SSH key and comment, and then in SAFE go to *Login accounts - \\<user>@archer2 - Add Credential* to add the new SSH key. Once the new key is active you can test that this has worked by attempting to ssh to ARCHER2 from ARCHER. All being well, we are now ready to transfer data directly between the two machines. We begin by combining our important research data in to a single archive file using the following command: tar -czf cat_pictures.tar.gz Cally.jpg Oscar.jpg Marvin.jpg The three important research images. Cally, Oscar, and Marvin (clockwise left to bottom right). We then initiate the data transfer from ARCHER to ARCHER2, here using rsync to allow the transfer to be recommenced without needing to start again, in the event of a loss of connection or other failure. rsync -Pv -e\"ssh -i /home/z01/z01/otbz01/.ssh/id_RSA_A2\" ./cat_pictures.tar.gz otbz19@login1.archer2.ac.uk:/work/z19/z19/otbz19/ Note the use of the -P flag to allow partial transfer -- the same command could be used to restart the transfer after a loss of connection. The -e flag allows specification of the ssh command - we have used this to add the location of the identity file. Unfortunately the ~ shortcut is not correctly expanded, so we have specified the full path. We move our research archive to our project work directory on ARCHER2. If we were unconcerned about being able to restart an interrupted transfer, we could instead use the scp command, scp -i ~/.ssh/id_RSA_A2 cat_pictures.tar.gz otbz19@login1.archer2.ac.uk:/work/z19/z19/otbz19/ but rsync is recommended for larger transfers.","title":"Data management and transfer"},{"location":"user-guide/data/#data-management-and-transfer","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development. This section covers best practice and tools for data management on ARCHER2. Tip If you have any questions on data management and transfer please do not hesitate to contact the ARCHER2 service desk at support@archer2.ac.uk .","title":"Data management and transfer"},{"location":"user-guide/data/#useful-resources-and-links","text":"Harry Mangalam's guide on How to transfer large amounts of data via network . This provides lots of useful advice on transferring data though we now recommend using Globus Online, rather than GridFTP directly.","title":"Useful resources and links"},{"location":"user-guide/data/#data-management","text":"We strongly recommend that you give some thought to how you use the various data storage facilities that are part of the ARCHER2 service. This will not only allow you to use the machine more effectively but also to ensure that your valuable data is protected.","title":"Data management"},{"location":"user-guide/data/#archer2-storage","text":"The ARCHER2 service, like many HPC systems has a complex structure. There are a number of different data storage types available to users: Home file systems Work file systems Each type of storage has different characteristics and policies, and is suitable for different types of use. There are also two different types of node available to users: Login nodes Compute nodes Each type of node sees a different combination of the storage types.","title":"ARCHER2 storage"},{"location":"user-guide/data/#home-file-systems","text":"There are four independent home file-systems. Every project has an allocation on one of the four. You do not need to know which one your project uses as your projects space can always be accessed via the path /home/project-code . Each home file-system is approximately 60TB in size and is implemented using standard Network Attached Storage (NAS) technology. This means that these disks are not particularly high performance but are well suited to standard operations like compilation and file editing. These file systems are visible from the ARCHER2 login nodes and the pre-/post-processing nodes. The home file systems are fully backed up . Full backups are taken weekly with incremental backups added every day in between. Backups are kept for disaster recovery purposes only. If you have accidentally lost data from a backed-up file-system and have no other way of recovering the data then contact us as quickly as possible but we may be unable to assist. These file-systems are a good location to keep source-code, copies of scripts and compiled binaries. Small amounts of important data can also be copied here for safe keeping though the file systems are not fast enough to manipulate large datasets effectively. Warning Files with filenames that contain non-ascii characters and/or non-printable characters cannot be backed up using our automated process and so will be omitted from all backups.","title":"Home file systems"},{"location":"user-guide/data/#work-file-systems","text":"There is one work file-systems: /work 3.4 PB Every project has an allocation on the file system. This is a high-performance, Lustre parallel file system. It is designed to support data in large files. The performance for data stored in large numbers of small files is probably not going to be as good. This is the only file system that is available on the compute nodes so all data read or written by jobs running on the compute nodes has to be hosted here. Warning There are no backups of any data on the work file system. You should not rely on these file systems for long term storage. Ideally, this file system should only contain data that is: actively in use; recently generated and in the process of being saved elsewhere; or being made ready for up-coming work. In practice it may be convenient to keep copies of datasets on the work file system that you know will be needed at a later date. However, make sure that important data is always backed up elsewhere and that your work would not be significantly impacted if the data on the work file system was lost. Large data sets can be moved to the RDF storage or transferred off the ARCHER2 service entirely. If you have data on the work file system that you are not going to need in the future please delete it.","title":"Work file systems"},{"location":"user-guide/data/#archiving-and-data-transfer","text":"Data transfer speed may be limited by many different factors so the best data transfer mechanism to use depends on the type of data being transferred and where the data is going. Disk speed - The ARCHER2 /work file system is highly parallel, consisting of a very large number of high performance disk drives. This allows it to support a very high data bandwidth. Unless the remote system has a similar parallel file-system you may find your transfer speed limited by disk performance. Meta-data performance - Meta-data operations such as opening and closing files or listing the owner or size of a file are much less parallel than read/write operations. If your data consists of a very large number of small files you may find your transfer speed is limited by meta-data operations. Meta-data operations performed by other users of the system will interact strongly with those you perform so reducing the number of such operations you use, may reduce variability in your IO timings. Network speed - Data transfer performance can be limited by network speed. More importantly it is limited by the slowest section of the network between source and destination. Firewall speed - Most modern networks are protected by some form of firewall that filters out malicious traffic. This filtering has some overhead and can result in a reduction in data transfer performance. The needs of a general purpose network that hosts email/web-servers and desktop machines are quite different from a research network that needs to support high volume data transfers. If you are trying to transfer data to or from a host on a general purpose network you may find the firewall for that network will limit the transfer rate you can achieve. The method you use to transfer data to/from ARCHER2 will depend on how much you want to transfer and where to. The methods we cover in this guide are: scp/sftp/rsync - These are the simplest methods of transferring data and can be used up to moderate amounts of data. If you are transferring data to your workstation/laptop then this is the method you will use. Before discussing specific data transfer methods, we cover archiving which is an essential process for transferring data efficiently.","title":"Archiving and data transfer"},{"location":"user-guide/data/#archiving","text":"If you have related data that consists of a large number of small files it is strongly recommended to pack the files into a larger \"archive\" file for ease of transfer and manipulation. A single large file makes more efficient use of the file system and is easier to move and copy and transfer because significantly fewer meta-data operations are required. Archive files can be created using tools like tar and zip .","title":"Archiving"},{"location":"user-guide/data/#tar","text":"The tar command packs files into a \"tape archive\" format. The command has general form: tar [options] [file(s)] Common options include: -c create a new archive -v verbosely list files processed -W verify the archive after writing -l confirm all file hard links are included in the archive -f use an archive file (for historical reasons, tar writes its output to stdout by default rather than a file). Putting these together: tar -cvWlf mydata.tar mydata will create and verify an archive. To extract files from a tar file, the option -x is used. For example: tar -xf mydata.tar will recover the contents of mydata.tar to the current working directory. To verify an existing tar file against a set of data, the -d (diff) option can be used. By default, no output will be given if a verification succeeds and an example of a failed verification follows: $> tar -df mydata.tar mydata/* mydata/damaged_file: Mod time differs mydata/damaged_file: Size differs Note tar files do not store checksums with their data, requiring the original data to be present during verification. Tip Further information on using tar can be found in the tar manual (accessed via man tar or at man tar ).","title":"tar"},{"location":"user-guide/data/#zip","text":"The zip file format is widely used for archiving files and is supported by most major operating systems. The utility to create zip files can be run from the command line as: zip [options] mydata.zip [file(s)] Common options are: -r used to zip up a directory -# where \"#\" represents a digit ranging from 0 to 9 to specify compression level, 0 being the least and 9 the most. Default compression is -6 but we recommend using -0 to speed up the archiving process. Together: zip -0r mydata.zip mydata will create an archive. Note Unlike tar, zip files do not preserve hard links. File data will be copied on archive creation, e.g. an uncompressed zip archive of a 100MB file and a hard link to that file will be approximately 200MB in size. This makes zip an unsuitable format if you wish to precisely reproduce the file system layout. The corresponding unzip command is used to extract data from the archive. The simplest use case is: unzip mydata.zip which recovers the contents of the archive to the current working directory. Files in a zip archive are stored with a CRC checksum to help detect data loss. unzip provides options for verifying this checksum against the stored files. The relevant flag is -t and is used as follows: $> unzip -t mydata.zip Archive: mydata.zip testing: mydata/ OK testing: mydata/file OK No errors detected in compressed data of mydata.zip. Tip Further information on using zip can be found in the zip manual (accessed via man zip or at man zip ).","title":"zip"},{"location":"user-guide/data/#data-transfer-via-ssh","text":"The easiest way of transferring data to/from ARCHER2 is to use one of the standard programs based on the SSH protocol such as scp , sftp or rsync . These all use the same underlying mechanism (SSH) as you normally use to log-in to ARCHER2. So, once the the command has been executed via the command line, you will be prompted for your password for the specified account on the remote machine (ARCHER2 in this case). To avoid having to type in your password multiple times you can set up a SSH key pair and use an SSH agent as documented in the User Guide at connecting .","title":"Data transfer via SSH"},{"location":"user-guide/data/#ssh-data-transfer-performance-considerations","text":"The SSH protocol encrypts all traffic it sends. This means that file transfer using SSH consumes a relatively large amount of CPU time at both ends of the transfer (for encryption and decryption). The ARCHER2 login nodes have fairly fast processors that can sustain about 100 MB/s transfer. The encryption algorithm used is negotiated between the SSH client and the SSH server. There are command line flags that allow you to specify a preference for which encryption algorithm should be used. You may be able to improve transfer speeds by requesting a different algorithm than the default. The aes128-ctr or aes256-ctr algorithms are well supported and fast as they are implemented in hardware. These are not usually the default choice when using scp so you will need to manually specify them. A single SSH based transfer will usually not be able to saturate the available network bandwidth or the available disk bandwidth so you may see an overall improvement by running several data transfer operations in parallel. To reduce metadata interactions it is a good idea to overlap transfers of files from different directories. In addition, you should consider the following when transferring data: Only transfer those files that are required. Consider which data you really need to keep. Combine lots of small files into a single tar archive, to reduce the overheads associated in initiating many separate data transfers (over SSH, each file counts as an individual transfer). Compress data before transferring it, e.g. using gzip .","title":"SSH data transfer performance considerations"},{"location":"user-guide/data/#scp","text":"The scp command creates a copy of a file, or if given the -r flag, a directory either from a local machine onto a remote machine or from a remote machine onto a local machine. For example, to transfer files to ARCHER2 from a local machine: scp [options] source user@login.archer2.ac.uk:[destination] (Remember to replace user with your ARCHER2 username in the example above.) In the above example, the [destination] is optional, as when left out scp will copy the source into your home directory. Also, the source should be the absolute path of the file/directory being copied or the command should be executed in the directory containing the source file/directory. If you want to request a different encryption algorithm add the -c [algorithm-name] flag to the scp options. For example, to use the (usually faster) arcfour encryption algorithm you would use: scp [options] -c aes128-ctr source user@login.archer2.ac.uk:[destination] (Remember to replace user with your ARCHER2 username in the example above.)","title":"scp"},{"location":"user-guide/data/#rsync","text":"The rsync command can also transfer data between hosts using a ssh connection. It creates a copy of a file or, if given the -r flag, a directory at the given destination, similar to scp above. Given the -a option rsync can also make exact copies (including permissions), this is referred to as mirroring . In this case the rsync command is executed with ssh to create the copy on a remote machine. To transfer files to ARCHER2 using rsync with ssh the command has the form: rsync [options] -e ssh source user@login.archer2.ac.uk:[destination] (Remember to replace user with your ARCHER2 username in the example above.) In the above example, the [destination] is optional, as when left out rsync will copy the source into your home directory. Also the source should be the absolute path of the file/directory being copied or the command should be executed in the directory containing the source file/directory. Additional flags can be specified for the underlying ssh command by using a quoted string as the argument of the -e flag. e.g. rsync [options] -e \"ssh -c arcfour\" source user@login.archer2.ac.uk:[destination] (Remember to replace user with your ARCHER2 username in the example above.) Tip Further information on using rsync can be found in the rsync manual (accessed via man rsync or at man rsync ).","title":"rsync"},{"location":"user-guide/data/#ssh-data-transfer-example","text":"Here we have a short example demonstrating transfer of data directly from ARCHER to ARCHER2. The first step will be to set up an SSH key for access to ARCHER2 directly from ARCHER. First log in to ARCHER, and generate a new SSH key. To do this we use the following command: ssh-keygen -b 4096 -C \"otbz01@archer -> otbz19@archer2\" -f ~/.ssh/id_RSA_A2 This generates a new 4096 bit RSA SSH key with the comment otbz01@archer -> otbz19archer2 , and stores the private key in the file ~/.ssh/id_RSA_A2 , and the public key in a corresponding .pub file. During key generation process we are asked to enter a passphrase. SSH keys should always be passphrase protected, but this is especially important when they are kept on a publicly accessible machine such as ARCHER. Please do not leave the passphrase blank when setting up your SSH key. Next we must add the new public key to ARCHER2 through SAFE. We can either open the id_RSA_A2.pub file in our preferred text editor on ARCHER, or use cat id_RSA_A2.pub to output the file contents to screen. Either way we should carefully copy the full SSH key and comment, and then in SAFE go to *Login accounts - \\<user>@archer2 - Add Credential* to add the new SSH key. Once the new key is active you can test that this has worked by attempting to ssh to ARCHER2 from ARCHER. All being well, we are now ready to transfer data directly between the two machines. We begin by combining our important research data in to a single archive file using the following command: tar -czf cat_pictures.tar.gz Cally.jpg Oscar.jpg Marvin.jpg The three important research images. Cally, Oscar, and Marvin (clockwise left to bottom right). We then initiate the data transfer from ARCHER to ARCHER2, here using rsync to allow the transfer to be recommenced without needing to start again, in the event of a loss of connection or other failure. rsync -Pv -e\"ssh -i /home/z01/z01/otbz01/.ssh/id_RSA_A2\" ./cat_pictures.tar.gz otbz19@login1.archer2.ac.uk:/work/z19/z19/otbz19/ Note the use of the -P flag to allow partial transfer -- the same command could be used to restart the transfer after a loss of connection. The -e flag allows specification of the ssh command - we have used this to add the location of the identity file. Unfortunately the ~ shortcut is not correctly expanded, so we have specified the full path. We move our research archive to our project work directory on ARCHER2. If we were unconcerned about being able to restart an interrupted transfer, we could instead use the scp command, scp -i ~/.ssh/id_RSA_A2 cat_pictures.tar.gz otbz19@login1.archer2.ac.uk:/work/z19/z19/otbz19/ but rsync is recommended for larger transfers.","title":"SSH data transfer example"},{"location":"user-guide/debug/","text":"Debugging Warning The ARCHER2 Service is not yet available. This documentation is in development. The following debugging tools are available on ARCHER2: gdb4hpc is a command-line debugging tool provided by HPE Cray. It works similarly to gdb , but allows the user to debug multiple parallel processes without multiple windows. gdb4hpc can be used to investigate deadlocked code, segfaults, and other errors for C/C++ and Fortran code. Users can single-step code and focus on specific processes groups to help identify unexpected code behavior. (text from ALCF ). valgrind4hpc is a parallel memory debugging tool that aids in detection of memory leaks and errors in parallel applications. It aggregates like errors across processes and threads to simply debugging of parallel applications. STAT generate merged stack traces for parallel applications. Also has visualisation tools. ATP provides scalable core file and backtrace analysis when parallel programs crash. CCDB Cray Comparative Debugger. Compare two versions of code side-by-side to analyse differences. (Not currently described in this documentation.) gdb4hpc The GNU Debugger for HPC (gdb4hpc) is a GDB-based debugger used to debug applications compiled with CCE, PGI, GNU, and Intel Fortran, C and C++ compilers. It allows programmers to either launch an application within it or to attach to an already-running application. Attaching to an already-running and hanging application is a quick way of understanding why the application is hanging, whereas launching an application through gdb4hpc will allow you to see your application running step-by-step, output the values of variables, and check whether the application runs as expected. Tip For your executable to be compatible with gdb4hpc, it will need to be coded with MPI. You will also need to compile your code with the debugging flag -g (e.g. cc -g my_program.c -o my_exe ). Launching through gdb4hpc Launch gdb4hpc : module load gdb4hpc gdb4hpc You will get some information about this version of the program and, eventually, you will get a command prompt: gdb4hpc 4.5 - Cray Line Mode Parallel Debugger With Cray Comparative Debugging Technology. Copyright 2007-2019 Cray Inc. All Rights Reserved. Copyright 1996-2016 University of Queensland. All Rights Reserved. Type \"help\" for a list of commands. Type \"help <cmd>\" for detailed help about a command. dbg all> We will use launch to begin a multi-process application within gdb4hpc. Consider that we are wanting to test an application called my_exe , and that we want this to be launched across all 256 processes in two nodes. We would launch this in gdb4hpc by running: dbg all> launch --launcher-args=\"--account=[budget code] --partition=standard --qos=standard --nodes=2 --tasks-per-node=128 --cpus-per-task=1 --exclusive --export=ALL\" $my_prog{256} ./my_ex Make sure to replace the --account input to your budget code ( e.g. if you are using budget t01, that part should look like --account=t01 ). The default launcher is srun and the --launcher-args=\"...\" allows you to set launcher flags for srun . The variable $my_prog is a dummy name for the program being launched and you could use whatever name you want for it -- this will be the name of the srun job that will be run. The number in the brackets {256} is the number of processes over which the program will be executed, it's 256 here, but you could use any number. You should try to run this on as few processors as possible -- the more you use, the longer it will take for gdb4hpc to load the program. Once the program is launched, gdb4hpc will load up the program and begin to run it. You will get output to screen something that looks like: Starting application, please wait... Creating MRNet communication network... Waiting for debug servers to attach to MRNet communications network... Timeout in 400 seconds. Please wait for the attach to complete. Number of dbgsrvs connected: [0]; Timeout Counter: [1] Number of dbgsrvs connected: [0]; Timeout Counter: [2] Number of dbgsrvs connected: [0]; Timeout Counter: [3] Number of dbgsrvs connected: [1]; Timeout Counter: [0] Number of dbgsrvs connected: [1]; Timeout Counter: [1] Number of dbgsrvs connected: [2]; Timeout Counter: [0] Finalizing setup... Launch complete. my_prog{0..255}: Initial breakpoint, main at /PATH/TO/my_program.c:34 The line number at which the initial breakpoint is made (in the above example, line 34) corresponds to the line number at which MPI is initialised. You will not be able to see any parts of the code outside of the MPI region of a code with gdb4hpc. Once the code is loaded, you can use various commands to move through your code. The following lists and describes some of the most useful ones: help -- Lists all gdb4hpc commands. You can run help COMMAND_NAME to learn more about a specific command ( e.g. help launch will tell you about the launch command list -- Will show the current line of code and the 9 lines following. Repeated use of list will move you down the code in ten-line chunks. next -- Will jump to the next step in the program for each process and output which line of code each process is one. It will not enter subroutines. !!! note that there is no reverse-step in gdb4hpc. step -- Like next , but this will step into subroutines. up -- Go up one level in the program ( e.g. from a subroutine back to main). print var -- Prints the value of variable var at this point in the code. watch var -- Like print, but will print whenever a variable changes value. quit -- Exits gdb4hpc. Remember to exit the interactive session once you are done debugging. Attaching with gdb4hpc Attaching to a hanging job using gdb4hpc is a great way of seeing which state each processor is in. However, this does not produce the most visually appealing results. For a more easy-to-read program, please take a look at the STAT tool. In your interactive session, launch your executable as a background task (by adding an & at the end of the command). For example, if you are running an executable called my_exe using 256 processes, you would run: srun -n 256 --nodes=2 --tasks-per-node=128 --cpus-per-task=1 --time=01:00:00 --export=ALL \\ --account=[budget code] --partition=standard --qos=standard ./my_exe & Make sure to replace the --account input to your budget code ( e.g. if you are using budget t01, that part should look like --account=t01 ). You will need to get the full job ID of the job you have just launched. To do this, run: squeue -u $USER and find the job ID associated with this interactive session -- this will be the one with the jobname bash . In this example: JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 1050 workq my_mpi_j jsindt R 0:16 1 nid000001 1051 workq bash jsindt R 0:12 1 nid000002 the appropriate job id is 1051. Next, you will need to run sstat on this job id: sstat 1051 This will output a large amount of information about this specific job. We are looking for the first number of this output, which should look like JOB_ID.## -- the number after the job ID is the number of slurm tasks performed in this interactive session. For our example (where srun is the first slurm task performed), the number is 1051.0. Launch gdb4hpc : module load gdb4hpc gdb4hpc You will get some information about this version of the program and, eventually, you will get a command prompt: gdb4hpc 4.5 - Cray Line Mode Parallel Debugger With Cray Comparative Debugging Technology. Copyright 2007-2019 Cray Inc. All Rights Reserved. Copyright 1996-2016 University of Queensland. All Rights Reserved. Type \"help\" for a list of commands. Type \"help <cmd>\" for detailed help about a command. dbg all> We will be using the attach command to attach to our program that hangs. This is done by writing: dbg all> attach $my_prog JOB_ID.## where JOB_ID.## is the full job ID found using sstat (in our example, this would be 1051.0). The name $my_prog is a dummy-name -- it could be whatever name you like. As it is attaching, gdb4hpc will output text to screen that looks like: Attaching to application, please wait... Creating MRNet communication network... Waiting for debug servers to attach to MRNet communications network... Timeout in 400 seconds. Please wait for the attach to complete. Number of dbgsrvs connected: [0]; Timeout Counter: [1] ... Finalizing setup... Attach complete. Current rank location: After this, you will get an output that, among other things, tells you which line of your code each process is on, and what each process is doing. This can be helpful to see where the hang-up is. If you accidentally attached to the wrong job, you can detach by running: dbg all> release $my_prog and re-attach with the correct job ID. You will need to change your dummy name from $my_prog to something else. When you are finished using gbd4hpc , simply run: dbg all> quit Do not forget to exit your interactive session. valgrind4hpc valgrind4hpc is a Valgrind-based debugging tool to aid in the detection of memory leaks and errors in parallel applications. Valgrind4hpc aggregates any duplicate messages across ranks to help provide an understandable picture of program behavior. Valgrind4hpc manages starting and redirecting output from many copies of Valgrind, as well as recombining and filtering Valgrind messages. If your program can be debugged with Valgrind, it can be debugged with valgrind4hpc. The valgrind4hpc module enables the use of standard valgrind as well as the valgrind4hpc version more suitable to parallel programs. Using Valgrind with serial programs Launch valgrind4hpc : module load valgrind4hpc Next, run your executable through valgrind: valgrind --tool=memcheck --leak-check=yes my_executable The log outputs to screen. The ERROR SUMMARY will tell you whether, and how many, memory errors there are in your program. Furthermore, if you compile your code using the -g debugging flag ( e.g. gcc -g my_program.c -o my_executable.c ), the log will point out the code lines where the error occurs. Valgrind also includes a tool called Massif that can be used to give insight into the memory usage of your program. It takes regular snapshots and outputs this data into a single file, which can be visualised to show the total amount of memory used as a function of time. This shows when peaks and bottlenecks occur and allows you to identify which data structures in your code are responsible for the largest memory usage of your program. Documentation explaining how to use Massif is available at the official Massif manual . In short, you should run your executable as follows: valgrind --tool=massif my_executable The memory profiling data will be output into a file called massif.out.pid , where pid is the runtime process ID of your program. A custom filename can be chosen using the --massif-out-file option , as follows: valgrind --tool=massif --massif-out-file=optional_filename.out my_executable The output file contains raw profiling statistics. To view a summary including a graphical plot of memory usage over time, use the ms_print command as follows: ms_print massif.out.12345 or, to save to a file: ms_print massif.out.12345 > massif.analysis.12345 This will show total memory usage over time as well as a breakdown of the top data structures contributing to memory usage at each snapshot where there has been a significant allocation or deallocation of memory. Using Valgrind4hpc with parallel programs First, load valgrind4hpc : module load valgrind4hpc Valgrind4hpc will launch an srun job to run the executable while it profiles. To test an executable called my_executable that requires two arguments arg1 and arg2 on two nodes and 256 processes, run: valgrind4hpc --tool=memcheck --num-ranks=256 --launcher-args=\"--account=[budget code] --nodes=2 \\ --partition=standard --qos=standard --export=ALL -ntasks-per-node=128 --cpus-per-task=1\" \\ my_executable -- arg1 arg2 In particular, note the -- separating the executable from the arguments (this is not necessary if your executable takes no arguments). The --lancher-args=\"arguments\" allow you to set launcher flags for srun . Valgrind4hpc only supports certain tools found in valgrind. These are: memcheck, helgrind, exp-sgcheck, or drd. The --valgrind-args=\"arguments\" allows users to use valgrind options not supported in valgrind4hpc ( e.g. --leak-check ) -- note, however, that some of these options might interfere with valgrind4hpc. More information on valgrind4hpc can be found in the manual ( man valgrind4hpc ). STAT The Stack Trace Analysis Tool (STAT) is a cross-platform debugging tool from the University of Wisconsin-Madison. ATP is based on the same technology as STAT, both are designed to gather and merge stack traces from a running application's parallel processes. The STAT tool can be useful when application seems to be deadlocked or stuck, i.e. they don't crash but they don't progress as expected, and it has been designed to scale to a very large number of processes. Full information on STAT, including use cases, is available at the STAT website . STAT will attach to a running program and query that program to find out where all the processes in that program currently are. It will then process that data and produce a graph displaying the unique process locations (i.e. where all the processes in the running program currently are). To make this easily understandable it collates together all processes that are in the same place providing only unique program locations for display. Using STAT on ARCHER2 On the login node, load the cray-stat module: module load cray-stat Then, launch your job using srun as a background task (by adding an & at the end of the command). For example, if you are running an executable called my_exe using 256 processes, you would run: srun -n 256 --nodes=2 --tasks-per-node=128 --cpus-per-task=1 --time=01:00:00 --export=ALL\\ --account=[budget code] --partition=standard --qos=standard./my_exe & Note This example has set the job time limit to 1 hour -- if you need longer, change the --time command. You will need the Program ID (PID) of the job you have just launched -- the PID is printed to screen upon launch, or you can get it by running: ps -u $USER This will present you with a set of text that looks like this: PID TTY TIME CMD 154296 ? 00:00:00 systemd 154297 ? 00:00:00 (sd-pam) 154302 ? 00:00:00 sshd 154303 pts/8 00:00:00 bash 157150 pts/8 00:00:00 salloc 157152 pts/8 00:00:00 bash 157183 pts/8 00:00:00 srun 157185 pts/8 00:00:00 srun 157191 pts/8 00:00:00 ps Once your application has reached the point where it hangs, issue the following command (replacing PID with the ID of the first srun task -- in the above example, I would replace PID with 157183): stat-cl -i PID You will get an output that looks like this: STAT started at 2020-07-22-13:31:35 Attaching to job launcher (null):157565 and launching tool daemons... Tool daemons launched and connected! Attaching to application... Attached! Application already paused... ignoring request to pause Sampling traces... Traces sampled! Resuming the application... Resumed! Pausing the application... Paused! ... Detaching from application... Detached! Results written to $PATH_TO_RUN_DIRECTORY/stat_results/my_exe.0000 Once STAT is finished, you can kill the srun job using scancel (replacing JID with the job ID of the job you just launched): scancel JID You can view the results that STAT has produced using the following command (note that \"my_exe\" will need to be replaced with the name of the executable you ran): stat-view stat_results/my_exe.0000/00_my_exe.0000.3D.dot This produces a graph displaying all the different places within the program that the parallel processes were when you queried them. Note To see the graph, you will need to have exported your X display when logging in. ATP To enable ATP you should load the atp module and set the ATP_ENABLED environment variable to 1 on the login node: module load atp export ATP_ENABLED=1 Then, launch your job using srun as a background task (by adding an & at the end of the command). For example, if you are running an executable called my_exe using 256 processes, you would run: srun -n=256 --nodes=2 --tasks-per-node=128 --cpus-per-task=1 --time=01:00:00 --export=ALL \\ --account=[budget code] --partition=standard --qos=standard ./my_exe & Note This example has set the job time limit to 1 hour -- if you need longer, change the --time command. Once the job has finished running, load the stat module to view the results: module load cray-stat and view the merged stack trace using: stat-view atpMergedBT.dot Note To see the graph, you will need to have exported your X display when logging in.","title":"Debugging"},{"location":"user-guide/debug/#debugging","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development. The following debugging tools are available on ARCHER2: gdb4hpc is a command-line debugging tool provided by HPE Cray. It works similarly to gdb , but allows the user to debug multiple parallel processes without multiple windows. gdb4hpc can be used to investigate deadlocked code, segfaults, and other errors for C/C++ and Fortran code. Users can single-step code and focus on specific processes groups to help identify unexpected code behavior. (text from ALCF ). valgrind4hpc is a parallel memory debugging tool that aids in detection of memory leaks and errors in parallel applications. It aggregates like errors across processes and threads to simply debugging of parallel applications. STAT generate merged stack traces for parallel applications. Also has visualisation tools. ATP provides scalable core file and backtrace analysis when parallel programs crash. CCDB Cray Comparative Debugger. Compare two versions of code side-by-side to analyse differences. (Not currently described in this documentation.)","title":"Debugging"},{"location":"user-guide/debug/#gdb4hpc","text":"The GNU Debugger for HPC (gdb4hpc) is a GDB-based debugger used to debug applications compiled with CCE, PGI, GNU, and Intel Fortran, C and C++ compilers. It allows programmers to either launch an application within it or to attach to an already-running application. Attaching to an already-running and hanging application is a quick way of understanding why the application is hanging, whereas launching an application through gdb4hpc will allow you to see your application running step-by-step, output the values of variables, and check whether the application runs as expected. Tip For your executable to be compatible with gdb4hpc, it will need to be coded with MPI. You will also need to compile your code with the debugging flag -g (e.g. cc -g my_program.c -o my_exe ).","title":"gdb4hpc"},{"location":"user-guide/debug/#launching-through-gdb4hpc","text":"Launch gdb4hpc : module load gdb4hpc gdb4hpc You will get some information about this version of the program and, eventually, you will get a command prompt: gdb4hpc 4.5 - Cray Line Mode Parallel Debugger With Cray Comparative Debugging Technology. Copyright 2007-2019 Cray Inc. All Rights Reserved. Copyright 1996-2016 University of Queensland. All Rights Reserved. Type \"help\" for a list of commands. Type \"help <cmd>\" for detailed help about a command. dbg all> We will use launch to begin a multi-process application within gdb4hpc. Consider that we are wanting to test an application called my_exe , and that we want this to be launched across all 256 processes in two nodes. We would launch this in gdb4hpc by running: dbg all> launch --launcher-args=\"--account=[budget code] --partition=standard --qos=standard --nodes=2 --tasks-per-node=128 --cpus-per-task=1 --exclusive --export=ALL\" $my_prog{256} ./my_ex Make sure to replace the --account input to your budget code ( e.g. if you are using budget t01, that part should look like --account=t01 ). The default launcher is srun and the --launcher-args=\"...\" allows you to set launcher flags for srun . The variable $my_prog is a dummy name for the program being launched and you could use whatever name you want for it -- this will be the name of the srun job that will be run. The number in the brackets {256} is the number of processes over which the program will be executed, it's 256 here, but you could use any number. You should try to run this on as few processors as possible -- the more you use, the longer it will take for gdb4hpc to load the program. Once the program is launched, gdb4hpc will load up the program and begin to run it. You will get output to screen something that looks like: Starting application, please wait... Creating MRNet communication network... Waiting for debug servers to attach to MRNet communications network... Timeout in 400 seconds. Please wait for the attach to complete. Number of dbgsrvs connected: [0]; Timeout Counter: [1] Number of dbgsrvs connected: [0]; Timeout Counter: [2] Number of dbgsrvs connected: [0]; Timeout Counter: [3] Number of dbgsrvs connected: [1]; Timeout Counter: [0] Number of dbgsrvs connected: [1]; Timeout Counter: [1] Number of dbgsrvs connected: [2]; Timeout Counter: [0] Finalizing setup... Launch complete. my_prog{0..255}: Initial breakpoint, main at /PATH/TO/my_program.c:34 The line number at which the initial breakpoint is made (in the above example, line 34) corresponds to the line number at which MPI is initialised. You will not be able to see any parts of the code outside of the MPI region of a code with gdb4hpc. Once the code is loaded, you can use various commands to move through your code. The following lists and describes some of the most useful ones: help -- Lists all gdb4hpc commands. You can run help COMMAND_NAME to learn more about a specific command ( e.g. help launch will tell you about the launch command list -- Will show the current line of code and the 9 lines following. Repeated use of list will move you down the code in ten-line chunks. next -- Will jump to the next step in the program for each process and output which line of code each process is one. It will not enter subroutines. !!! note that there is no reverse-step in gdb4hpc. step -- Like next , but this will step into subroutines. up -- Go up one level in the program ( e.g. from a subroutine back to main). print var -- Prints the value of variable var at this point in the code. watch var -- Like print, but will print whenever a variable changes value. quit -- Exits gdb4hpc. Remember to exit the interactive session once you are done debugging.","title":"Launching through gdb4hpc"},{"location":"user-guide/debug/#attaching-with-gdb4hpc","text":"Attaching to a hanging job using gdb4hpc is a great way of seeing which state each processor is in. However, this does not produce the most visually appealing results. For a more easy-to-read program, please take a look at the STAT tool. In your interactive session, launch your executable as a background task (by adding an & at the end of the command). For example, if you are running an executable called my_exe using 256 processes, you would run: srun -n 256 --nodes=2 --tasks-per-node=128 --cpus-per-task=1 --time=01:00:00 --export=ALL \\ --account=[budget code] --partition=standard --qos=standard ./my_exe & Make sure to replace the --account input to your budget code ( e.g. if you are using budget t01, that part should look like --account=t01 ). You will need to get the full job ID of the job you have just launched. To do this, run: squeue -u $USER and find the job ID associated with this interactive session -- this will be the one with the jobname bash . In this example: JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 1050 workq my_mpi_j jsindt R 0:16 1 nid000001 1051 workq bash jsindt R 0:12 1 nid000002 the appropriate job id is 1051. Next, you will need to run sstat on this job id: sstat 1051 This will output a large amount of information about this specific job. We are looking for the first number of this output, which should look like JOB_ID.## -- the number after the job ID is the number of slurm tasks performed in this interactive session. For our example (where srun is the first slurm task performed), the number is 1051.0. Launch gdb4hpc : module load gdb4hpc gdb4hpc You will get some information about this version of the program and, eventually, you will get a command prompt: gdb4hpc 4.5 - Cray Line Mode Parallel Debugger With Cray Comparative Debugging Technology. Copyright 2007-2019 Cray Inc. All Rights Reserved. Copyright 1996-2016 University of Queensland. All Rights Reserved. Type \"help\" for a list of commands. Type \"help <cmd>\" for detailed help about a command. dbg all> We will be using the attach command to attach to our program that hangs. This is done by writing: dbg all> attach $my_prog JOB_ID.## where JOB_ID.## is the full job ID found using sstat (in our example, this would be 1051.0). The name $my_prog is a dummy-name -- it could be whatever name you like. As it is attaching, gdb4hpc will output text to screen that looks like: Attaching to application, please wait... Creating MRNet communication network... Waiting for debug servers to attach to MRNet communications network... Timeout in 400 seconds. Please wait for the attach to complete. Number of dbgsrvs connected: [0]; Timeout Counter: [1] ... Finalizing setup... Attach complete. Current rank location: After this, you will get an output that, among other things, tells you which line of your code each process is on, and what each process is doing. This can be helpful to see where the hang-up is. If you accidentally attached to the wrong job, you can detach by running: dbg all> release $my_prog and re-attach with the correct job ID. You will need to change your dummy name from $my_prog to something else. When you are finished using gbd4hpc , simply run: dbg all> quit Do not forget to exit your interactive session.","title":"Attaching with gdb4hpc"},{"location":"user-guide/debug/#valgrind4hpc","text":"valgrind4hpc is a Valgrind-based debugging tool to aid in the detection of memory leaks and errors in parallel applications. Valgrind4hpc aggregates any duplicate messages across ranks to help provide an understandable picture of program behavior. Valgrind4hpc manages starting and redirecting output from many copies of Valgrind, as well as recombining and filtering Valgrind messages. If your program can be debugged with Valgrind, it can be debugged with valgrind4hpc. The valgrind4hpc module enables the use of standard valgrind as well as the valgrind4hpc version more suitable to parallel programs.","title":"valgrind4hpc"},{"location":"user-guide/debug/#using-valgrind-with-serial-programs","text":"Launch valgrind4hpc : module load valgrind4hpc Next, run your executable through valgrind: valgrind --tool=memcheck --leak-check=yes my_executable The log outputs to screen. The ERROR SUMMARY will tell you whether, and how many, memory errors there are in your program. Furthermore, if you compile your code using the -g debugging flag ( e.g. gcc -g my_program.c -o my_executable.c ), the log will point out the code lines where the error occurs. Valgrind also includes a tool called Massif that can be used to give insight into the memory usage of your program. It takes regular snapshots and outputs this data into a single file, which can be visualised to show the total amount of memory used as a function of time. This shows when peaks and bottlenecks occur and allows you to identify which data structures in your code are responsible for the largest memory usage of your program. Documentation explaining how to use Massif is available at the official Massif manual . In short, you should run your executable as follows: valgrind --tool=massif my_executable The memory profiling data will be output into a file called massif.out.pid , where pid is the runtime process ID of your program. A custom filename can be chosen using the --massif-out-file option , as follows: valgrind --tool=massif --massif-out-file=optional_filename.out my_executable The output file contains raw profiling statistics. To view a summary including a graphical plot of memory usage over time, use the ms_print command as follows: ms_print massif.out.12345 or, to save to a file: ms_print massif.out.12345 > massif.analysis.12345 This will show total memory usage over time as well as a breakdown of the top data structures contributing to memory usage at each snapshot where there has been a significant allocation or deallocation of memory.","title":"Using Valgrind with serial programs"},{"location":"user-guide/debug/#using-valgrind4hpc-with-parallel-programs","text":"First, load valgrind4hpc : module load valgrind4hpc Valgrind4hpc will launch an srun job to run the executable while it profiles. To test an executable called my_executable that requires two arguments arg1 and arg2 on two nodes and 256 processes, run: valgrind4hpc --tool=memcheck --num-ranks=256 --launcher-args=\"--account=[budget code] --nodes=2 \\ --partition=standard --qos=standard --export=ALL -ntasks-per-node=128 --cpus-per-task=1\" \\ my_executable -- arg1 arg2 In particular, note the -- separating the executable from the arguments (this is not necessary if your executable takes no arguments). The --lancher-args=\"arguments\" allow you to set launcher flags for srun . Valgrind4hpc only supports certain tools found in valgrind. These are: memcheck, helgrind, exp-sgcheck, or drd. The --valgrind-args=\"arguments\" allows users to use valgrind options not supported in valgrind4hpc ( e.g. --leak-check ) -- note, however, that some of these options might interfere with valgrind4hpc. More information on valgrind4hpc can be found in the manual ( man valgrind4hpc ).","title":"Using Valgrind4hpc with parallel programs"},{"location":"user-guide/debug/#stat","text":"The Stack Trace Analysis Tool (STAT) is a cross-platform debugging tool from the University of Wisconsin-Madison. ATP is based on the same technology as STAT, both are designed to gather and merge stack traces from a running application's parallel processes. The STAT tool can be useful when application seems to be deadlocked or stuck, i.e. they don't crash but they don't progress as expected, and it has been designed to scale to a very large number of processes. Full information on STAT, including use cases, is available at the STAT website . STAT will attach to a running program and query that program to find out where all the processes in that program currently are. It will then process that data and produce a graph displaying the unique process locations (i.e. where all the processes in the running program currently are). To make this easily understandable it collates together all processes that are in the same place providing only unique program locations for display.","title":"STAT"},{"location":"user-guide/debug/#using-stat-on-archer2","text":"On the login node, load the cray-stat module: module load cray-stat Then, launch your job using srun as a background task (by adding an & at the end of the command). For example, if you are running an executable called my_exe using 256 processes, you would run: srun -n 256 --nodes=2 --tasks-per-node=128 --cpus-per-task=1 --time=01:00:00 --export=ALL\\ --account=[budget code] --partition=standard --qos=standard./my_exe & Note This example has set the job time limit to 1 hour -- if you need longer, change the --time command. You will need the Program ID (PID) of the job you have just launched -- the PID is printed to screen upon launch, or you can get it by running: ps -u $USER This will present you with a set of text that looks like this: PID TTY TIME CMD 154296 ? 00:00:00 systemd 154297 ? 00:00:00 (sd-pam) 154302 ? 00:00:00 sshd 154303 pts/8 00:00:00 bash 157150 pts/8 00:00:00 salloc 157152 pts/8 00:00:00 bash 157183 pts/8 00:00:00 srun 157185 pts/8 00:00:00 srun 157191 pts/8 00:00:00 ps Once your application has reached the point where it hangs, issue the following command (replacing PID with the ID of the first srun task -- in the above example, I would replace PID with 157183): stat-cl -i PID You will get an output that looks like this: STAT started at 2020-07-22-13:31:35 Attaching to job launcher (null):157565 and launching tool daemons... Tool daemons launched and connected! Attaching to application... Attached! Application already paused... ignoring request to pause Sampling traces... Traces sampled! Resuming the application... Resumed! Pausing the application... Paused! ... Detaching from application... Detached! Results written to $PATH_TO_RUN_DIRECTORY/stat_results/my_exe.0000 Once STAT is finished, you can kill the srun job using scancel (replacing JID with the job ID of the job you just launched): scancel JID You can view the results that STAT has produced using the following command (note that \"my_exe\" will need to be replaced with the name of the executable you ran): stat-view stat_results/my_exe.0000/00_my_exe.0000.3D.dot This produces a graph displaying all the different places within the program that the parallel processes were when you queried them. Note To see the graph, you will need to have exported your X display when logging in.","title":"Using STAT on ARCHER2"},{"location":"user-guide/debug/#atp","text":"To enable ATP you should load the atp module and set the ATP_ENABLED environment variable to 1 on the login node: module load atp export ATP_ENABLED=1 Then, launch your job using srun as a background task (by adding an & at the end of the command). For example, if you are running an executable called my_exe using 256 processes, you would run: srun -n=256 --nodes=2 --tasks-per-node=128 --cpus-per-task=1 --time=01:00:00 --export=ALL \\ --account=[budget code] --partition=standard --qos=standard ./my_exe & Note This example has set the job time limit to 1 hour -- if you need longer, change the --time command. Once the job has finished running, load the stat module to view the results: module load cray-stat and view the merged stack trace using: stat-view atpMergedBT.dot Note To see the graph, you will need to have exported your X display when logging in.","title":"ATP"},{"location":"user-guide/dev-environment/","text":"Application development environment Warning The ARCHER2 Service is not yet available. This documentation is in development. What's available ARCHER2 runs on the Cray Linux Environment (a version of SUSE Linux), and provides a development environment which includes: Software modules via a standard module framework Three different compiler environments (AMD, Cray, and GNU) MPI, OpenMP, and SHMEM Scientific and numerical libraries Parallel Python and R Parallel debugging and profiling Singularity containers Access to particular software, and particular versions, is managed by a standard TCL module framework. Most software is available via standard software modules and the different programming environments are available via module collections. You can see what programming environments are available with: auser@uan01:~> module savelist Named collection list: 1) PrgEnv-aocc 2) PrgEnv-cray 3) PrgEnv-gnu Other software modules can be listed with auser@uan01:~> module avail ------------------------------- /opt/cray/pe/perftools/20.09.0/modulefiles -------------------------------- perftools perftools-lite-events perftools-lite-hbm perftools-nwpc perftools-lite perftools-lite-gpu perftools-lite-loops perftools-preload ---------------------------------- /opt/cray/pe/craype/2.7.0/modulefiles ---------------------------------- craype-hugepages1G craype-hugepages8M craype-hugepages128M craype-network-ofi craype-hugepages2G craype-hugepages16M craype-hugepages256M craype-network-slingshot10 craype-hugepages2M craype-hugepages32M craype-hugepages512M craype-x86-rome craype-hugepages4M craype-hugepages64M craype-network-none ------------------------------------- /usr/local/Modules/modulefiles -------------------------------------- dot module-git module-info modules null use.own -------------------------------------- /opt/cray/pe/cpe-prgenv/7.0.0 -------------------------------------- cpe-aocc cpe-cray cpe-gnu -------------------------------------------- /opt/modulefiles --------------------------------------------- aocc/2.1.0.3(default) cray-R/4.0.2.0(default) gcc/8.1.0 gcc/9.3.0 gcc/10.1.0(default) ---------------------------------------- /opt/cray/pe/modulefiles ----------------------------------------- atp/3.7.4(default) cray-mpich-abi/8.0.15 craype-dl-plugin-py3/20.06.1(default) cce/10.0.3(default) cray-mpich-ucx/8.0.15 craype/2.7.0(default) cray-ccdb/4.7.1(default) cray-mpich/8.0.15(default) craypkg-gen/1.3.10(default) cray-cti/2.7.3(default) cray-netcdf-hdf5parallel/4.7.4.0 gdb4hpc/4.7.3(default) cray-dsmml/0.1.2(default) cray-netcdf/4.7.4.0 iobuf/2.0.10(default) cray-fftw/3.3.8.7(default) cray-openshmemx/11.1.1(default) papi/6.0.0.2(default) cray-ga/5.7.0.3 cray-parallel-netcdf/1.12.1.0 perftools-base/20.09.0(default) cray-hdf5-parallel/1.12.0.0 cray-pmi-lib/6.0.6(default) valgrind4hpc/2.7.2(default) cray-hdf5/1.12.0.0 cray-pmi/6.0.6(default) cray-libsci/20.08.1.2(default) cray-python/3.8.5.0(default) A full discussion of the module system is available in the Software environment section . A consistent set of modules is loaded on login to the machine (currently PrgEnv-cray , see below). Developing applications then means selecting and loading the appropriate set of modules before starting work. This section is aimed at code developers and will concentrate on the compilation environment and building libraries and executables, and specifically parallel executables. Other topics such as Python and Containers are covered in more detail in separate sections of the documentation. Managing development ARCHER2 supports common revision control software such as git . Standard GNU autoconf tools are available, along with make (which is GNU Make). Versions of cmake are available. Note Some of these tools are part of the system software, and typically reside in /usr/bin , while others are provided as part of the module system. Some tools may be available in different versions via both /usr/bin and via the module system. Compilation environment There are three different compiler environments available on ARCHER2: AMD (AOCC), Cray (CCE), and GNU (GCC). The current compiler suite is selected via the programming environment, while the specific compiler versions are determined by the relevant compiler module. A summary is: Suite name Module Programming environment collection CCE cce PrgEnv-cray GCC gcc PrgEnv-gnu AOCC aocc PrgEnv-aocc For example, at login, the default set of modules are: Currently Loaded Modulefiles: 1) cpe-cray 7) cray-dsmml/0.1.2(default) 2) cce/10.0.3(default) 8) perftools-base/20.09.0(default) 3) craype/2.7.0(default) 9) xpmem/2.2.35-7.0.1.0_1.3__gd50fabf.shasta(default) 4) craype-x86-rome 10) cray-mpich/8.0.15(default) 5) libfabric/1.11.0.0.233(default) 11) cray-libsci/20.08.1.2(default) 6) craype-network-ofi from which we see the default programming environment is Cray (indicated by cpe-cray (at 1 in the list above) and the default compiler module is cce/10.0.3 (at 2 in the list above). The programming environment will give access to a consistent set of compiler, MPI library via cray-mpich (at 10), and other libraries e.g., cray-libsci (at 11 in the list above) infrastructure. Within a given programming environment, it is possible to swap to a different compiler version by swapping the relevant compiler module. To ensure consistent behaviour, compilation of C, C++, and Fortran source code should then take place using the appropriate compiler wrapper: cc , CC , and ftn , respectively. The wrapper will automatically call the relevant underlying compiler and add the appropriate include directories and library locations to the invocation. This typically eliminates the need to specify this additional information explicitly in the configuration stage. To see the details of the exact compiler invocation use the -craype-verbose flag to the compiler wrapper. The default link time behaviour is also related to the current programming environment. See the section below on Linking and libraries . Users should not, in general, invoke specific compilers at compile/link stages. In particular, gcc , which may default to /usr/bin/gcc , should not be used. The compiler wrappers cc , CC , and ftn should be used via the appropriate module. Other common MPI compiler wrappers e.g., mpicc should also be replaced by the relevant wrapper cc ( mpicc etc are not available). Important Always use the compiler wrappers cc , CC , and/or ftn and not a specific compiler invocation. This will ensure consistent compile/link time behaviour. Compiler man pages and help Further information on both the compiler wrappers, and the individual compilers themselves are available via the command line, and via standard man pages. The man page for the compiler wrappers is common to all programming environments, while the man page for individual compilers depends on the currently loaded programming environment. The following table summarises options for obtaining information on the compiler and compile options: Compiler suite C C++ Fortran Cray man craycc man crayCC man crayftn GNU man gcc man g++ man gfortran Wrappers man cc man CC man ftn Tip You can also pass the --help option to any of the compilers or wrappers to get a summary of how to use them. The Cray Fortran compiler uses ftn --craype-help to access the help options. Tip There are no man pages for the AOCC compilers at the moment. Tip Cray C/C++ is based on Clang and therefore supports similar options to clang/gcc ( man clang is in fact equivalent to man craycc ). clang --help will produce a full summary of options with Cray-specific options marked \"Cray\". The craycc man page concentrates on these Cray extensions to the clang front end and does not provide an exhaustive description of all clang options. Cray Fortran is not based on Flang and so takes different options from flang/gfortran. Dynamic Linking Executables on ARCHER2 link dynamically, and the Cray Programming Environment does not currently support static linking. This is in contrast to ARCHER where the default was to build statically. If you attempt to link statically, you will see errors similar to: /usr/bin/ld: cannot find -lpmi /usr/bin/ld: cannot find -lpmi2 collect2: error: ld returned 1 exit status The compiler wrapper scripts on ARCHER link runtime libraries in using the runpath by default. This means that the paths to the runtime libraries are encoded into the executable so you do not need to load the compiler environment in your job submission scripts. Which compiler environment? If you are unsure which compiler you should choose, we suggest the starting point should be the GNU compiler collection (GCC, PrgEnv-gnu ); this is perhaps the most commonly used by code developers, particularly in the open source software domain. A portable, standard-conforming code should (in principle) compile in any of the three programming environments. For users requiring specific compiler features, such as co-array Fortran, the recommended starting point would be Cray. The following sections provide further details of the different programming environments. Warning Intel compilers are not available on ARCHER2. AMD Optimizing C/C++ Compiler (AOCC) The AMD Optimizing C/++ Compiler (AOCC) is a clang-based optimising compiler. AOCC (despite its name) includes a flang-based Fortran compiler. Switch the the AOCC programming environment via $ module restore PrgEnv-aocc Note Further details on AOCC will appear here as they become available. AOCC reference material AMD website https://developer.amd.com/amd-aocc/ Cray compiler environment (CCE) The Cray compiler environment (CCE) is the default compiler at the point of login. CCE supports C/C++ (along with unified parallel C UPC), and Fortran (including co-array Fortran). Support for OpenMP parallelism is available for both C/C++ and Fortran (currently OpenMP 4.5, with a number of exceptions). The Cray C/C++ compiler is based on a clang front end, and so compiler options are similar to those for gcc/clang. However, the Fortran compiler remains based around Cray-specific options. Be sure to separate C/C++ compiler options and Fortran compiler options (typically CFLAGS and FFLAGS ) if compiling mixed C/Fortran applications. Switch the the Cray programming environment via $ module restore PrgEnv-cray Useful CCE C/C++ options When using the compiler wrappers cc or CC , some of the following options may be useful: Language, warning, Debugging options: Option Comment -std=<standard> Default is -std=gnu11 ( gnu++14 for C++) [1] Performance options: Option Comment -Ofast Optimisation levels: -O0, -O1, -O2, -O3, -Ofast -ffp=level Floating point maths optimisations levels 0-4 [2] -flto Link time optimisation Miscellaneous options: Option Comment -fopenmp Compile OpenMP (default is off) -v Display verbose output from compiler stages Notes Option -std=gnu11 gives c11 plus GNU extensions (likewise c++14 plus GNU extensions). See https://gcc.gnu.org/onlinedocs/gcc-4.8.2/gcc/C-Extensions.html Option -ffp=3 is implied by -Ofast or -ffast-math Useful CCE Fortran options Language, Warning, Debugging options: Option Comment -m <level> Message level (default -m 3 errors and warnings) Performance options: Option Comment -O <level> Optimisation levels: -O0 to -O3 (default -O2) -h fp<level> Floating point maths optimisations levels 0-3 -h ipa Inter-procedural analysis Miscellaneous options: Option Comment -h omp Compile OpenMP (default is -hnoomp ) -v Display verbose output from compiler stages GNU compiler collection (GCC) The commonly used open source GNU compiler collection is available and provides C/C++ and Fortran compilers. The GNU compiler collection is loaded by switching to the GNU programming environment: $ module restore PrgEnv-gnu Bug The gcc/8.1.0 module is available on ARCHER2 but cannot be used as the supporting scientific and system libraries are not available. You should not use this version of GCC. Warning If you want to use GCC version 10 or greater to compile Fortran code, you must add the -fallow-argument-mismatch option when compiling otherwise you will see compile errors associated with MPI functions. Useful Gnu Fortran options Option Comment -std=<standard> Default is gnu -fallow-argument-mismatch Allow mismatched procedure arguments. This argument is required for compiling MPI Fortran code with GCC version 10 or greater -fbounds-check Use runtime checking of array indices -fopenmp Compile OpenMP (default is no OpenMP) -v Display verbose output from compiler stages Tip The standard in -std may be one of f95 f2003 , f2008 or f2018 . The default option -std=gnu is the latest Fortran standard plus gnu extensions. Warning Past versions of gfortran have allowed mismatched arguments to external procedures (e.g., where an explicit interface is not available). This is often the case for MPI routines where arrays of different types are passed to MPI_Send() and so on. This will now generate an error as not standard conforming. Use -fallow-argument-mismatch to reduce the error to a warning. The same effect may be achieved via -std=legacy . Reference material C/C++ documentation https://gcc.gnu.org/onlinedocs/gcc-9.3.0/gcc/ Fortran documentation https://gcc.gnu.org/onlinedocs/gcc-9.3.0/gfortran/ Message passing interface (MPI) HPE Cray MPICH HPE Cray provide, as standard, an MPICH implementation of the message passing interface which is specifically optimised for the ARCHER2 network. The current implementation supports MPI standard version 3.1. The HPE Cray MPICH implementation is linked into software by default when compiling using the standard wrapper scripts: cc , CC and ftn . MPI reference material MPI standard documents: https://www.mpi-forum.org/docs/ Linking and libraries Linking to libraries is performed dynamically on ARCHER2. One can use the -craype-verbose flag to the compiler wrapper to check exactly what linker arguments are invoked. The compiler wrapper scripts encode the paths to the programming environment system libraries using RUNPATH. This ensures that the executable can find the correct runtime libraries without the matching software modules loaded. The library RUNPATH associated with an executable can be inspected via, e.g., $ readelf -d ./a.out (swap a.out for the name of the executable you are querying). Commonly used libraries Modules with names prefixed by cray- are provided by HPE Cray, and are supported to be consistent with any of the programming environments and associated compilers. These modules should be the first choice for access to software libraries if available. Tip More information on the different software libraries on ARCHER2 can be found in the Software libraries section of the user guide. Build instructions for software on ARCHER2 The ARCHER2 CSE team at EPCC and other contributors provide build configurations ando instructions for a range of research software, software libraries and tools on a variety of HPC systems (including ARCHER2) in a public Github repository. See: Build instructions repository The repository always welcomes contributions from the ARCHER2 user community. Support for building software on ARCHER2 If you run into issues building software on ARCHER2 or the software you require is not available then please contact the ARCHER2 Service Desk with any questions you have.","title":"Application development environment"},{"location":"user-guide/dev-environment/#application-development-environment","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development.","title":"Application development environment"},{"location":"user-guide/dev-environment/#whats-available","text":"ARCHER2 runs on the Cray Linux Environment (a version of SUSE Linux), and provides a development environment which includes: Software modules via a standard module framework Three different compiler environments (AMD, Cray, and GNU) MPI, OpenMP, and SHMEM Scientific and numerical libraries Parallel Python and R Parallel debugging and profiling Singularity containers Access to particular software, and particular versions, is managed by a standard TCL module framework. Most software is available via standard software modules and the different programming environments are available via module collections. You can see what programming environments are available with: auser@uan01:~> module savelist Named collection list: 1) PrgEnv-aocc 2) PrgEnv-cray 3) PrgEnv-gnu Other software modules can be listed with auser@uan01:~> module avail ------------------------------- /opt/cray/pe/perftools/20.09.0/modulefiles -------------------------------- perftools perftools-lite-events perftools-lite-hbm perftools-nwpc perftools-lite perftools-lite-gpu perftools-lite-loops perftools-preload ---------------------------------- /opt/cray/pe/craype/2.7.0/modulefiles ---------------------------------- craype-hugepages1G craype-hugepages8M craype-hugepages128M craype-network-ofi craype-hugepages2G craype-hugepages16M craype-hugepages256M craype-network-slingshot10 craype-hugepages2M craype-hugepages32M craype-hugepages512M craype-x86-rome craype-hugepages4M craype-hugepages64M craype-network-none ------------------------------------- /usr/local/Modules/modulefiles -------------------------------------- dot module-git module-info modules null use.own -------------------------------------- /opt/cray/pe/cpe-prgenv/7.0.0 -------------------------------------- cpe-aocc cpe-cray cpe-gnu -------------------------------------------- /opt/modulefiles --------------------------------------------- aocc/2.1.0.3(default) cray-R/4.0.2.0(default) gcc/8.1.0 gcc/9.3.0 gcc/10.1.0(default) ---------------------------------------- /opt/cray/pe/modulefiles ----------------------------------------- atp/3.7.4(default) cray-mpich-abi/8.0.15 craype-dl-plugin-py3/20.06.1(default) cce/10.0.3(default) cray-mpich-ucx/8.0.15 craype/2.7.0(default) cray-ccdb/4.7.1(default) cray-mpich/8.0.15(default) craypkg-gen/1.3.10(default) cray-cti/2.7.3(default) cray-netcdf-hdf5parallel/4.7.4.0 gdb4hpc/4.7.3(default) cray-dsmml/0.1.2(default) cray-netcdf/4.7.4.0 iobuf/2.0.10(default) cray-fftw/3.3.8.7(default) cray-openshmemx/11.1.1(default) papi/6.0.0.2(default) cray-ga/5.7.0.3 cray-parallel-netcdf/1.12.1.0 perftools-base/20.09.0(default) cray-hdf5-parallel/1.12.0.0 cray-pmi-lib/6.0.6(default) valgrind4hpc/2.7.2(default) cray-hdf5/1.12.0.0 cray-pmi/6.0.6(default) cray-libsci/20.08.1.2(default) cray-python/3.8.5.0(default) A full discussion of the module system is available in the Software environment section . A consistent set of modules is loaded on login to the machine (currently PrgEnv-cray , see below). Developing applications then means selecting and loading the appropriate set of modules before starting work. This section is aimed at code developers and will concentrate on the compilation environment and building libraries and executables, and specifically parallel executables. Other topics such as Python and Containers are covered in more detail in separate sections of the documentation.","title":"What's available"},{"location":"user-guide/dev-environment/#managing-development","text":"ARCHER2 supports common revision control software such as git . Standard GNU autoconf tools are available, along with make (which is GNU Make). Versions of cmake are available. Note Some of these tools are part of the system software, and typically reside in /usr/bin , while others are provided as part of the module system. Some tools may be available in different versions via both /usr/bin and via the module system.","title":"Managing development"},{"location":"user-guide/dev-environment/#compilation-environment","text":"There are three different compiler environments available on ARCHER2: AMD (AOCC), Cray (CCE), and GNU (GCC). The current compiler suite is selected via the programming environment, while the specific compiler versions are determined by the relevant compiler module. A summary is: Suite name Module Programming environment collection CCE cce PrgEnv-cray GCC gcc PrgEnv-gnu AOCC aocc PrgEnv-aocc For example, at login, the default set of modules are: Currently Loaded Modulefiles: 1) cpe-cray 7) cray-dsmml/0.1.2(default) 2) cce/10.0.3(default) 8) perftools-base/20.09.0(default) 3) craype/2.7.0(default) 9) xpmem/2.2.35-7.0.1.0_1.3__gd50fabf.shasta(default) 4) craype-x86-rome 10) cray-mpich/8.0.15(default) 5) libfabric/1.11.0.0.233(default) 11) cray-libsci/20.08.1.2(default) 6) craype-network-ofi from which we see the default programming environment is Cray (indicated by cpe-cray (at 1 in the list above) and the default compiler module is cce/10.0.3 (at 2 in the list above). The programming environment will give access to a consistent set of compiler, MPI library via cray-mpich (at 10), and other libraries e.g., cray-libsci (at 11 in the list above) infrastructure. Within a given programming environment, it is possible to swap to a different compiler version by swapping the relevant compiler module. To ensure consistent behaviour, compilation of C, C++, and Fortran source code should then take place using the appropriate compiler wrapper: cc , CC , and ftn , respectively. The wrapper will automatically call the relevant underlying compiler and add the appropriate include directories and library locations to the invocation. This typically eliminates the need to specify this additional information explicitly in the configuration stage. To see the details of the exact compiler invocation use the -craype-verbose flag to the compiler wrapper. The default link time behaviour is also related to the current programming environment. See the section below on Linking and libraries . Users should not, in general, invoke specific compilers at compile/link stages. In particular, gcc , which may default to /usr/bin/gcc , should not be used. The compiler wrappers cc , CC , and ftn should be used via the appropriate module. Other common MPI compiler wrappers e.g., mpicc should also be replaced by the relevant wrapper cc ( mpicc etc are not available). Important Always use the compiler wrappers cc , CC , and/or ftn and not a specific compiler invocation. This will ensure consistent compile/link time behaviour.","title":"Compilation environment"},{"location":"user-guide/dev-environment/#compiler-man-pages-and-help","text":"Further information on both the compiler wrappers, and the individual compilers themselves are available via the command line, and via standard man pages. The man page for the compiler wrappers is common to all programming environments, while the man page for individual compilers depends on the currently loaded programming environment. The following table summarises options for obtaining information on the compiler and compile options: Compiler suite C C++ Fortran Cray man craycc man crayCC man crayftn GNU man gcc man g++ man gfortran Wrappers man cc man CC man ftn Tip You can also pass the --help option to any of the compilers or wrappers to get a summary of how to use them. The Cray Fortran compiler uses ftn --craype-help to access the help options. Tip There are no man pages for the AOCC compilers at the moment. Tip Cray C/C++ is based on Clang and therefore supports similar options to clang/gcc ( man clang is in fact equivalent to man craycc ). clang --help will produce a full summary of options with Cray-specific options marked \"Cray\". The craycc man page concentrates on these Cray extensions to the clang front end and does not provide an exhaustive description of all clang options. Cray Fortran is not based on Flang and so takes different options from flang/gfortran.","title":"Compiler man pages and help"},{"location":"user-guide/dev-environment/#dynamic-linking","text":"Executables on ARCHER2 link dynamically, and the Cray Programming Environment does not currently support static linking. This is in contrast to ARCHER where the default was to build statically. If you attempt to link statically, you will see errors similar to: /usr/bin/ld: cannot find -lpmi /usr/bin/ld: cannot find -lpmi2 collect2: error: ld returned 1 exit status The compiler wrapper scripts on ARCHER link runtime libraries in using the runpath by default. This means that the paths to the runtime libraries are encoded into the executable so you do not need to load the compiler environment in your job submission scripts.","title":"Dynamic Linking"},{"location":"user-guide/dev-environment/#which-compiler-environment","text":"If you are unsure which compiler you should choose, we suggest the starting point should be the GNU compiler collection (GCC, PrgEnv-gnu ); this is perhaps the most commonly used by code developers, particularly in the open source software domain. A portable, standard-conforming code should (in principle) compile in any of the three programming environments. For users requiring specific compiler features, such as co-array Fortran, the recommended starting point would be Cray. The following sections provide further details of the different programming environments. Warning Intel compilers are not available on ARCHER2.","title":"Which compiler environment?"},{"location":"user-guide/dev-environment/#amd-optimizing-cc-compiler-aocc","text":"The AMD Optimizing C/++ Compiler (AOCC) is a clang-based optimising compiler. AOCC (despite its name) includes a flang-based Fortran compiler. Switch the the AOCC programming environment via $ module restore PrgEnv-aocc Note Further details on AOCC will appear here as they become available.","title":"AMD Optimizing C/C++ Compiler (AOCC)"},{"location":"user-guide/dev-environment/#aocc-reference-material","text":"AMD website https://developer.amd.com/amd-aocc/","title":"AOCC reference material"},{"location":"user-guide/dev-environment/#cray-compiler-environment-cce","text":"The Cray compiler environment (CCE) is the default compiler at the point of login. CCE supports C/C++ (along with unified parallel C UPC), and Fortran (including co-array Fortran). Support for OpenMP parallelism is available for both C/C++ and Fortran (currently OpenMP 4.5, with a number of exceptions). The Cray C/C++ compiler is based on a clang front end, and so compiler options are similar to those for gcc/clang. However, the Fortran compiler remains based around Cray-specific options. Be sure to separate C/C++ compiler options and Fortran compiler options (typically CFLAGS and FFLAGS ) if compiling mixed C/Fortran applications. Switch the the Cray programming environment via $ module restore PrgEnv-cray","title":"Cray compiler environment (CCE)"},{"location":"user-guide/dev-environment/#useful-cce-cc-options","text":"When using the compiler wrappers cc or CC , some of the following options may be useful: Language, warning, Debugging options: Option Comment -std=<standard> Default is -std=gnu11 ( gnu++14 for C++) [1] Performance options: Option Comment -Ofast Optimisation levels: -O0, -O1, -O2, -O3, -Ofast -ffp=level Floating point maths optimisations levels 0-4 [2] -flto Link time optimisation Miscellaneous options: Option Comment -fopenmp Compile OpenMP (default is off) -v Display verbose output from compiler stages Notes Option -std=gnu11 gives c11 plus GNU extensions (likewise c++14 plus GNU extensions). See https://gcc.gnu.org/onlinedocs/gcc-4.8.2/gcc/C-Extensions.html Option -ffp=3 is implied by -Ofast or -ffast-math","title":"Useful CCE C/C++ options"},{"location":"user-guide/dev-environment/#useful-cce-fortran-options","text":"Language, Warning, Debugging options: Option Comment -m <level> Message level (default -m 3 errors and warnings) Performance options: Option Comment -O <level> Optimisation levels: -O0 to -O3 (default -O2) -h fp<level> Floating point maths optimisations levels 0-3 -h ipa Inter-procedural analysis Miscellaneous options: Option Comment -h omp Compile OpenMP (default is -hnoomp ) -v Display verbose output from compiler stages","title":"Useful CCE Fortran options"},{"location":"user-guide/dev-environment/#gnu-compiler-collection-gcc","text":"The commonly used open source GNU compiler collection is available and provides C/C++ and Fortran compilers. The GNU compiler collection is loaded by switching to the GNU programming environment: $ module restore PrgEnv-gnu Bug The gcc/8.1.0 module is available on ARCHER2 but cannot be used as the supporting scientific and system libraries are not available. You should not use this version of GCC. Warning If you want to use GCC version 10 or greater to compile Fortran code, you must add the -fallow-argument-mismatch option when compiling otherwise you will see compile errors associated with MPI functions.","title":"GNU compiler collection (GCC)"},{"location":"user-guide/dev-environment/#useful-gnu-fortran-options","text":"Option Comment -std=<standard> Default is gnu -fallow-argument-mismatch Allow mismatched procedure arguments. This argument is required for compiling MPI Fortran code with GCC version 10 or greater -fbounds-check Use runtime checking of array indices -fopenmp Compile OpenMP (default is no OpenMP) -v Display verbose output from compiler stages Tip The standard in -std may be one of f95 f2003 , f2008 or f2018 . The default option -std=gnu is the latest Fortran standard plus gnu extensions. Warning Past versions of gfortran have allowed mismatched arguments to external procedures (e.g., where an explicit interface is not available). This is often the case for MPI routines where arrays of different types are passed to MPI_Send() and so on. This will now generate an error as not standard conforming. Use -fallow-argument-mismatch to reduce the error to a warning. The same effect may be achieved via -std=legacy .","title":"Useful Gnu Fortran options"},{"location":"user-guide/dev-environment/#reference-material","text":"C/C++ documentation https://gcc.gnu.org/onlinedocs/gcc-9.3.0/gcc/ Fortran documentation https://gcc.gnu.org/onlinedocs/gcc-9.3.0/gfortran/","title":"Reference material"},{"location":"user-guide/dev-environment/#message-passing-interface-mpi","text":"","title":"Message passing interface (MPI)"},{"location":"user-guide/dev-environment/#hpe-cray-mpich","text":"HPE Cray provide, as standard, an MPICH implementation of the message passing interface which is specifically optimised for the ARCHER2 network. The current implementation supports MPI standard version 3.1. The HPE Cray MPICH implementation is linked into software by default when compiling using the standard wrapper scripts: cc , CC and ftn .","title":"HPE Cray MPICH"},{"location":"user-guide/dev-environment/#mpi-reference-material","text":"MPI standard documents: https://www.mpi-forum.org/docs/","title":"MPI reference material"},{"location":"user-guide/dev-environment/#linking-and-libraries","text":"Linking to libraries is performed dynamically on ARCHER2. One can use the -craype-verbose flag to the compiler wrapper to check exactly what linker arguments are invoked. The compiler wrapper scripts encode the paths to the programming environment system libraries using RUNPATH. This ensures that the executable can find the correct runtime libraries without the matching software modules loaded. The library RUNPATH associated with an executable can be inspected via, e.g., $ readelf -d ./a.out (swap a.out for the name of the executable you are querying).","title":"Linking and libraries"},{"location":"user-guide/dev-environment/#commonly-used-libraries","text":"Modules with names prefixed by cray- are provided by HPE Cray, and are supported to be consistent with any of the programming environments and associated compilers. These modules should be the first choice for access to software libraries if available. Tip More information on the different software libraries on ARCHER2 can be found in the Software libraries section of the user guide.","title":"Commonly used libraries"},{"location":"user-guide/dev-environment/#build-instructions-for-software-on-archer2","text":"The ARCHER2 CSE team at EPCC and other contributors provide build configurations ando instructions for a range of research software, software libraries and tools on a variety of HPC systems (including ARCHER2) in a public Github repository. See: Build instructions repository The repository always welcomes contributions from the ARCHER2 user community.","title":"Build instructions for software on ARCHER2"},{"location":"user-guide/dev-environment/#support-for-building-software-on-archer2","text":"If you run into issues building software on ARCHER2 or the software you require is not available then please contact the ARCHER2 Service Desk with any questions you have.","title":"Support for building software on ARCHER2"},{"location":"user-guide/io/","text":"I/O and file systems Warning The ARCHER2 Service is not yet available. This documentation is in development. Using the ARCHER2 file systems Different file systems are configured for different purposes and performance. ARCHER2 has three file systems available to users: Node type Available file systems Login /home, /work Compute /work Warning Any data used in a parallel jobs should be located on /work (Lustre). Home file systems Home directories provide a convenient means for a user to have access to files such as source files, input files or configuration files. This file system is only mounted on the login nodes. The home directory for each user is located at: /home/[project code]/[group code]/[username] where [project code] is the code for your project (e.g., x01); [group code] is the code for your project group, if your project has groups, (e.g. x01-a) or the same as the project code, if not; [username] is your login name. Each project is allocated a portion of the total storage available, and the project PI will be able to sub-divide this quota among the groups and users within the project. As is standard practice on UNIX and Linux systems, the environment variable $HOME is automatically set to point to your home directory. It should be noted that the home file system is not designed, and does not have the capacity, to act as a long term archive for large sets of results. Work file system Warning There is no backup of data on any of the work file systems, which means that in the event of a major hardware failure, or if a user accidently deletes essential data, it will not be possible to recover the lost files. High-performance Lustre file system mounted on the compute nodes. All parallel calculations must be run from directories on the /work file system and all files required by the calculation (apart from the executable) must reside on /work . Each project will be assigned space on a particular Lustre partition with the assignments chosen to balance the load across the available infrastructure. The work directory for each user is located at: /work/[project code]/[group code]/[username] where [project code] is the code for your project (e.g., x01); [group code] is the code for your project group, if your project has groups, (e.g. x01-a) or the same as the project code, if not; [username] is your login name. Links from the /home file system to directories or files on /work are strongly discouraged. If links are used, executables and data files on /work to be used by applications on the compute nodes (i.e. those executed via the aprun command) should be referenced directly on /work . Sharing data with other ARCHER2 users How you share data with other ARCHER2 users depends on whether they belong to the same project as you or not. Each project has two levels of shared directories that can be used for sharing data. Sharing data with users in your project Each project has a directory called: /work/[project code]/[project code]/shared that has read/write permissions for all project members. You can place any data you wish to share with other project members in this directory. For example, if your project code is x01 the shared project directory would be located at: /work/x01/x01/shared Sharing data with all users Each project also has a higher level directory called: /work/[project code]/shared that is writable by all project members and readable by any user on the system. You can place any data you wish to share with other ARCHER2 users who are not members of your project in this directory. For example, if your project code is x01 the sharing directory would be located at: /work/x01/shared Common I/O patterns There is a number of I/O patterns that are frequently used in applications: Single file, single writer (Serial I/O) A common approach is to funnel all the I/O through a single master process. Although this has the advantage of producing a single file, the fact that only a single client is doing all the I/O means that it gains little benefit from the parallel file system. File-per-process (FPP) One of the first parallel strategies people use for I/O is for each parallel process to write to its own file. This is a simple scheme to implement and understand but has the disadvantage that, at the end of the calculation, the data is spread across many different files and may therefore be difficult to use for further analysis without a data reconstruction stage. Single file, multiple writers without collective operations There are a number of ways to achieve this. For example, many processes can open the same file but access different parts by skipping some initial offset; parallel I/O libraries such as MPI-IO, HDF5 and NetCDF also enable this. Shared-file I/O has the advantage that all the data is organised correctly in a single file making analysis or restart more straightforward. The problem is that, with many clients all accessing the same file, there can be a lot of contention for file system resources. Single Shared File with collective writes (SSF) The problem with having many clients performing I/O at the same time is that, to prevent them clashing with each other, the I/O library may have to take a conservative approach. For example, a file may be locked while each client is accessing it which means that I/O is effectively serialised and performance may be poor. However, if I/O is done collectively where the library knows that all clients are doing I/O at the same time, then reads and writes can be explicitly coordinated to avoid clashes. It is only through collective I/O that the full bandwidth of the file system can be realised while accessing a single file. Achieving efficient I/O This section provides information on getting the best performance out of the parallel /work file systems on ARCHER2 when writing data, particularly using parallel I/O patterns. Lustre The ARCHER2 /work file systems use Lustre as a parallel file system technology. The Lustre file system provides POSIX semantics (changes on one node are immediately visible on other nodes) and can support very high data rates for appropriate I/O patterns. Striping One of the main factors leading to the high performance of Lustre file systems is the ability to stripe data across multiple Object Storage Targets (OSTs) in a round-robin fashion. Files are striped when the data is split up in chunks that will then be stored on different OSTs across the Lustre system. Striping might improve the I/O performance because it increases the available bandwith since multiple processes can read and write the same files simultaneously. However striping can also increase the overhead. Choosing the right striping configuration is key to obtain high performance results. Users have control of a number of striping settings on Lustre file systems. Although these parameters can be set on a per-file basis they are usually set on directory where your output files will be written so that all output files inherit the settings. Default configuration The /work file systems on ARCHER2 have the same default stripe settings: A default stripe count of 1 A default stripe size of 1 MiB (1048576 bytes) These settings have been chosen to provide a good compromise for the wide variety of I/O patterns that are seen on the system but are unlikely to be optimal for any one particular scenario. The Lustre command to query the stripe settings for a directory (or file) is lfs getstripe . For example, to query the stripe settings of an already created directory res_dir : [auser@archer2]$ lfs getstripe res_dir/ res_dir stripe_count: 1 stripe_size: 1048576 stripe_offset: -1 Setting Custom Striping Configurations Hint You cannot currently perform parallel IO to a striped file if the number of MPI processes is larger than the stripe count. Although this is unlikely to be an issue in production runs, where the number of processes will normally be in the hundreds, it could cause an issue during development or benchmarking. Users can set stripe settings for a directory (or file) using the lfs setstripe command. The options for lfs setstripe are: [--stripe-count|-c] to set the stripe count; 0 means use the system default (usually 1) and -1 means stripe over all available OSTs. [--stripe-size|-s] to set the stripe size; 0 means use the system default (usually 1 MB) otherwise use k, m or g for KB, MB or GB respectively [--stripe-index|-i] to set the OST index (starting at 0) on which to start striping for this file. An index of -1 allows the MDS to choose the starting index and it is strongly recommended, as this allows space and load balancing to be done by the MDS as needed. For example, to set a stripe size of 4 MiB for the existing directory res_dir , along with maximum striping count you would use: [auser@archer2]$ lfs setstripe -s 4m -c -1 res_dir/ Recommended ARCHER2 I/O settings Note We will add advice on I/O settings soon. I/O Profiling Note We will add advice on I/O profiling soon.","title":"I/O and file systems"},{"location":"user-guide/io/#io-and-file-systems","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development.","title":"I/O and file systems"},{"location":"user-guide/io/#using-the-archer2-file-systems","text":"Different file systems are configured for different purposes and performance. ARCHER2 has three file systems available to users: Node type Available file systems Login /home, /work Compute /work Warning Any data used in a parallel jobs should be located on /work (Lustre).","title":"Using the ARCHER2 file systems"},{"location":"user-guide/io/#home-file-systems","text":"Home directories provide a convenient means for a user to have access to files such as source files, input files or configuration files. This file system is only mounted on the login nodes. The home directory for each user is located at: /home/[project code]/[group code]/[username] where [project code] is the code for your project (e.g., x01); [group code] is the code for your project group, if your project has groups, (e.g. x01-a) or the same as the project code, if not; [username] is your login name. Each project is allocated a portion of the total storage available, and the project PI will be able to sub-divide this quota among the groups and users within the project. As is standard practice on UNIX and Linux systems, the environment variable $HOME is automatically set to point to your home directory. It should be noted that the home file system is not designed, and does not have the capacity, to act as a long term archive for large sets of results.","title":"Home file systems"},{"location":"user-guide/io/#work-file-system","text":"Warning There is no backup of data on any of the work file systems, which means that in the event of a major hardware failure, or if a user accidently deletes essential data, it will not be possible to recover the lost files. High-performance Lustre file system mounted on the compute nodes. All parallel calculations must be run from directories on the /work file system and all files required by the calculation (apart from the executable) must reside on /work . Each project will be assigned space on a particular Lustre partition with the assignments chosen to balance the load across the available infrastructure. The work directory for each user is located at: /work/[project code]/[group code]/[username] where [project code] is the code for your project (e.g., x01); [group code] is the code for your project group, if your project has groups, (e.g. x01-a) or the same as the project code, if not; [username] is your login name. Links from the /home file system to directories or files on /work are strongly discouraged. If links are used, executables and data files on /work to be used by applications on the compute nodes (i.e. those executed via the aprun command) should be referenced directly on /work .","title":"Work file system"},{"location":"user-guide/io/#sharing-data-with-other-archer2-users","text":"How you share data with other ARCHER2 users depends on whether they belong to the same project as you or not. Each project has two levels of shared directories that can be used for sharing data.","title":"Sharing data with other ARCHER2 users"},{"location":"user-guide/io/#sharing-data-with-users-in-your-project","text":"Each project has a directory called: /work/[project code]/[project code]/shared that has read/write permissions for all project members. You can place any data you wish to share with other project members in this directory. For example, if your project code is x01 the shared project directory would be located at: /work/x01/x01/shared","title":"Sharing data with users in your project"},{"location":"user-guide/io/#sharing-data-with-all-users","text":"Each project also has a higher level directory called: /work/[project code]/shared that is writable by all project members and readable by any user on the system. You can place any data you wish to share with other ARCHER2 users who are not members of your project in this directory. For example, if your project code is x01 the sharing directory would be located at: /work/x01/shared","title":"Sharing data with all users"},{"location":"user-guide/io/#common-io-patterns","text":"There is a number of I/O patterns that are frequently used in applications:","title":"Common I/O patterns"},{"location":"user-guide/io/#single-file-single-writer-serial-io","text":"A common approach is to funnel all the I/O through a single master process. Although this has the advantage of producing a single file, the fact that only a single client is doing all the I/O means that it gains little benefit from the parallel file system.","title":"Single file, single writer (Serial I/O)"},{"location":"user-guide/io/#file-per-process-fpp","text":"One of the first parallel strategies people use for I/O is for each parallel process to write to its own file. This is a simple scheme to implement and understand but has the disadvantage that, at the end of the calculation, the data is spread across many different files and may therefore be difficult to use for further analysis without a data reconstruction stage.","title":"File-per-process (FPP)"},{"location":"user-guide/io/#single-file-multiple-writers-without-collective-operations","text":"There are a number of ways to achieve this. For example, many processes can open the same file but access different parts by skipping some initial offset; parallel I/O libraries such as MPI-IO, HDF5 and NetCDF also enable this. Shared-file I/O has the advantage that all the data is organised correctly in a single file making analysis or restart more straightforward. The problem is that, with many clients all accessing the same file, there can be a lot of contention for file system resources.","title":"Single file, multiple writers without collective operations"},{"location":"user-guide/io/#single-shared-file-with-collective-writes-ssf","text":"The problem with having many clients performing I/O at the same time is that, to prevent them clashing with each other, the I/O library may have to take a conservative approach. For example, a file may be locked while each client is accessing it which means that I/O is effectively serialised and performance may be poor. However, if I/O is done collectively where the library knows that all clients are doing I/O at the same time, then reads and writes can be explicitly coordinated to avoid clashes. It is only through collective I/O that the full bandwidth of the file system can be realised while accessing a single file.","title":"Single Shared File with collective writes (SSF)"},{"location":"user-guide/io/#achieving-efficient-io","text":"This section provides information on getting the best performance out of the parallel /work file systems on ARCHER2 when writing data, particularly using parallel I/O patterns.","title":"Achieving efficient I/O"},{"location":"user-guide/io/#lustre","text":"The ARCHER2 /work file systems use Lustre as a parallel file system technology. The Lustre file system provides POSIX semantics (changes on one node are immediately visible on other nodes) and can support very high data rates for appropriate I/O patterns.","title":"Lustre"},{"location":"user-guide/io/#striping","text":"One of the main factors leading to the high performance of Lustre file systems is the ability to stripe data across multiple Object Storage Targets (OSTs) in a round-robin fashion. Files are striped when the data is split up in chunks that will then be stored on different OSTs across the Lustre system. Striping might improve the I/O performance because it increases the available bandwith since multiple processes can read and write the same files simultaneously. However striping can also increase the overhead. Choosing the right striping configuration is key to obtain high performance results. Users have control of a number of striping settings on Lustre file systems. Although these parameters can be set on a per-file basis they are usually set on directory where your output files will be written so that all output files inherit the settings.","title":"Striping"},{"location":"user-guide/io/#default-configuration","text":"The /work file systems on ARCHER2 have the same default stripe settings: A default stripe count of 1 A default stripe size of 1 MiB (1048576 bytes) These settings have been chosen to provide a good compromise for the wide variety of I/O patterns that are seen on the system but are unlikely to be optimal for any one particular scenario. The Lustre command to query the stripe settings for a directory (or file) is lfs getstripe . For example, to query the stripe settings of an already created directory res_dir : [auser@archer2]$ lfs getstripe res_dir/ res_dir stripe_count: 1 stripe_size: 1048576 stripe_offset: -1","title":"Default configuration"},{"location":"user-guide/io/#setting-custom-striping-configurations","text":"Hint You cannot currently perform parallel IO to a striped file if the number of MPI processes is larger than the stripe count. Although this is unlikely to be an issue in production runs, where the number of processes will normally be in the hundreds, it could cause an issue during development or benchmarking. Users can set stripe settings for a directory (or file) using the lfs setstripe command. The options for lfs setstripe are: [--stripe-count|-c] to set the stripe count; 0 means use the system default (usually 1) and -1 means stripe over all available OSTs. [--stripe-size|-s] to set the stripe size; 0 means use the system default (usually 1 MB) otherwise use k, m or g for KB, MB or GB respectively [--stripe-index|-i] to set the OST index (starting at 0) on which to start striping for this file. An index of -1 allows the MDS to choose the starting index and it is strongly recommended, as this allows space and load balancing to be done by the MDS as needed. For example, to set a stripe size of 4 MiB for the existing directory res_dir , along with maximum striping count you would use: [auser@archer2]$ lfs setstripe -s 4m -c -1 res_dir/","title":"Setting Custom Striping Configurations"},{"location":"user-guide/io/#recommended-archer2-io-settings","text":"Note We will add advice on I/O settings soon.","title":"Recommended ARCHER2 I/O settings"},{"location":"user-guide/io/#io-profiling","text":"Note We will add advice on I/O profiling soon.","title":"I/O Profiling"},{"location":"user-guide/profile/","text":"Profiling Warning The ARCHER2 Service is not yet available. This documentation is in development. CrayPat-lite CrayPat-lite is a simplified and easy-to-use version of the Cray Performance Measurement and Analysis Tool (CrayPat) set. CrayPat-lite provides basic performance analysis information automatically, with a minimum of user interaction, and yet offers information useful to users wishing to explore a program's behavior further using the full CrayPat tool set. To use CrayPat-lite you only need to make sure that the base CrayPat perftools-base module has been loaded, an instrumentation module can then be loaded for further experimentation. How to use CrayPat-lite Ensure the perftools-base module is loaded auser@uan01:/work/t01/t01/auser> module load perftools-base Load perfotools-lite module auser@uan01:/work/t01/t01/auser> module load perftools-lite Compile your application normally. An information message from CrayPat-lite will appear indicating that the executable has been instrumented. auser@uan01:/work/t01/t01/auser> cc -h std=c99 -o myapplication.x myapplication.c INFO: creating the CrayPat-instrumented executable 'myapplication.x' (lite-samples) ...OK Run the generated executable normally submitting a job. #!/bin/bash #SBATCH --job-name=craypat_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the batch environment module load epcc-job-env # Launch the parallel program srun mpi_test.x Analyse the data After the job finishes executing, CrayPat-lite output should be printed to stdout i.e. at the end of the job's output file generated. A new directory will also be created in the directory the run occurred in with .rpt and .ap2 files. The .rpt files are text files that contain the same information printed in the job's output file, the .ap2 files can be used to obtained more detailed information and can be visualized with the Cray Apprentice2 tool (for information on using this, please take a look at Cray Apprentice2 ). Further help CrayPat-lite User Guide CrayPat The Cray Performance Analysis Tool (CrayPAT) is a powerful framework for analysing a parallel application\u2019s performance on Cray supercomputers. It can provide very detailed information on the timing and performance of individual application procedures. CrayPat can perform two types of performance analysis: sampling experiments and tracing experiments. A sampling experiment probes the code at a predefined interval and produces a report based on these statistics. A tracing experiment explicitly monitors the code performance within named routines. Typically, the overhead associated with a tracing experiment is higher than that associated with a sampling experiment but provides much more detailed information. The key to getting useful data out of a sampling experiment is to run your profiling for a representative length of time. Sampling analysis Ensure the perftools-base module is loaded module load perftools-base Load perftools module module load perftools Compile your code in the standard way always using the Cray compiler wrappers (ftn, cc and CC). Object files need to be made available to CrayPat to correctly build an instrumented executable for profiling or tracing, this means that compile and link stage should be separated by using the -c compile flag. auser@uan01:/work/t01/t01/auser> cc -h std=c99 -c jacobi.c auser@uan01:/work/t01/t01/auser> cc jacobi.o -o jacobi Instrument your application To instrument then the binary, run the pat_build command. This will generate a new binary with +pat appended to the end (e.g. jacobi+pat ) auser@uan01:/work/t01/t01/auser> pat_build jacobi Run the new executable with +pat appended as you would with the regular executable. This will generate performance data files with the suffix .xf (e.g. jacobi+pat+12265-1573s/xf-files ). Generate report data This .xt file contains the raw sampling data from the run and needs to be post processed to produce useful results. This is done using the pat_report tool which converts all the raw data into a summarised and readable form. [user@archer2]$ pat_report jacobi+pat+12265-1573s Table 1: Profile by Function (limited entries shown) Samp% | Samp | Imb. | Imb. | Group | | Samp | Samp% | Function | | | | PE=HIDE 100.0% | 849.5 | -- | -- | Total |-------------------------------------------------- | 56.7% | 481.4 | -- | -- | MPI ||------------------------------------------------- || 48.7% | 414.1 | 50.9 | 11.0% | MPI_Allreduce || 4.4% | 37.5 | 118.5 | 76.6% | MPI_Waitall || 3.0% | 25.2 | 44.8 | 64.5% | MPI_Isend ||================================================= | 29.9% | 253.9 | 55.1 | 18.0% | USER ||------------------------------------------------- || 29.9% | 253.9 | 55.1 | 18.0% | main ||================================================= | 13.4% | 114.1 | -- | -- | ETC ||------------------------------------------------- || 13.4% | 113.9 | 26.1 | 18.8% | __cray_memcpy_SNB |================================================== This report will generate two more files, one with the extension .ap2 which holds the same data as the .xf but in the post processed form. The other file has a .apa extension and is a text file with a suggested configuration for generating a traced experiment. The .ap2 file generated is used to view performance data graphically with the Cray Apprentice2 tool (for information on using this, please take a look at Cray Apprentice2 ), and the latter is used for more detailed tracing experiments. The pat_report command is able to produce many different profile reports from the profile data. You can select a predefined report with the -O flag to pat_report . A selection of the most generally useful predefined report types are ca+src - Show the callers (bottom-up view) leading to the routines that have a high use in the report and include source code line numbers for the calls and time-consuming statements. load_balance - Show load-balance statistics for the high-use routines in the program. Parallel processes with minimum, maximum and median times for routines will be displayed. Only available with tracing experiments. mpi_callers - Show MPI message statistics. Only available with tracing experiments. Example output: auser@uan01:/work/t01/t01/auser> pat_report -O ca+src,load_balance jacobi+pat+12265-1573s Table 1: Profile by Function and Callers, with Line Numbers (limited entries shown) Samp% | Samp | Imb. | Imb. | Group | | Samp | Samp% | Function | | | | PE=HIDE 100.0% | 849.5 | -- | -- | Total |-------------------------------------------------- |-------------------------------------- | 56.7% | 481.4 | MPI ||------------------------------------- || 48.7% | 414.1 | MPI_Allreduce 3| | | main:jacobi.c:line.80 || 4.4% | 37.5 | MPI_Waitall 3| | | main:jacobi.c:line.73 || 3.0% | 25.2 | MPI_Isend |||------------------------------------ 3|| 1.6% | 13.2 | main:jacobi.c:line.65 3|| 1.4% | 12.0 | main:jacobi.c:line.69 ||===================================== | 29.9% | 253.9 | USER ||------------------------------------- || 29.9% | 253.9 | main |||------------------------------------ 3|| 18.7% | 159.0 | main:jacobi.c:line.76 3|| 9.1% | 76.9 | main:jacobi.c:line.84 |||==================================== ||===================================== | 13.4% | 114.1 | ETC ||------------------------------------- || 13.4% | 113.9 | __cray_memcpy_SNB 3| | | __cray_memcpy_SNB |====================================== Tracing analysis Automatic Program Analysis (APA) We can produce a focused tracing experiment based on the results from the sampling experiment using pat_build with the .apa file produced during the sampling. auser@uan01:/work/t01/t01/auser> pat_build -O jacobi+pat+12265-1573s/build-options.apa This will produce a third binary with extension +apa . This binary should once again be run on the compute nodes and the name of the executable changed to jacobi+apa . As with the sampling analysis, a report can be produced using pat_report . For example: auser@uan01:/work/t01/t01/auser> pat_report jacobi+apa+13955-1573t Table 1: Profile by Function Group and Function (limited entries shown) Time% | Time | Imb. | Imb. | Calls | Group | | Time | Time% | | Function | | | | | PE=HIDE 100.0% | 12.987762 | -- | -- | 1,387,544.9 | Total |------------------------------------------------------------------------- | 44.9% | 5.831320 | -- | -- | 2.0 | USER ||------------------------------------------------------------------------ || 44.9% | 5.831229 | 0.398671 | 6.4% | 1.0 | main ||======================================================================== | 29.2% | 3.789904 | -- | -- | 199,111.0 | MPI_SYNC ||------------------------------------------------------------------------ || 29.2% | 3.789115 | 1.792050 | 47.3% | 199,109.0 | MPI_Allreduce(sync) ||======================================================================== | 25.9% | 3.366537 | -- | -- | 1,188,431.9 | MPI ||------------------------------------------------------------------------ || 18.0% | 2.334765 | 0.164646 | 6.6% | 199,109.0 | MPI_Allreduce || 3.7% | 0.486714 | 0.882654 | 65.0% | 199,108.0 | MPI_Waitall || 3.3% | 0.428731 | 0.557342 | 57.0% | 395,104.9 | MPI_Isend |========================================================================= Manual Program Analysis CrayPat allows you to manually choose your profiling preference. This is particularly useful if the APA mode does not meet your tracing analysis requirements. The entire program can be traced as a whole using -w : auser@uan01:/work/t01/t01/auser> pat_build -w jacobi Using -g a program can be instrumented to trace all function entry point references belonging to the trace function group tracegroup (mpi, libsci, lapack, scalapack, heap, etc) auser@uan01:/work/t01/t01/auser> pat_build -w -g mpi jacobi Dynamically-linked binaries CrayPat allows you to profile un-instrumented, dynamically linked binaries with the pat_run utility. pat_run delivers profiling information for codes that cannot easily be rebuilt. To use pat_run : Load the perfotools-base module if it is not already loaded module load perftools-base Run your application normally including the pat_run command rigth after your srun options srun [srun-options] pat_run [pat_run-options] program [program-options] Use pat_report to examine any data collected during the execution of your application. auser@uan01:/work/t01/t01/auser> pat_report jacobi+pat+12265-1573s Some useful pat_run options are: -w Collect data by tracing. -g Trace functions belonging to group names. See the -g option in pat_build(1) for a list of valid tracegroup values. -r Generate a text report upon successful execution. Further help CrayPat User Guide Cray Apprentice2 Cray Apprentice2 is an optional GUI tool that is used to visualize and manipulate the performance analysis data captured during program execution. Cray Apprentice2 can be run either on the Cray system or, optionally, on a standalone Linux desktop machine. Cray Apprentice2 can display a wide variety of reports and graphs, depending on the type of program being analyzed, the way in which the program was instrumented for data capture, and the data that was collected during program execution. You will need to use CrayPat first, to instrument your program and capture performance analysis data, and then use Cray Apprentice2 to visualize and explore the resulting data files. The number and appearance of the reports that can be generated using Cray Apprentice2 is determined by the kind and quantity of data captured during program execution, which in turn is determined by the way in which the program was instrumented and the environment variables in effect at the time of program execution. For example, changing the PAT_RT_SUMMARY environment variable to 0 before executing the instrumented program nearly doubles the number of reports available when analyzing the resulting data in Cray Apprentice2. export PAT_RT_SUMMARY=0 To use Cray Apprentice2 ( app2 ), load perftools-base module if it is not already loaded module load perftools-base then open the Cray Apprentice2 data ( .ap2 ) generated during the instrumentation phase auser@uan01:/work/t01/t01/auser> app2 jacobi+pat+12265-1573s/datafile.ap2 Hardware Performance Counters Note Information on hardware counters will be added soon.","title":"Profiling"},{"location":"user-guide/profile/#profiling","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development.","title":"Profiling"},{"location":"user-guide/profile/#craypat-lite","text":"CrayPat-lite is a simplified and easy-to-use version of the Cray Performance Measurement and Analysis Tool (CrayPat) set. CrayPat-lite provides basic performance analysis information automatically, with a minimum of user interaction, and yet offers information useful to users wishing to explore a program's behavior further using the full CrayPat tool set. To use CrayPat-lite you only need to make sure that the base CrayPat perftools-base module has been loaded, an instrumentation module can then be loaded for further experimentation.","title":"CrayPat-lite"},{"location":"user-guide/profile/#how-to-use-craypat-lite","text":"Ensure the perftools-base module is loaded auser@uan01:/work/t01/t01/auser> module load perftools-base Load perfotools-lite module auser@uan01:/work/t01/t01/auser> module load perftools-lite Compile your application normally. An information message from CrayPat-lite will appear indicating that the executable has been instrumented. auser@uan01:/work/t01/t01/auser> cc -h std=c99 -o myapplication.x myapplication.c INFO: creating the CrayPat-instrumented executable 'myapplication.x' (lite-samples) ...OK Run the generated executable normally submitting a job. #!/bin/bash #SBATCH --job-name=craypat_test #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --time=00:20:00 #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the batch environment module load epcc-job-env # Launch the parallel program srun mpi_test.x Analyse the data After the job finishes executing, CrayPat-lite output should be printed to stdout i.e. at the end of the job's output file generated. A new directory will also be created in the directory the run occurred in with .rpt and .ap2 files. The .rpt files are text files that contain the same information printed in the job's output file, the .ap2 files can be used to obtained more detailed information and can be visualized with the Cray Apprentice2 tool (for information on using this, please take a look at Cray Apprentice2 ).","title":"How to use CrayPat-lite"},{"location":"user-guide/profile/#further-help","text":"CrayPat-lite User Guide","title":"Further help"},{"location":"user-guide/profile/#craypat","text":"The Cray Performance Analysis Tool (CrayPAT) is a powerful framework for analysing a parallel application\u2019s performance on Cray supercomputers. It can provide very detailed information on the timing and performance of individual application procedures. CrayPat can perform two types of performance analysis: sampling experiments and tracing experiments. A sampling experiment probes the code at a predefined interval and produces a report based on these statistics. A tracing experiment explicitly monitors the code performance within named routines. Typically, the overhead associated with a tracing experiment is higher than that associated with a sampling experiment but provides much more detailed information. The key to getting useful data out of a sampling experiment is to run your profiling for a representative length of time.","title":"CrayPat"},{"location":"user-guide/profile/#sampling-analysis","text":"Ensure the perftools-base module is loaded module load perftools-base Load perftools module module load perftools Compile your code in the standard way always using the Cray compiler wrappers (ftn, cc and CC). Object files need to be made available to CrayPat to correctly build an instrumented executable for profiling or tracing, this means that compile and link stage should be separated by using the -c compile flag. auser@uan01:/work/t01/t01/auser> cc -h std=c99 -c jacobi.c auser@uan01:/work/t01/t01/auser> cc jacobi.o -o jacobi Instrument your application To instrument then the binary, run the pat_build command. This will generate a new binary with +pat appended to the end (e.g. jacobi+pat ) auser@uan01:/work/t01/t01/auser> pat_build jacobi Run the new executable with +pat appended as you would with the regular executable. This will generate performance data files with the suffix .xf (e.g. jacobi+pat+12265-1573s/xf-files ). Generate report data This .xt file contains the raw sampling data from the run and needs to be post processed to produce useful results. This is done using the pat_report tool which converts all the raw data into a summarised and readable form. [user@archer2]$ pat_report jacobi+pat+12265-1573s Table 1: Profile by Function (limited entries shown) Samp% | Samp | Imb. | Imb. | Group | | Samp | Samp% | Function | | | | PE=HIDE 100.0% | 849.5 | -- | -- | Total |-------------------------------------------------- | 56.7% | 481.4 | -- | -- | MPI ||------------------------------------------------- || 48.7% | 414.1 | 50.9 | 11.0% | MPI_Allreduce || 4.4% | 37.5 | 118.5 | 76.6% | MPI_Waitall || 3.0% | 25.2 | 44.8 | 64.5% | MPI_Isend ||================================================= | 29.9% | 253.9 | 55.1 | 18.0% | USER ||------------------------------------------------- || 29.9% | 253.9 | 55.1 | 18.0% | main ||================================================= | 13.4% | 114.1 | -- | -- | ETC ||------------------------------------------------- || 13.4% | 113.9 | 26.1 | 18.8% | __cray_memcpy_SNB |================================================== This report will generate two more files, one with the extension .ap2 which holds the same data as the .xf but in the post processed form. The other file has a .apa extension and is a text file with a suggested configuration for generating a traced experiment. The .ap2 file generated is used to view performance data graphically with the Cray Apprentice2 tool (for information on using this, please take a look at Cray Apprentice2 ), and the latter is used for more detailed tracing experiments. The pat_report command is able to produce many different profile reports from the profile data. You can select a predefined report with the -O flag to pat_report . A selection of the most generally useful predefined report types are ca+src - Show the callers (bottom-up view) leading to the routines that have a high use in the report and include source code line numbers for the calls and time-consuming statements. load_balance - Show load-balance statistics for the high-use routines in the program. Parallel processes with minimum, maximum and median times for routines will be displayed. Only available with tracing experiments. mpi_callers - Show MPI message statistics. Only available with tracing experiments. Example output: auser@uan01:/work/t01/t01/auser> pat_report -O ca+src,load_balance jacobi+pat+12265-1573s Table 1: Profile by Function and Callers, with Line Numbers (limited entries shown) Samp% | Samp | Imb. | Imb. | Group | | Samp | Samp% | Function | | | | PE=HIDE 100.0% | 849.5 | -- | -- | Total |-------------------------------------------------- |-------------------------------------- | 56.7% | 481.4 | MPI ||------------------------------------- || 48.7% | 414.1 | MPI_Allreduce 3| | | main:jacobi.c:line.80 || 4.4% | 37.5 | MPI_Waitall 3| | | main:jacobi.c:line.73 || 3.0% | 25.2 | MPI_Isend |||------------------------------------ 3|| 1.6% | 13.2 | main:jacobi.c:line.65 3|| 1.4% | 12.0 | main:jacobi.c:line.69 ||===================================== | 29.9% | 253.9 | USER ||------------------------------------- || 29.9% | 253.9 | main |||------------------------------------ 3|| 18.7% | 159.0 | main:jacobi.c:line.76 3|| 9.1% | 76.9 | main:jacobi.c:line.84 |||==================================== ||===================================== | 13.4% | 114.1 | ETC ||------------------------------------- || 13.4% | 113.9 | __cray_memcpy_SNB 3| | | __cray_memcpy_SNB |======================================","title":"Sampling analysis"},{"location":"user-guide/profile/#tracing-analysis","text":"","title":"Tracing analysis"},{"location":"user-guide/profile/#automatic-program-analysis-apa","text":"We can produce a focused tracing experiment based on the results from the sampling experiment using pat_build with the .apa file produced during the sampling. auser@uan01:/work/t01/t01/auser> pat_build -O jacobi+pat+12265-1573s/build-options.apa This will produce a third binary with extension +apa . This binary should once again be run on the compute nodes and the name of the executable changed to jacobi+apa . As with the sampling analysis, a report can be produced using pat_report . For example: auser@uan01:/work/t01/t01/auser> pat_report jacobi+apa+13955-1573t Table 1: Profile by Function Group and Function (limited entries shown) Time% | Time | Imb. | Imb. | Calls | Group | | Time | Time% | | Function | | | | | PE=HIDE 100.0% | 12.987762 | -- | -- | 1,387,544.9 | Total |------------------------------------------------------------------------- | 44.9% | 5.831320 | -- | -- | 2.0 | USER ||------------------------------------------------------------------------ || 44.9% | 5.831229 | 0.398671 | 6.4% | 1.0 | main ||======================================================================== | 29.2% | 3.789904 | -- | -- | 199,111.0 | MPI_SYNC ||------------------------------------------------------------------------ || 29.2% | 3.789115 | 1.792050 | 47.3% | 199,109.0 | MPI_Allreduce(sync) ||======================================================================== | 25.9% | 3.366537 | -- | -- | 1,188,431.9 | MPI ||------------------------------------------------------------------------ || 18.0% | 2.334765 | 0.164646 | 6.6% | 199,109.0 | MPI_Allreduce || 3.7% | 0.486714 | 0.882654 | 65.0% | 199,108.0 | MPI_Waitall || 3.3% | 0.428731 | 0.557342 | 57.0% | 395,104.9 | MPI_Isend |=========================================================================","title":"Automatic Program Analysis (APA)"},{"location":"user-guide/profile/#manual-program-analysis","text":"CrayPat allows you to manually choose your profiling preference. This is particularly useful if the APA mode does not meet your tracing analysis requirements. The entire program can be traced as a whole using -w : auser@uan01:/work/t01/t01/auser> pat_build -w jacobi Using -g a program can be instrumented to trace all function entry point references belonging to the trace function group tracegroup (mpi, libsci, lapack, scalapack, heap, etc) auser@uan01:/work/t01/t01/auser> pat_build -w -g mpi jacobi","title":"Manual Program Analysis"},{"location":"user-guide/profile/#dynamically-linked-binaries","text":"CrayPat allows you to profile un-instrumented, dynamically linked binaries with the pat_run utility. pat_run delivers profiling information for codes that cannot easily be rebuilt. To use pat_run : Load the perfotools-base module if it is not already loaded module load perftools-base Run your application normally including the pat_run command rigth after your srun options srun [srun-options] pat_run [pat_run-options] program [program-options] Use pat_report to examine any data collected during the execution of your application. auser@uan01:/work/t01/t01/auser> pat_report jacobi+pat+12265-1573s Some useful pat_run options are: -w Collect data by tracing. -g Trace functions belonging to group names. See the -g option in pat_build(1) for a list of valid tracegroup values. -r Generate a text report upon successful execution.","title":"Dynamically-linked binaries"},{"location":"user-guide/profile/#further-help_1","text":"CrayPat User Guide","title":"Further help"},{"location":"user-guide/profile/#cray-apprentice2","text":"Cray Apprentice2 is an optional GUI tool that is used to visualize and manipulate the performance analysis data captured during program execution. Cray Apprentice2 can be run either on the Cray system or, optionally, on a standalone Linux desktop machine. Cray Apprentice2 can display a wide variety of reports and graphs, depending on the type of program being analyzed, the way in which the program was instrumented for data capture, and the data that was collected during program execution. You will need to use CrayPat first, to instrument your program and capture performance analysis data, and then use Cray Apprentice2 to visualize and explore the resulting data files. The number and appearance of the reports that can be generated using Cray Apprentice2 is determined by the kind and quantity of data captured during program execution, which in turn is determined by the way in which the program was instrumented and the environment variables in effect at the time of program execution. For example, changing the PAT_RT_SUMMARY environment variable to 0 before executing the instrumented program nearly doubles the number of reports available when analyzing the resulting data in Cray Apprentice2. export PAT_RT_SUMMARY=0 To use Cray Apprentice2 ( app2 ), load perftools-base module if it is not already loaded module load perftools-base then open the Cray Apprentice2 data ( .ap2 ) generated during the instrumentation phase auser@uan01:/work/t01/t01/auser> app2 jacobi+pat+12265-1573s/datafile.ap2","title":"Cray Apprentice2"},{"location":"user-guide/profile/#hardware-performance-counters","text":"Note Information on hardware counters will be added soon.","title":"Hardware Performance Counters"},{"location":"user-guide/python/","text":"Using Python Python is supported on ARCHER2 both for running intensive parallel jobs and also as an analysis tool. This section describes how to use Python in either of these scenarios. The Python installations on ARCHER2 contain some of the most commonly used modules. If you wish to install additional Python modules, we recommend that you use the pip command after loading the cray-python module. This is described in more detail below. Note When you log onto ARCHER2, no Python module is loaded by default. You will generally need to load the cray-python module to access the functionality described below. Running python without loading a module first will result in your using the operating system default Python which is likely not what you intend. HPE Cray Python distribution The recommended way to use Python on ARCHER2 is to use the HPE Cray Python distribution. The HPE Cray distribution provides Python 3 along with some of the most common packages used for scientific computation and data analysis. These include: numpy and scipy - built against HPE Cray LibSci mpi4py - built against HPE Cray MPICH dask The HPE Cray Python distribution can be loaded (either on the front-end or in a submission script) using: module load cray-python Tip The HPE Cray Python distribution provides Python 3. There is no Python 2 version as Python 2 is now deprecated. Adding your own packages If the packages you require are not included in the HPE Cray Python distribution, further packages can be added using pip . However, as the /home file systems are not available on the compute nodes, you will need to modify the default install location that pip uses to point to a location on the /work file systems (by default, pip installs into $HOME/.local ). To do this, you set the PYTHONUSERBASE environment variable to point to a location on /work, for example: export PYTHONUSERBASE=/work/t01/t01/auser/.local You will also need to ensure that the location of commands installed by pip are available on the command line by modifying the PATH environment variable. Once you have set PYTHONUSERBASE as described above, you can do this with the command: export PATH=$PYTHONUSERBASE/bin:$PATH We would recommend adding both of these commands to your $HOME/.bashrc file to ensure they are set by default when you log in to ARCHER2. Once, you have done this, you can use pip to add packages. This can be done using: pip install --user <package_name> This uses the --user flag to ensure the packages are installed in your user directory. We recommend that you use the pipenv and/or virtualenv packages to manage your Python environments. For information on how to do this see: Pipenv and Virtual Environments Running Python on the compute nodes In this section we provide example Python job submission scripts for a variety of scenarios of using Python on the ARCHER2 compute nodes. Example serial Python submission script #!/bin/bash --login #SBATCH --name=python_test #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=00:10:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the batch environment module load epcc-job-env # Load the Python module module load cray-python # Run your Python progamme python python_test.py Example mpi4py job submission script Programmes that have been parallelised with mpi4py can be run on multiple processors on ARCHER2. A sample submission script is given below. The primary difference from the Python submission script in the previous section is that we must run the programme using srun python my_prog.py instead of python my_prog,py . Failing to do so will cause a segmentation fault in your programme when it reaches the line from mpi4py import MPI . #!/bin/bash --login # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=mpi4py_test #SBATCH --nodes=1 #SBATCH --tasks-per-node=2 #SBATCH --cpus-per-task=1 #SBATCH --time=0:10:0 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the batch environment module load epcc-job-env # Load the Python module module load cray-python # Run your Python programme # Note that srun MUST be used to wrap the call to python, otherwise an error # will occur srun python mpi4py_test.py","title":"Using Python"},{"location":"user-guide/python/#using-python","text":"Python is supported on ARCHER2 both for running intensive parallel jobs and also as an analysis tool. This section describes how to use Python in either of these scenarios. The Python installations on ARCHER2 contain some of the most commonly used modules. If you wish to install additional Python modules, we recommend that you use the pip command after loading the cray-python module. This is described in more detail below. Note When you log onto ARCHER2, no Python module is loaded by default. You will generally need to load the cray-python module to access the functionality described below. Running python without loading a module first will result in your using the operating system default Python which is likely not what you intend.","title":"Using Python"},{"location":"user-guide/python/#hpe-cray-python-distribution","text":"The recommended way to use Python on ARCHER2 is to use the HPE Cray Python distribution. The HPE Cray distribution provides Python 3 along with some of the most common packages used for scientific computation and data analysis. These include: numpy and scipy - built against HPE Cray LibSci mpi4py - built against HPE Cray MPICH dask The HPE Cray Python distribution can be loaded (either on the front-end or in a submission script) using: module load cray-python Tip The HPE Cray Python distribution provides Python 3. There is no Python 2 version as Python 2 is now deprecated.","title":"HPE Cray Python distribution"},{"location":"user-guide/python/#adding-your-own-packages","text":"If the packages you require are not included in the HPE Cray Python distribution, further packages can be added using pip . However, as the /home file systems are not available on the compute nodes, you will need to modify the default install location that pip uses to point to a location on the /work file systems (by default, pip installs into $HOME/.local ). To do this, you set the PYTHONUSERBASE environment variable to point to a location on /work, for example: export PYTHONUSERBASE=/work/t01/t01/auser/.local You will also need to ensure that the location of commands installed by pip are available on the command line by modifying the PATH environment variable. Once you have set PYTHONUSERBASE as described above, you can do this with the command: export PATH=$PYTHONUSERBASE/bin:$PATH We would recommend adding both of these commands to your $HOME/.bashrc file to ensure they are set by default when you log in to ARCHER2. Once, you have done this, you can use pip to add packages. This can be done using: pip install --user <package_name> This uses the --user flag to ensure the packages are installed in your user directory. We recommend that you use the pipenv and/or virtualenv packages to manage your Python environments. For information on how to do this see: Pipenv and Virtual Environments","title":"Adding your own packages"},{"location":"user-guide/python/#running-python-on-the-compute-nodes","text":"In this section we provide example Python job submission scripts for a variety of scenarios of using Python on the ARCHER2 compute nodes.","title":"Running Python on the compute nodes"},{"location":"user-guide/python/#example-serial-python-submission-script","text":"#!/bin/bash --login #SBATCH --name=python_test #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=00:10:00 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the batch environment module load epcc-job-env # Load the Python module module load cray-python # Run your Python progamme python python_test.py","title":"Example serial Python submission script"},{"location":"user-guide/python/#example-mpi4py-job-submission-script","text":"Programmes that have been parallelised with mpi4py can be run on multiple processors on ARCHER2. A sample submission script is given below. The primary difference from the Python submission script in the previous section is that we must run the programme using srun python my_prog.py instead of python my_prog,py . Failing to do so will cause a segmentation fault in your programme when it reaches the line from mpi4py import MPI . #!/bin/bash --login # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=mpi4py_test #SBATCH --nodes=1 #SBATCH --tasks-per-node=2 #SBATCH --cpus-per-task=1 #SBATCH --time=0:10:0 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the batch environment module load epcc-job-env # Load the Python module module load cray-python # Run your Python programme # Note that srun MUST be used to wrap the call to python, otherwise an error # will occur srun python mpi4py_test.py","title":"Example mpi4py job submission script"},{"location":"user-guide/scheduler/","text":"Running jobs on ARCHER2 Warning The ARCHER2 Service is not yet available. This documentation is in development. As with most HPC services, ARCHER2 uses a scheduler to manage access to resources and ensure that the thousands of different users of system are able to share the system and all get access to the resources they require. ARCHER2 uses the Slurm software to schedule jobs. Writing a submission script is typically the most convenient way to submit your job to the scheduler. Example submission scripts (with explanations) for the most common job types are provided below. Interactive jobs are also available and can be particularly useful for developing and debugging applications. More details are available below. Hint If you have any questions on how to run jobs on ARCHER2 do not hesitate to contact the ARCHER2 Service Desk . You typically interact with Slurm by issuing Slurm commands from the login nodes (to submit, check and cancel jobs), and by specifying Slurm directives that describe the resources required for your jobs in job submission scripts. Basic Slurm commands There are three key commands used to interact with the Slurm on the command line: sinfo - Get information on the partitions and resources available sbatch jobscript.slurm - Submit a job submission script (in this case called: jobscript.slurm ) to the scheduler squeue - Get the current status of jobs submitted to the scheduler scancel 12345 - Cancel a job (in this case with the job ID 12345 ) We cover each of these commands in more detail below. sinfo : information on resources sinfo is used to query information about available resources and partitions. Without any options, sinfo lists the status of all resources and partitions, e.g. sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST standard up 1-00:00:00 105 down* nid[001006,...,002014] standard up 1-00:00:00 12 drain nid[001016,...,001969] standard up 1-00:00:00 5 resv nid[001000,001002-001004,001114] standard up 1-00:00:00 683 alloc nid[001001,...,001970-001991] standard up 1-00:00:00 214 idle nid[001022-001023,...,002015-002023] standard up 1-00:00:00 2 down nid[001021,001050] Here we see the number of nodes in different states. For example, 683 nodes are allocated (running jobs), and 214 are idle (available to run jobs). !!! note that long lists of node IDs have been abbreviated with ... . sbatch : submitting jobs sbatch is used to submit a job script to the job submission system. The script will typically contain one or more srun commands to launch parallel tasks. When you submit the job, the scheduler provides the job ID, which is used to identify this job in other Slurm commands and when looking at resource usage in SAFE. sbatch test-job.slurm Submitted batch job 12345 squeue : monitoring jobs squeue without any options or arguments shows the current status of all jobs known to the scheduler. For example: squeue will list all jobs on ARCHER2. The output of this is often overwhelmingly large. You can restrict the output to just your jobs by adding the -u $USER option: squeue -u $USER scancel : deleting jobs scancel is used to delete a jobs from the scheduler. If the job is waiting to run it is simply cancelled, if it is a running job then it is stopped immediately. You need to provide the job ID of the job you wish to cancel/stop. For example: scancel 12345 will cancel (if waiting) or stop (if running) the job with ID 12345 . Resource Limits The ARCHER2 resource limits for any given job are covered by three separate attributes. The amount of primary resource you require, i.e., number of compute nodes. The partition that you want to use - this specifies the nodes that are eligible to run your job. The Quality of Service (QoS) that you want to use - this specifies the job limits that apply. Primary resource The primary resource you can request for your job is the compute node. Information The --exclusive option is enforced on ARCHER2 which means you will always have access to all of the memory on the compute node regardless of how many processes are actually running on the node. Note You will not generally have access to the full amount of memory resource on the the node as some is retained for running the operating system and other system processes. Partitions On ARCHER2, compute nodes are grouped into partitions. You will have to specify a partition using the --partition option in your Slurm submission script. The following table has a list of active partitions on ARCHER2. Partition Description Max nodes available standard CPU nodes with AMD EPYC 7742 64-core processor \u00d7 2 1024 ARCHER2 Partitions You can list the active partitions by running sinfo . Tip You may not have access to all the available partitions. Quality of Service (QoS) On ARCHER2, job limits are defined by the requested Quality of Service (QoS), as specified by the --qos Slurm directive. The following table lists the active QoS on ARCHER2. QoS Max Nodes Per Job Max Walltime Jobs Queued Jobs Running Partition(s) standard 940 24 hrs 64 16 standard short 8 20 mins 2 2 short long 16 48 hrs 16 16 standard Warning If you want to use the short QoS then you also need to add the --reservation=shortqos to your job submission command. Please note, there are two other limits not covered by the above table. The short QoS has restricted hours of service, 08:00-20:00 Mon-Fri. Long jobs must have a minimum walltime of 24 hrs. You can find out the QoS that you can use by running the following command: sacctmgr show assoc user=$USER cluster=archer2-es format=cluster,account,user,qos%50 Hint If you have needs which do not fit within the current QoS, please contact the Service Desk and we can discuss how to accommodate your requirements. Troubleshooting Slurm error messages Note More information on common error messages will be added when the ARCHER2 system is available. Slurm queued reasons Note Explanations of the reasons for jobs being queued and not running will be added when the ARCHER2 system is available. Output from Slurm jobs Slurm places standard output (STDOUT) and standard error (STDERR) for each job in the file slurm_<JobID>.out . This file appears in the job's working directory once your job starts running. Specifying resources in job scripts You specify the resources you require for your job using directives at the top of your job submission script using lines that start with the directive #SBATCH . Hint Options provided using #SBATCH directives can also be specified as command line options to srun . If you do not specify any options, then the default for each option will be applied. As a minimum, all job submissions must specify the budget that they wish to charge the job too with the option: --account=<budgetID> your budget ID is usually something like t01 or t01-test . You can see which budget codes you can charge to in SAFE. Other common options that are used are: --time=<hh:mm:ss> the maximum walltime for your job. e.g. For a 6.5 hour walltime, you would use --time=6:30:0 . --job-name=<jobname> set a name for the job to help identify it in In addition, parallel jobs will also need to specify how many nodes, parallel processes and threads they require. --nodes=<nodes> the number of nodes to use for the job. --tasks-per-node=<processes per node> the number of parallel processes (e.g. MPI ranks) per node. --cpus-per-task=1 if you are using parallel processes only with no threading then you should set the number of CPUs (cores) per parallel process to 1. !!! note: if you are using threading (e.g. with OpenMP) then you will need to change this option as described below. For parallel jobs that use threading (e.g. OpenMP), you will also need to change the --cpus-per-task option. --cpus-per-task=<threads per task> the number of threads per parallel process (e.g. number of OpenMP threads per MPI task for hybrid MPI/OpenMP jobs). !!! note: you must also set the OMP_NUM_THREADS environment variable if using OpenMP in your job. Note For parallel jobs, ARCHER2 operates in a node exclusive way. This means that you are assigned resources in the units of full compute nodes for your jobs ( i.e. 128 cores) and that no other user can share those compute nodes with you. Hence, the minimum amount of resource you can request for a parallel job is 1 node (or 128 cores). To prevent the behaviour of batch scripts being dependent on the user environment at the point of submission, the option --export=none prevents the user environment from being exported to the batch system. Using the --export=none means that the behaviour of batch submissions should be repeatable. We strongly recommend its use. Using modules in the batch system: the epcc-job-env module Batch jobs must be submitted in the work file system /work as the compute nodes do not have access to the /home file system. This has a knock-on effect on the behaviour of module collections, which the module system expects to find in a user's home directory. In order that the module system work correctly, batch scripts should contain module load epcc-job-env as the first module command in the script to ensure that the environment is set correctly for the job. This will also ensure all relevant library paths are set correctly at run time. Tip module -s can be used to suppress the associated messages if desired. srun : Launching parallel jobs If you are running parallel jobs, your job submission script should contain one or more srun commands to launch the parallel executable across the compute nodes. Warning To ensure that processes and threads are correctly mapped (or pinned ) to cores, you should always specify --cpu-bind=cores option to srun . bolt: Job submission script creation tool The bolt job submission script creation tool has been written by EPCC to simplify the process of writing job submission scripts for modern multicore architectures. Based on the options you supply, bolt will generate a job submission script that uses ARCHER2 in a reasonable way. MPI, OpenMP and hybrid MPI/OpenMP jobs are supported. Warning The tool will allow you to generate scripts for jobs that use the long QoS but you will need to manually modify the resulting script to change the QoS to long . If there are problems or errors in your job parameter specifications then bolt will print warnings or errors. However, bolt cannot detect all problems. Basic Usage The basic syntax for using bolt is: bolt -n [parallel tasks] -N [parallel tasks per node] -d [number of threads per task] \\ -t [wallclock time (h:m:s)] -o [script name] -j [job name] -A [project code] [arguments...] Example 1: to generate a job script to run an executable called my_prog.x for 24 hours using 8192 parallel (MPI) processes and 128 (MPI) processes per compute node you would use something like: bolt -n 8192 -N 128 -t 24:0:0 -o my_job.bolt -j my_job -A z01-budget my_prog.x arg1 arg2 (remember to substitute z01-budget for your actual budget code.) Example 2: to generate a job script to run an executable called my_prog.x for 3 hours using 2048 parallel (MPI) processes and 64 (MPI) processes per compute node (i.e. using half of the cores on a compute node), you would use: bolt -n 2048 -N 64 -t 3:0:0 -o my_job.bolt -j my_job -A z01-budget my_prog.x arg1 arg2 These examples generate the job script my_job.bolt with the correct options to run my_prog.x with command line arguments arg1 and arg2 . The project code against which the job will be charged is specified with the ' -A ' option. As usual, the job script is submitted as follows: sbatch my_job.bolt Hint If you do not specify the script name with the '-o' option then your script will be a file called a.bolt . Hint If you do not specify the number of parallel tasks then bolt will try to generate a serial job submission script (and throw an error on the ARCHER2 4 cabinet system as serial jobs are not supported). Hint If you do not specify a project code, bolt will use your default project code (set by your login account). Hint If you do not specify a job name, bolt will use either bolt_ser_job (for serial jobs) or bolt_par_job (for parallel jobs). Further help You can access further help on using bolt on ARCHER2 with the ' -h ' option: bolt -h A selection of other useful options are: -s Write and submit the job script rather than just writing the job script. -p Force the job to be parallel even if it only uses a single parallel task. checkScript job submission script validation tool The checkScript tool has been written to allow users to validate their job submission scripts before submitting their jobs. The tool will read your job submission script and try to identify errors, problems or inconsistencies. An example of the sort of output the tool can give would be: auser@uan01:/work/t01/t01/auser> checkScript submit.slurm =========================================================================== checkScript --------------------------------------------------------------------------- Copyright 2011-2020 EPCC, The University of Edinburgh This program comes with ABSOLUTELY NO WARRANTY. This is free software, and you are welcome to redistribute it under certain conditions. =========================================================================== Script details --------------- User: auser Script file: submit.slurm Directory: /work/t01/t01/auser (ok) Job name: test (ok) Partition: standard (ok) QoS: standard (ok) Combination: (ok) Requested resources ------------------- nodes = 3 (ok) tasks per node = 16 cpus per task = 8 cores per node = 128 (ok) OpenMP defined = True (ok) walltime = 1:0:0 (ok) CU Usage Estimate (if full job time used) ------------------------------------------ CU = 3.000 checkScript finished: 0 warning(s) and 0 error(s). Example job submission scripts A subset of example job submission scripts are included in full below. You can also download these examples at: Example: job submission script for MPI parallel job A simple MPI job submission script to submit a job using 4 compute nodes and 128 MPI ranks per node for 20 minutes would look like: #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=Example_MPI_Job #SBATCH --time=0:20:0 #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env # Set the number of threads to 1 # This prevents any threaded system libraries from automatically # using threading. export OMP_NUM_THREADS=1 # Launch the parallel job # Using 512 MPI processes and 128 MPI processes per node # srun picks up the distribution from the sbatch options srun --cpu-bind=cores ./my_mpi_executable.x This will run your executable \"my_mpi_executable.x\" in parallel on 512 MPI processes using 4 nodes (128 cores per node, i.e. not using hyper-threading). Slurm will allocate 4 nodes to your job and srun will place 128 MPI processes on each node (one per physical core). See above for a more detailed discussion of the different sbatch options Example: job submission script for MPI+OpenMP (mixed mode) parallel job Mixed mode codes that use both MPI (or another distributed memory parallel model) and OpenMP should take care to ensure that the shared memory portion of the process/thread placement does not span more than one NUMA region. Nodes on ARCHER2 are made up of two sockets each containing 4 NUMA regions of 16 cores, i.e. there are 8 NUMA regions in total. Therefore the total number of threads should ideally not be greater than 16, and also needs to be a factor of 16. Sensible choices for the number of threads are therefore 1 (single-threaded), 2, 4, 8, and 16. More information about using OpenMP and MPI+OpenMP can be found in the Tuning chapter. To ensure correct placement of MPI processes the number of cpus-per-task needs to match the number of OpenMP threads, and the number of tasks-per-node should be set to ensure the entire node is filled with MPI tasks. In the example below, we are using 4 nodes for 6 hours. There are 32 MPI processes in total (8 MPI processes per node) and 16 OpenMP threads per MPI process. This results in all 128 physical cores per node being used. Hint Note the use of the export OMP_PLACES=cores environment option and the --hint=nomultithread and --distribution=block:block options to srun to generate the correct pinning. #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=Example_MPI_Job #SBATCH --time=0:20:0 #SBATCH --nodes=4 #SBATCH --ntasks=32 #SBATCH --tasks-per-node=8 #SBATCH --cpus-per-task=16 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env # Set the number of threads to 16 and specify placement # There are 16 OpenMP threads per MPI process # We want one thread per physical core export OMP_NUM_THREADS=16 export OMP_PLACES=cores # Launch the parallel job # Using 32 MPI processes # 8 MPI processes per node # 16 OpenMP threads per MPI process # Additional srun options to pin one thread per physical core srun --hint=nomultithread --distribution=block:block ./my_mixed_executable.x arg1 arg2 Job arrays The Slurm job scheduling system offers the job array concept, for running collections of almost-identical jobs. For example, running the same program several times with different arguments or input data. Each job in a job array is called a subjob . The subjobs of a job array can be submitted and queried as a unit, making it easier and cleaner to handle the full set, compared to individual jobs. All subjobs in a job array are started by running the same job script. The job script also contains information on the number of jobs to be started, and Slurm provides a subjob index which can be passed to the individual subjobs or used to select the input data per subjob. Job script for a job array As an example, the following script runs 56 subjobs, with the subjob index as the only argument to the executable. Each subjob requests a single node and uses all 128 cores on the node by placing 1 MPI process per core and specifies 4 hours maximum runtime per subjob: #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=Example_Array_Job #SBATCH --time=04:00:00 #SBATCH --nodes=1 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --array=0-55 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env # Set the number of threads to 1 # This prevents any threaded system libraries from automatically # using threading. export OMP_NUM_THREADS=1 srun --cpu-bind=cores /path/to/exe $SLURM_ARRAY_TASK_ID Submitting a job array Job arrays are submitted using sbatch in the same way as for standard jobs: sbatch job_script.pbs Job chaining Job dependencies can be used to construct complex pipelines or chain together long simulations requiring multiple steps. Hint The --parsable option to sbatch can simplify working with job dependencies. It returns the job ID in a format that can be used as the input to other commands. For example: jobid=$(sbatch --parsable first_job.sh) sbatch --dependency=afterok:$jobid second_job.sh or for a longer chain: jobid1=$(sbatch --parsable first_job.sh) jobid2=$(sbatch --parsable --dependency=afterok:$jobid1 second_job.sh) jobid3=$(sbatch --parsable --dependency=afterok:$jobid1 third_job.sh) sbatch --dependency=afterok:$jobid2,afterok:$jobid3 last_job.sh Interactive Jobs: salloc When you are developing or debugging code you often want to run many short jobs with a small amount of editing the code between runs. This can be achieved by using the login nodes to run MPI but you may want to test on the compute nodes (e.g. you may want to test running on multiple nodes across the high performance interconnect). One of the best ways to achieve this on ARCHER2 is to use interactive jobs. An interactive job allows you to issue srun commands directly from the command line without using a job submission script, and to see the output from your program directly in the terminal. You use the salloc command to reserve compute nodes for interactive jobs. To submit a request for an interactive job reserving 8 nodes (1024 physical cores) for 1 hour you would issue the following qsub command from the command line: auser@uan01:> salloc --nodes=8 --tasks-per-node=128 --cpus-per-task=1 \\ --time=01:00:00 --partition=standard --qos=standard \\ --account=[budget code] When you submit this job your terminal will display something like: salloc: Granted job allocation 24236 salloc: Waiting for resource configuration salloc: Nodes nid000002 are ready for job auser@uan01:> It may take some time for your interactive job to start. Once it runs you will enter a standard interactive terminal session (a new shell). Note that this shell is still on the front end (the prompt has not change). Whilst the interactive session lasts you will be able to run parallel jobs on the compute nodes by issuing the srun --cpu-bind=cores command directly at your command prompt using the same syntax as you would inside a job script. The maximum number of nodes you can use is limited by resources requested in the salloc command. If you know you will be doing a lot of intensive debugging you may find it useful to request an interactive session lasting the expected length of your working session, say a full day. Your session will end when you hit the requested walltime. If you wish to finish before this you should use the exit command - this will return you to your prompt before you issued the salloc command. Using srun directly A second way to run an interactive job is to use srun directly in the following way: auser@uan01:/work/t01/t01/auser> srun --nodes=1 --exclusive --time=00:20:00 --account=[] \\ --partition=standard --qos=standard --pty /bin/bash auser@uan01:/work/t01/t01/auser> hostname nid001261 The --pty /bin/bash will cause a new shell to be started on the first node of a new allocation (note that while the shell prompt has not changed, we are now on the compute node). This is perhaps closer to what many people consider an 'interactive' job than the method using salloc appears. One can now issue shell commands in the usual way. A further invocation of srun is required to launch a parallel job in the allocation. When finished, type exit to relinquish the allocation and control will be returned to the front end. By default, the interactive shell will retain the environment of the parent. If you want a clean shell, remember to specify --export=none . Reservations The mechanism for submitting reservations on ARCHER2 has yet to be specified. Best practices for job submission This guidance is adapted from the advice provided by NERSC Time Limits Due to backfill scheduling, short and variable-length jobs generally start quickly resulting in much better job throughput. You can specify a minimum time for your job with the --time-min option to SBATCH: #SBATCH --time-min=<lower_bound> #SBATCH --time=<upper_bound> Within your job script, you can get the time remaining in the job with squeue -h -j ${Slurm_JOBID} -o %L to allow you to deal with potentially varying runtimes when using this option. Long Running Jobs Simulations which must run for a long period of time achieve the best throughput when composed of many small jobs using a checkpoint and restart method chained together (see above for how to chain jobs together). However, this method does occur a startup and shutdown overhead for each job as the state is saved and loaded so you should experiment to find the best balance between runtime (long runtimes minimise the checkpoint/restart overheads) and throughput (short runtimes maximise throughput). I/O performance Large Jobs Large jobs may take longer to start up. The sbcast command is recommended for large jobs requesting over 1500 MPI tasks. By default, Slurm reads the executable on the allocated compute nodes from the location where it is installed; this may take long time when the file system (where the executable resides) is slow or busy. The sbcast command, the executable can be copied to the /tmp directory on each of the compute nodes. Since /tmp is part of the memory on the compute nodes, it can speed up the job startup time. sbcast --compress=lz4 /path/to/exe /tmp/exe srun /tmp/exe Process Placement Several mechanisms exist to control process placement on ARCHER2. Application performance can depend strongly on placement depending on the communication pattern and other computational characteristics. Default The default is to place MPI tasks sequentially on nodes until the maximum number of tasks is reached: salloc --nodes=8 --tasks-per-node=2 --cpus-per-task=1 --time=0:10:0 \\ --account=[account code] --partition=partition code] --qos=standard salloc: Granted job allocation 24236 salloc: Waiting for resource configuration salloc: Nodes cn13 are ready for job module load xthi export OMP_NUM_THREADS=1 srun --cpu-bind=cores xthi Hello from rank 0, thread 0, on nid000001. (core affinity = 0,128) Hello from rank 1, thread 0, on nid000001. (core affinity = 16,144) Hello from rank 2, thread 0, on nid000002. (core affinity = 0,128) Hello from rank 3, thread 0, on nid000002. (core affinity = 16,144) Hello from rank 4, thread 0, on nid000003. (core affinity = 0,128) Hello from rank 5, thread 0, on nid000003. (core affinity = 16,144) Hello from rank 6, thread 0, on nid000004. (core affinity = 0,128) Hello from rank 7, thread 0, on nid000004. (core affinity = 16,144) Hello from rank 8, thread 0, on nid000005. (core affinity = 0,128) Hello from rank 9, thread 0, on nid000005. (core affinity = 16,144) Hello from rank 10, thread 0, on nid000006. (core affinity = 0,128) Hello from rank 11, thread 0, on nid000006. (core affinity = 16,144) Hello from rank 12, thread 0, on nid000007. (core affinity = 0,128) Hello from rank 13, thread 0, on nid000007. (core affinity = 16,144) Hello from rank 14, thread 0, on nid000008. (core affinity = 0,128) Hello from rank 15, thread 0, on nid000008. (core affinity = 16,144) MPICH_RANK_REORDER_METHOD The MPICH_RANK_REORDER_METHOD environment variable is used to specify other types of MPI task placement. For example, setting it to 0 results in a round-robin placement: salloc --nodes=8 --tasks-per-node=2 --cpus-per-task=1 --time=0:10:0 --account=t01 salloc: Granted job allocation 24236 salloc: Waiting for resource configuration salloc: Nodes cn13 are ready for job module load xthi export OMP_NUM_THREADS=1 export MPICH_RANK_REORDER_METHOD=0 srun --cpu-bind=cores xthi Hello from rank 0, thread 0, on nid000001. (core affinity = 0,128) Hello from rank 1, thread 0, on nid000002. (core affinity = 0,128) Hello from rank 2, thread 0, on nid000003. (core affinity = 0,128) Hello from rank 3, thread 0, on nid000004. (core affinity = 0,128) Hello from rank 4, thread 0, on nid000005. (core affinity = 0,128) Hello from rank 5, thread 0, on nid000006. (core affinity = 0,128) Hello from rank 6, thread 0, on nid000007. (core affinity = 0,128) Hello from rank 7, thread 0, on nid000008. (core affinity = 0,128) Hello from rank 8, thread 0, on nid000001. (core affinity = 16,144) Hello from rank 9, thread 0, on nid000002. (core affinity = 16,144) Hello from rank 10, thread 0, on nid000003. (core affinity = 16,144) Hello from rank 11, thread 0, on nid000004. (core affinity = 16,144) Hello from rank 12, thread 0, on nid000005. (core affinity = 16,144) Hello from rank 13, thread 0, on nid000006. (core affinity = 16,144) Hello from rank 14, thread 0, on nid000007. (core affinity = 16,144) Hello from rank 15, thread 0, on nid000008. (core affinity = 16,144) There are other modes available with the MPICH_RANK_REORDER_METHOD environment variable, including one which lets the user provide a file called MPICH_RANK_ORDER which contains a list of each task's placement on each node. These options are described in detail in the intro_mpi man page. grid_order For MPI applications which perform a large amount of nearest-neighbor communication, e.g., stencil-based applications on structured grids, Cray provides a tool in the perftools-base module called grid_order which can generate a MPICH_RANK_ORDER file automatically by taking as parameters the dimensions of the grid, core count, etc. For example, to place MPI tasks in row-major order on a Cartesian grid of size $(4, 4, 4)$, using 32 tasks per node: module load perftools-base grid_order -R -c 32 -g 4,4,4 # grid_order -R -Z -c 32 -g 4,4,4 # Region 3: 0,0,1 (0..63) 0,1,2,3,16,17,18,19,32,33,34,35,48,49,50,51,4,5,6,7,20,21,22,23,36,37,38,39,52,53,54,55 8,9,10,11,24,25,26,27,40,41,42,43,56,57,58,59,12,13,14,15,28,29,30,31,44,45,46,47,60,61,62,63 One can then save this output to a file called MPICH_RANK_ORDER and then set MPICH_RANK_REORDER_METHOD=3 before running the job, which tells Cray MPI to read the MPICH_RANK_ORDER file to set the MPI task placement. For more information, please see the man page man grid_order (available when the perftools-base module is loaded). Huge pages Huge pages are virtual memory pages which are bigger than the default page size of 4K bytes. Huge pages can improve memory performance for common access patterns on large data sets since it helps to reduce the number of virtual to physical address translations when compared to using the default 4KB. To use huge pages for an application (with the 2 MB huge pages as an example): module load craype-hugepages2M cc -o mycode.exe mycode.c And also load the same huge pages module at runtime. Warning Due to the huge pages memory fragmentation issue, applications may get Cannot allocate memory warnings or errors when there are not enough hugepages on the compute node, such as: libhugetlbfs [nid0000xx:xxxxx]: WARNING: New heap segment map at 0x10000000 failed: Cannot allocate memory`` By default, The verbosity level of libhugetlbfs HUGETLB_VERBOSE is set to 0 on ARCHER2 to surpress debugging messages. Users can adjust this value to obtain more information on huge pages use. When to Use Huge Pages For MPI applications, map the static data and/or heap onto huge pages. For an application which uses shared memory, which needs to be concurrently registered with the high speed network drivers for remote communication. For SHMEM applications, map the static data and/or private heap onto huge pages. For applications written in Unified Parallel C, Coarray Fortran, and other languages based on the PGAS programming model, map the static data and/or private heap onto huge pages. For an application doing heavy I/O. To improve memory performance for common access patterns on large data sets. When to Avoid Huge Pages Applications sometimes consist of many steering programs in addition to the core application. Applying huge page behavior to all processes would not provide any benefit and would consume huge pages that would otherwise benefit the core application. The runtime environment variable HUGETLB_RESTRICT_EXE can be used to specify the susbset of the programs to use hugepages. For certain applications if using hugepages either causes issues or slows down performance. One such example is that when an application forks more subprocesses (such as pthreads) and these threads allocate memory, the newly allocated memory are the default 4 KB pages.","title":"Running jobs on ARCHER2"},{"location":"user-guide/scheduler/#running-jobs-on-archer2","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development. As with most HPC services, ARCHER2 uses a scheduler to manage access to resources and ensure that the thousands of different users of system are able to share the system and all get access to the resources they require. ARCHER2 uses the Slurm software to schedule jobs. Writing a submission script is typically the most convenient way to submit your job to the scheduler. Example submission scripts (with explanations) for the most common job types are provided below. Interactive jobs are also available and can be particularly useful for developing and debugging applications. More details are available below. Hint If you have any questions on how to run jobs on ARCHER2 do not hesitate to contact the ARCHER2 Service Desk . You typically interact with Slurm by issuing Slurm commands from the login nodes (to submit, check and cancel jobs), and by specifying Slurm directives that describe the resources required for your jobs in job submission scripts.","title":"Running jobs on ARCHER2"},{"location":"user-guide/scheduler/#basic-slurm-commands","text":"There are three key commands used to interact with the Slurm on the command line: sinfo - Get information on the partitions and resources available sbatch jobscript.slurm - Submit a job submission script (in this case called: jobscript.slurm ) to the scheduler squeue - Get the current status of jobs submitted to the scheduler scancel 12345 - Cancel a job (in this case with the job ID 12345 ) We cover each of these commands in more detail below.","title":"Basic Slurm commands"},{"location":"user-guide/scheduler/#sinfo-information-on-resources","text":"sinfo is used to query information about available resources and partitions. Without any options, sinfo lists the status of all resources and partitions, e.g. sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST standard up 1-00:00:00 105 down* nid[001006,...,002014] standard up 1-00:00:00 12 drain nid[001016,...,001969] standard up 1-00:00:00 5 resv nid[001000,001002-001004,001114] standard up 1-00:00:00 683 alloc nid[001001,...,001970-001991] standard up 1-00:00:00 214 idle nid[001022-001023,...,002015-002023] standard up 1-00:00:00 2 down nid[001021,001050] Here we see the number of nodes in different states. For example, 683 nodes are allocated (running jobs), and 214 are idle (available to run jobs). !!! note that long lists of node IDs have been abbreviated with ... .","title":"sinfo: information on resources"},{"location":"user-guide/scheduler/#sbatch-submitting-jobs","text":"sbatch is used to submit a job script to the job submission system. The script will typically contain one or more srun commands to launch parallel tasks. When you submit the job, the scheduler provides the job ID, which is used to identify this job in other Slurm commands and when looking at resource usage in SAFE. sbatch test-job.slurm Submitted batch job 12345","title":"sbatch: submitting jobs"},{"location":"user-guide/scheduler/#squeue-monitoring-jobs","text":"squeue without any options or arguments shows the current status of all jobs known to the scheduler. For example: squeue will list all jobs on ARCHER2. The output of this is often overwhelmingly large. You can restrict the output to just your jobs by adding the -u $USER option: squeue -u $USER","title":"squeue: monitoring jobs"},{"location":"user-guide/scheduler/#scancel-deleting-jobs","text":"scancel is used to delete a jobs from the scheduler. If the job is waiting to run it is simply cancelled, if it is a running job then it is stopped immediately. You need to provide the job ID of the job you wish to cancel/stop. For example: scancel 12345 will cancel (if waiting) or stop (if running) the job with ID 12345 .","title":"scancel: deleting jobs"},{"location":"user-guide/scheduler/#resource-limits","text":"The ARCHER2 resource limits for any given job are covered by three separate attributes. The amount of primary resource you require, i.e., number of compute nodes. The partition that you want to use - this specifies the nodes that are eligible to run your job. The Quality of Service (QoS) that you want to use - this specifies the job limits that apply.","title":"Resource Limits"},{"location":"user-guide/scheduler/#primary-resource","text":"The primary resource you can request for your job is the compute node. Information The --exclusive option is enforced on ARCHER2 which means you will always have access to all of the memory on the compute node regardless of how many processes are actually running on the node. Note You will not generally have access to the full amount of memory resource on the the node as some is retained for running the operating system and other system processes.","title":"Primary resource"},{"location":"user-guide/scheduler/#partitions","text":"On ARCHER2, compute nodes are grouped into partitions. You will have to specify a partition using the --partition option in your Slurm submission script. The following table has a list of active partitions on ARCHER2. Partition Description Max nodes available standard CPU nodes with AMD EPYC 7742 64-core processor \u00d7 2 1024 ARCHER2 Partitions You can list the active partitions by running sinfo . Tip You may not have access to all the available partitions.","title":"Partitions"},{"location":"user-guide/scheduler/#quality-of-service-qos","text":"On ARCHER2, job limits are defined by the requested Quality of Service (QoS), as specified by the --qos Slurm directive. The following table lists the active QoS on ARCHER2. QoS Max Nodes Per Job Max Walltime Jobs Queued Jobs Running Partition(s) standard 940 24 hrs 64 16 standard short 8 20 mins 2 2 short long 16 48 hrs 16 16 standard Warning If you want to use the short QoS then you also need to add the --reservation=shortqos to your job submission command. Please note, there are two other limits not covered by the above table. The short QoS has restricted hours of service, 08:00-20:00 Mon-Fri. Long jobs must have a minimum walltime of 24 hrs. You can find out the QoS that you can use by running the following command: sacctmgr show assoc user=$USER cluster=archer2-es format=cluster,account,user,qos%50 Hint If you have needs which do not fit within the current QoS, please contact the Service Desk and we can discuss how to accommodate your requirements.","title":"Quality of Service (QoS)"},{"location":"user-guide/scheduler/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"user-guide/scheduler/#slurm-error-messages","text":"Note More information on common error messages will be added when the ARCHER2 system is available.","title":"Slurm error messages"},{"location":"user-guide/scheduler/#slurm-queued-reasons","text":"Note Explanations of the reasons for jobs being queued and not running will be added when the ARCHER2 system is available.","title":"Slurm queued reasons"},{"location":"user-guide/scheduler/#output-from-slurm-jobs","text":"Slurm places standard output (STDOUT) and standard error (STDERR) for each job in the file slurm_<JobID>.out . This file appears in the job's working directory once your job starts running.","title":"Output from Slurm jobs"},{"location":"user-guide/scheduler/#specifying-resources-in-job-scripts","text":"You specify the resources you require for your job using directives at the top of your job submission script using lines that start with the directive #SBATCH . Hint Options provided using #SBATCH directives can also be specified as command line options to srun . If you do not specify any options, then the default for each option will be applied. As a minimum, all job submissions must specify the budget that they wish to charge the job too with the option: --account=<budgetID> your budget ID is usually something like t01 or t01-test . You can see which budget codes you can charge to in SAFE. Other common options that are used are: --time=<hh:mm:ss> the maximum walltime for your job. e.g. For a 6.5 hour walltime, you would use --time=6:30:0 . --job-name=<jobname> set a name for the job to help identify it in In addition, parallel jobs will also need to specify how many nodes, parallel processes and threads they require. --nodes=<nodes> the number of nodes to use for the job. --tasks-per-node=<processes per node> the number of parallel processes (e.g. MPI ranks) per node. --cpus-per-task=1 if you are using parallel processes only with no threading then you should set the number of CPUs (cores) per parallel process to 1. !!! note: if you are using threading (e.g. with OpenMP) then you will need to change this option as described below. For parallel jobs that use threading (e.g. OpenMP), you will also need to change the --cpus-per-task option. --cpus-per-task=<threads per task> the number of threads per parallel process (e.g. number of OpenMP threads per MPI task for hybrid MPI/OpenMP jobs). !!! note: you must also set the OMP_NUM_THREADS environment variable if using OpenMP in your job. Note For parallel jobs, ARCHER2 operates in a node exclusive way. This means that you are assigned resources in the units of full compute nodes for your jobs ( i.e. 128 cores) and that no other user can share those compute nodes with you. Hence, the minimum amount of resource you can request for a parallel job is 1 node (or 128 cores). To prevent the behaviour of batch scripts being dependent on the user environment at the point of submission, the option --export=none prevents the user environment from being exported to the batch system. Using the --export=none means that the behaviour of batch submissions should be repeatable. We strongly recommend its use.","title":"Specifying resources in job scripts"},{"location":"user-guide/scheduler/#using-modules-in-the-batch-system-the-epcc-job-env-module","text":"Batch jobs must be submitted in the work file system /work as the compute nodes do not have access to the /home file system. This has a knock-on effect on the behaviour of module collections, which the module system expects to find in a user's home directory. In order that the module system work correctly, batch scripts should contain module load epcc-job-env as the first module command in the script to ensure that the environment is set correctly for the job. This will also ensure all relevant library paths are set correctly at run time. Tip module -s can be used to suppress the associated messages if desired.","title":"Using modules in the batch system: the epcc-job-env module"},{"location":"user-guide/scheduler/#srun-launching-parallel-jobs","text":"If you are running parallel jobs, your job submission script should contain one or more srun commands to launch the parallel executable across the compute nodes. Warning To ensure that processes and threads are correctly mapped (or pinned ) to cores, you should always specify --cpu-bind=cores option to srun .","title":"srun: Launching parallel jobs"},{"location":"user-guide/scheduler/#bolt-job-submission-script-creation-tool","text":"The bolt job submission script creation tool has been written by EPCC to simplify the process of writing job submission scripts for modern multicore architectures. Based on the options you supply, bolt will generate a job submission script that uses ARCHER2 in a reasonable way. MPI, OpenMP and hybrid MPI/OpenMP jobs are supported. Warning The tool will allow you to generate scripts for jobs that use the long QoS but you will need to manually modify the resulting script to change the QoS to long . If there are problems or errors in your job parameter specifications then bolt will print warnings or errors. However, bolt cannot detect all problems.","title":"bolt: Job submission script creation tool"},{"location":"user-guide/scheduler/#basic-usage","text":"The basic syntax for using bolt is: bolt -n [parallel tasks] -N [parallel tasks per node] -d [number of threads per task] \\ -t [wallclock time (h:m:s)] -o [script name] -j [job name] -A [project code] [arguments...] Example 1: to generate a job script to run an executable called my_prog.x for 24 hours using 8192 parallel (MPI) processes and 128 (MPI) processes per compute node you would use something like: bolt -n 8192 -N 128 -t 24:0:0 -o my_job.bolt -j my_job -A z01-budget my_prog.x arg1 arg2 (remember to substitute z01-budget for your actual budget code.) Example 2: to generate a job script to run an executable called my_prog.x for 3 hours using 2048 parallel (MPI) processes and 64 (MPI) processes per compute node (i.e. using half of the cores on a compute node), you would use: bolt -n 2048 -N 64 -t 3:0:0 -o my_job.bolt -j my_job -A z01-budget my_prog.x arg1 arg2 These examples generate the job script my_job.bolt with the correct options to run my_prog.x with command line arguments arg1 and arg2 . The project code against which the job will be charged is specified with the ' -A ' option. As usual, the job script is submitted as follows: sbatch my_job.bolt Hint If you do not specify the script name with the '-o' option then your script will be a file called a.bolt . Hint If you do not specify the number of parallel tasks then bolt will try to generate a serial job submission script (and throw an error on the ARCHER2 4 cabinet system as serial jobs are not supported). Hint If you do not specify a project code, bolt will use your default project code (set by your login account). Hint If you do not specify a job name, bolt will use either bolt_ser_job (for serial jobs) or bolt_par_job (for parallel jobs).","title":"Basic Usage"},{"location":"user-guide/scheduler/#further-help","text":"You can access further help on using bolt on ARCHER2 with the ' -h ' option: bolt -h A selection of other useful options are: -s Write and submit the job script rather than just writing the job script. -p Force the job to be parallel even if it only uses a single parallel task.","title":"Further help"},{"location":"user-guide/scheduler/#checkscript-job-submission-script-validation-tool","text":"The checkScript tool has been written to allow users to validate their job submission scripts before submitting their jobs. The tool will read your job submission script and try to identify errors, problems or inconsistencies. An example of the sort of output the tool can give would be: auser@uan01:/work/t01/t01/auser> checkScript submit.slurm =========================================================================== checkScript --------------------------------------------------------------------------- Copyright 2011-2020 EPCC, The University of Edinburgh This program comes with ABSOLUTELY NO WARRANTY. This is free software, and you are welcome to redistribute it under certain conditions. =========================================================================== Script details --------------- User: auser Script file: submit.slurm Directory: /work/t01/t01/auser (ok) Job name: test (ok) Partition: standard (ok) QoS: standard (ok) Combination: (ok) Requested resources ------------------- nodes = 3 (ok) tasks per node = 16 cpus per task = 8 cores per node = 128 (ok) OpenMP defined = True (ok) walltime = 1:0:0 (ok) CU Usage Estimate (if full job time used) ------------------------------------------ CU = 3.000 checkScript finished: 0 warning(s) and 0 error(s).","title":"checkScript job submission script validation tool"},{"location":"user-guide/scheduler/#example-job-submission-scripts","text":"A subset of example job submission scripts are included in full below. You can also download these examples at:","title":"Example job submission scripts"},{"location":"user-guide/scheduler/#example-job-submission-script-for-mpi-parallel-job","text":"A simple MPI job submission script to submit a job using 4 compute nodes and 128 MPI ranks per node for 20 minutes would look like: #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=Example_MPI_Job #SBATCH --time=0:20:0 #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env # Set the number of threads to 1 # This prevents any threaded system libraries from automatically # using threading. export OMP_NUM_THREADS=1 # Launch the parallel job # Using 512 MPI processes and 128 MPI processes per node # srun picks up the distribution from the sbatch options srun --cpu-bind=cores ./my_mpi_executable.x This will run your executable \"my_mpi_executable.x\" in parallel on 512 MPI processes using 4 nodes (128 cores per node, i.e. not using hyper-threading). Slurm will allocate 4 nodes to your job and srun will place 128 MPI processes on each node (one per physical core). See above for a more detailed discussion of the different sbatch options","title":"Example: job submission script for MPI parallel job"},{"location":"user-guide/scheduler/#example-job-submission-script-for-mpiopenmp-mixed-mode-parallel-job","text":"Mixed mode codes that use both MPI (or another distributed memory parallel model) and OpenMP should take care to ensure that the shared memory portion of the process/thread placement does not span more than one NUMA region. Nodes on ARCHER2 are made up of two sockets each containing 4 NUMA regions of 16 cores, i.e. there are 8 NUMA regions in total. Therefore the total number of threads should ideally not be greater than 16, and also needs to be a factor of 16. Sensible choices for the number of threads are therefore 1 (single-threaded), 2, 4, 8, and 16. More information about using OpenMP and MPI+OpenMP can be found in the Tuning chapter. To ensure correct placement of MPI processes the number of cpus-per-task needs to match the number of OpenMP threads, and the number of tasks-per-node should be set to ensure the entire node is filled with MPI tasks. In the example below, we are using 4 nodes for 6 hours. There are 32 MPI processes in total (8 MPI processes per node) and 16 OpenMP threads per MPI process. This results in all 128 physical cores per node being used. Hint Note the use of the export OMP_PLACES=cores environment option and the --hint=nomultithread and --distribution=block:block options to srun to generate the correct pinning. #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=Example_MPI_Job #SBATCH --time=0:20:0 #SBATCH --nodes=4 #SBATCH --ntasks=32 #SBATCH --tasks-per-node=8 #SBATCH --cpus-per-task=16 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env # Set the number of threads to 16 and specify placement # There are 16 OpenMP threads per MPI process # We want one thread per physical core export OMP_NUM_THREADS=16 export OMP_PLACES=cores # Launch the parallel job # Using 32 MPI processes # 8 MPI processes per node # 16 OpenMP threads per MPI process # Additional srun options to pin one thread per physical core srun --hint=nomultithread --distribution=block:block ./my_mixed_executable.x arg1 arg2","title":"Example: job submission script for MPI+OpenMP (mixed mode) parallel job"},{"location":"user-guide/scheduler/#job-arrays","text":"The Slurm job scheduling system offers the job array concept, for running collections of almost-identical jobs. For example, running the same program several times with different arguments or input data. Each job in a job array is called a subjob . The subjobs of a job array can be submitted and queried as a unit, making it easier and cleaner to handle the full set, compared to individual jobs. All subjobs in a job array are started by running the same job script. The job script also contains information on the number of jobs to be started, and Slurm provides a subjob index which can be passed to the individual subjobs or used to select the input data per subjob.","title":"Job arrays"},{"location":"user-guide/scheduler/#job-script-for-a-job-array","text":"As an example, the following script runs 56 subjobs, with the subjob index as the only argument to the executable. Each subjob requests a single node and uses all 128 cores on the node by placing 1 MPI process per core and specifies 4 hours maximum runtime per subjob: #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=Example_Array_Job #SBATCH --time=04:00:00 #SBATCH --nodes=1 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --array=0-55 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env # Set the number of threads to 1 # This prevents any threaded system libraries from automatically # using threading. export OMP_NUM_THREADS=1 srun --cpu-bind=cores /path/to/exe $SLURM_ARRAY_TASK_ID","title":"Job script for a job array"},{"location":"user-guide/scheduler/#submitting-a-job-array","text":"Job arrays are submitted using sbatch in the same way as for standard jobs: sbatch job_script.pbs","title":"Submitting a job array"},{"location":"user-guide/scheduler/#job-chaining","text":"Job dependencies can be used to construct complex pipelines or chain together long simulations requiring multiple steps. Hint The --parsable option to sbatch can simplify working with job dependencies. It returns the job ID in a format that can be used as the input to other commands. For example: jobid=$(sbatch --parsable first_job.sh) sbatch --dependency=afterok:$jobid second_job.sh or for a longer chain: jobid1=$(sbatch --parsable first_job.sh) jobid2=$(sbatch --parsable --dependency=afterok:$jobid1 second_job.sh) jobid3=$(sbatch --parsable --dependency=afterok:$jobid1 third_job.sh) sbatch --dependency=afterok:$jobid2,afterok:$jobid3 last_job.sh","title":"Job chaining"},{"location":"user-guide/scheduler/#interactive-jobs-salloc","text":"When you are developing or debugging code you often want to run many short jobs with a small amount of editing the code between runs. This can be achieved by using the login nodes to run MPI but you may want to test on the compute nodes (e.g. you may want to test running on multiple nodes across the high performance interconnect). One of the best ways to achieve this on ARCHER2 is to use interactive jobs. An interactive job allows you to issue srun commands directly from the command line without using a job submission script, and to see the output from your program directly in the terminal. You use the salloc command to reserve compute nodes for interactive jobs. To submit a request for an interactive job reserving 8 nodes (1024 physical cores) for 1 hour you would issue the following qsub command from the command line: auser@uan01:> salloc --nodes=8 --tasks-per-node=128 --cpus-per-task=1 \\ --time=01:00:00 --partition=standard --qos=standard \\ --account=[budget code] When you submit this job your terminal will display something like: salloc: Granted job allocation 24236 salloc: Waiting for resource configuration salloc: Nodes nid000002 are ready for job auser@uan01:> It may take some time for your interactive job to start. Once it runs you will enter a standard interactive terminal session (a new shell). Note that this shell is still on the front end (the prompt has not change). Whilst the interactive session lasts you will be able to run parallel jobs on the compute nodes by issuing the srun --cpu-bind=cores command directly at your command prompt using the same syntax as you would inside a job script. The maximum number of nodes you can use is limited by resources requested in the salloc command. If you know you will be doing a lot of intensive debugging you may find it useful to request an interactive session lasting the expected length of your working session, say a full day. Your session will end when you hit the requested walltime. If you wish to finish before this you should use the exit command - this will return you to your prompt before you issued the salloc command.","title":"Interactive Jobs: salloc"},{"location":"user-guide/scheduler/#using-srun-directly","text":"A second way to run an interactive job is to use srun directly in the following way: auser@uan01:/work/t01/t01/auser> srun --nodes=1 --exclusive --time=00:20:00 --account=[] \\ --partition=standard --qos=standard --pty /bin/bash auser@uan01:/work/t01/t01/auser> hostname nid001261 The --pty /bin/bash will cause a new shell to be started on the first node of a new allocation (note that while the shell prompt has not changed, we are now on the compute node). This is perhaps closer to what many people consider an 'interactive' job than the method using salloc appears. One can now issue shell commands in the usual way. A further invocation of srun is required to launch a parallel job in the allocation. When finished, type exit to relinquish the allocation and control will be returned to the front end. By default, the interactive shell will retain the environment of the parent. If you want a clean shell, remember to specify --export=none .","title":"Using srun directly"},{"location":"user-guide/scheduler/#reservations","text":"The mechanism for submitting reservations on ARCHER2 has yet to be specified.","title":"Reservations"},{"location":"user-guide/scheduler/#best-practices-for-job-submission","text":"This guidance is adapted from the advice provided by NERSC","title":"Best practices for job submission"},{"location":"user-guide/scheduler/#time-limits","text":"Due to backfill scheduling, short and variable-length jobs generally start quickly resulting in much better job throughput. You can specify a minimum time for your job with the --time-min option to SBATCH: #SBATCH --time-min=<lower_bound> #SBATCH --time=<upper_bound> Within your job script, you can get the time remaining in the job with squeue -h -j ${Slurm_JOBID} -o %L to allow you to deal with potentially varying runtimes when using this option.","title":"Time Limits"},{"location":"user-guide/scheduler/#long-running-jobs","text":"Simulations which must run for a long period of time achieve the best throughput when composed of many small jobs using a checkpoint and restart method chained together (see above for how to chain jobs together). However, this method does occur a startup and shutdown overhead for each job as the state is saved and loaded so you should experiment to find the best balance between runtime (long runtimes minimise the checkpoint/restart overheads) and throughput (short runtimes maximise throughput).","title":"Long Running Jobs"},{"location":"user-guide/scheduler/#io-performance","text":"","title":"I/O performance"},{"location":"user-guide/scheduler/#large-jobs","text":"Large jobs may take longer to start up. The sbcast command is recommended for large jobs requesting over 1500 MPI tasks. By default, Slurm reads the executable on the allocated compute nodes from the location where it is installed; this may take long time when the file system (where the executable resides) is slow or busy. The sbcast command, the executable can be copied to the /tmp directory on each of the compute nodes. Since /tmp is part of the memory on the compute nodes, it can speed up the job startup time. sbcast --compress=lz4 /path/to/exe /tmp/exe srun /tmp/exe","title":"Large Jobs"},{"location":"user-guide/scheduler/#process-placement","text":"Several mechanisms exist to control process placement on ARCHER2. Application performance can depend strongly on placement depending on the communication pattern and other computational characteristics.","title":"Process Placement"},{"location":"user-guide/scheduler/#default","text":"The default is to place MPI tasks sequentially on nodes until the maximum number of tasks is reached: salloc --nodes=8 --tasks-per-node=2 --cpus-per-task=1 --time=0:10:0 \\ --account=[account code] --partition=partition code] --qos=standard salloc: Granted job allocation 24236 salloc: Waiting for resource configuration salloc: Nodes cn13 are ready for job module load xthi export OMP_NUM_THREADS=1 srun --cpu-bind=cores xthi Hello from rank 0, thread 0, on nid000001. (core affinity = 0,128) Hello from rank 1, thread 0, on nid000001. (core affinity = 16,144) Hello from rank 2, thread 0, on nid000002. (core affinity = 0,128) Hello from rank 3, thread 0, on nid000002. (core affinity = 16,144) Hello from rank 4, thread 0, on nid000003. (core affinity = 0,128) Hello from rank 5, thread 0, on nid000003. (core affinity = 16,144) Hello from rank 6, thread 0, on nid000004. (core affinity = 0,128) Hello from rank 7, thread 0, on nid000004. (core affinity = 16,144) Hello from rank 8, thread 0, on nid000005. (core affinity = 0,128) Hello from rank 9, thread 0, on nid000005. (core affinity = 16,144) Hello from rank 10, thread 0, on nid000006. (core affinity = 0,128) Hello from rank 11, thread 0, on nid000006. (core affinity = 16,144) Hello from rank 12, thread 0, on nid000007. (core affinity = 0,128) Hello from rank 13, thread 0, on nid000007. (core affinity = 16,144) Hello from rank 14, thread 0, on nid000008. (core affinity = 0,128) Hello from rank 15, thread 0, on nid000008. (core affinity = 16,144)","title":"Default"},{"location":"user-guide/scheduler/#mpich_rank_reorder_method","text":"The MPICH_RANK_REORDER_METHOD environment variable is used to specify other types of MPI task placement. For example, setting it to 0 results in a round-robin placement: salloc --nodes=8 --tasks-per-node=2 --cpus-per-task=1 --time=0:10:0 --account=t01 salloc: Granted job allocation 24236 salloc: Waiting for resource configuration salloc: Nodes cn13 are ready for job module load xthi export OMP_NUM_THREADS=1 export MPICH_RANK_REORDER_METHOD=0 srun --cpu-bind=cores xthi Hello from rank 0, thread 0, on nid000001. (core affinity = 0,128) Hello from rank 1, thread 0, on nid000002. (core affinity = 0,128) Hello from rank 2, thread 0, on nid000003. (core affinity = 0,128) Hello from rank 3, thread 0, on nid000004. (core affinity = 0,128) Hello from rank 4, thread 0, on nid000005. (core affinity = 0,128) Hello from rank 5, thread 0, on nid000006. (core affinity = 0,128) Hello from rank 6, thread 0, on nid000007. (core affinity = 0,128) Hello from rank 7, thread 0, on nid000008. (core affinity = 0,128) Hello from rank 8, thread 0, on nid000001. (core affinity = 16,144) Hello from rank 9, thread 0, on nid000002. (core affinity = 16,144) Hello from rank 10, thread 0, on nid000003. (core affinity = 16,144) Hello from rank 11, thread 0, on nid000004. (core affinity = 16,144) Hello from rank 12, thread 0, on nid000005. (core affinity = 16,144) Hello from rank 13, thread 0, on nid000006. (core affinity = 16,144) Hello from rank 14, thread 0, on nid000007. (core affinity = 16,144) Hello from rank 15, thread 0, on nid000008. (core affinity = 16,144) There are other modes available with the MPICH_RANK_REORDER_METHOD environment variable, including one which lets the user provide a file called MPICH_RANK_ORDER which contains a list of each task's placement on each node. These options are described in detail in the intro_mpi man page. grid_order For MPI applications which perform a large amount of nearest-neighbor communication, e.g., stencil-based applications on structured grids, Cray provides a tool in the perftools-base module called grid_order which can generate a MPICH_RANK_ORDER file automatically by taking as parameters the dimensions of the grid, core count, etc. For example, to place MPI tasks in row-major order on a Cartesian grid of size $(4, 4, 4)$, using 32 tasks per node: module load perftools-base grid_order -R -c 32 -g 4,4,4 # grid_order -R -Z -c 32 -g 4,4,4 # Region 3: 0,0,1 (0..63) 0,1,2,3,16,17,18,19,32,33,34,35,48,49,50,51,4,5,6,7,20,21,22,23,36,37,38,39,52,53,54,55 8,9,10,11,24,25,26,27,40,41,42,43,56,57,58,59,12,13,14,15,28,29,30,31,44,45,46,47,60,61,62,63 One can then save this output to a file called MPICH_RANK_ORDER and then set MPICH_RANK_REORDER_METHOD=3 before running the job, which tells Cray MPI to read the MPICH_RANK_ORDER file to set the MPI task placement. For more information, please see the man page man grid_order (available when the perftools-base module is loaded).","title":"MPICH_RANK_REORDER_METHOD"},{"location":"user-guide/scheduler/#huge-pages","text":"Huge pages are virtual memory pages which are bigger than the default page size of 4K bytes. Huge pages can improve memory performance for common access patterns on large data sets since it helps to reduce the number of virtual to physical address translations when compared to using the default 4KB. To use huge pages for an application (with the 2 MB huge pages as an example): module load craype-hugepages2M cc -o mycode.exe mycode.c And also load the same huge pages module at runtime. Warning Due to the huge pages memory fragmentation issue, applications may get Cannot allocate memory warnings or errors when there are not enough hugepages on the compute node, such as: libhugetlbfs [nid0000xx:xxxxx]: WARNING: New heap segment map at 0x10000000 failed: Cannot allocate memory`` By default, The verbosity level of libhugetlbfs HUGETLB_VERBOSE is set to 0 on ARCHER2 to surpress debugging messages. Users can adjust this value to obtain more information on huge pages use.","title":"Huge pages"},{"location":"user-guide/scheduler/#when-to-use-huge-pages","text":"For MPI applications, map the static data and/or heap onto huge pages. For an application which uses shared memory, which needs to be concurrently registered with the high speed network drivers for remote communication. For SHMEM applications, map the static data and/or private heap onto huge pages. For applications written in Unified Parallel C, Coarray Fortran, and other languages based on the PGAS programming model, map the static data and/or private heap onto huge pages. For an application doing heavy I/O. To improve memory performance for common access patterns on large data sets.","title":"When to Use Huge Pages"},{"location":"user-guide/scheduler/#when-to-avoid-huge-pages","text":"Applications sometimes consist of many steering programs in addition to the core application. Applying huge page behavior to all processes would not provide any benefit and would consume huge pages that would otherwise benefit the core application. The runtime environment variable HUGETLB_RESTRICT_EXE can be used to specify the susbset of the programs to use hugepages. For certain applications if using hugepages either causes issues or slows down performance. One such example is that when an application forks more subprocesses (such as pthreads) and these threads allocate memory, the newly allocated memory are the default 4 KB pages.","title":"When to Avoid Huge Pages"},{"location":"user-guide/sw-environment/","text":"Software environment Warning The ARCHER2 Service is not yet available. This documentation is in development. The software environment on ARCHER2 is primarily controlled through the module command. By loading and switching software modules you control which software and versions are available to you. Information A module is a self-contained description of a software package -- it contains the settings required to run a software package and, usually, encodes required dependencies on other software packages. By default, all users on ARCHER2 start with the default software environment loaded. Software modules on ARCHER2 are provided by both HPE Cray (usually known as the Cray Development Environment, CDE ) and by EPCC, who provide the Service Provision, and Computational Science and Engineering services. In this section, we provide: A brief overview of the module command A brief description of how the module command manipulates your environment Using the module command We only cover basic usage of the module command here. For full documentation please see the Linux manual page on modules The module command takes a subcommand to indicate what operation you wish to perform. Common subcommands are: module list [name] - List modules currently loaded in your environment, optionally filtered by [name] module avail [name] - List modules available, optionally filtered by [name] module savelist - List module collections available (usually used for accessing different programming environments) module restore name - Restore the module collection called name (usually used for setting up a programming environment) module load name - Load the module called name into your environment module remove name - Remove the module called name from your environment module swap old new - Swap module new for module old in your environment module help name - Show help information on module name module show name - List what module name actually does to your environment These are described in more detail below. Information on the available modules The module list command will give the names of the modules and their versions you have presently loaded in your environment: auser@uan01:~> module list Currently Loaded Modulefiles: 1) cpe-aocc 7) cray-dsmml/0.1.2(default) 2) aocc/2.1.0.3(default) 8) perftools-base/20.09.0(default) 3) craype/2.7.0(default) 9) xpmem/2.2.35-7.0.1.0_1.3__gd50fabf.shasta(default) 4) craype-x86-rome 10) cray-mpich/8.0.15(default) 5) libfabric/1.11.0.0.233(default) 11) cray-libsci/20.08.1.2(default) 6) craype-network-ofi Finding out which software modules are available on the system is performed using the module avail command. To list all software modules available, use: auser@uan01:~> module avail ------------------------------- /opt/cray/pe/perftools/20.09.0/modulefiles -------------------------------- perftools perftools-lite-events perftools-lite-hbm perftools-nwpc perftools-lite perftools-lite-gpu perftools-lite-loops perftools-preload ---------------------------------- /opt/cray/pe/craype/2.7.0/modulefiles ---------------------------------- craype-hugepages1G craype-hugepages8M craype-hugepages128M craype-network-ofi craype-hugepages2G craype-hugepages16M craype-hugepages256M craype-network-slingshot10 craype-hugepages2M craype-hugepages32M craype-hugepages512M craype-x86-rome craype-hugepages4M craype-hugepages64M craype-network-none ------------------------------------- /usr/local/Modules/modulefiles -------------------------------------- dot module-git module-info modules null use.own -------------------------------------- /opt/cray/pe/cpe-prgenv/7.0.0 -------------------------------------- cpe-aocc cpe-cray cpe-gnu -------------------------------------------- /opt/modulefiles --------------------------------------------- aocc/2.1.0.3(default) cray-R/4.0.2.0(default) gcc/8.1.0 gcc/9.3.0 gcc/10.1.0(default) ---------------------------------------- /opt/cray/pe/modulefiles ----------------------------------------- atp/3.7.4(default) cray-mpich-abi/8.0.15 craype-dl-plugin-py3/20.06.1(default) cce/10.0.3(default) cray-mpich-ucx/8.0.15 craype/2.7.0(default) cray-ccdb/4.7.1(default) cray-mpich/8.0.15(default) craypkg-gen/1.3.10(default) cray-cti/2.7.3(default) cray-netcdf-hdf5parallel/4.7.4.0 gdb4hpc/4.7.3(default) cray-dsmml/0.1.2(default) cray-netcdf/4.7.4.0 iobuf/2.0.10(default) cray-fftw/3.3.8.7(default) cray-openshmemx/11.1.1(default) papi/6.0.0.2(default) cray-ga/5.7.0.3 cray-parallel-netcdf/1.12.1.0 perftools-base/20.09.0(default) cray-hdf5-parallel/1.12.0.0 cray-pmi-lib/6.0.6(default) valgrind4hpc/2.7.2(default) cray-hdf5/1.12.0.0 cray-pmi/6.0.6(default) cray-libsci/20.08.1.2(default) cray-python/3.8.5.0(default) This will list all the names and versions of the modules available on the service. Not all of them may work in your account though due to, for example, licencing restrictions. You will notice that for many modules we have more than one version, each of which is identified by a version number. One of these versions is the default. As the service develops the default version will change and old versions of software may be deleted. You can list all the modules of a particular type by providing an argument to the module avail command. For example, to list all available versions of the HPE Cray FFTW library, use: auser@uan01:~> module avail cray-fftw ---------------------------------------- /opt/cray/pe/modulefiles ----------------------------------------- cray-fftw/3.3.8.7(default) If you want more info on any of the modules, you can use the module help command: auser@uan01:~> module help cray-fftw ------------------------------------------------------------------- Module Specific Help for /opt/cray/pe/modulefiles/cray-fftw/3.3.8.7: =================================================================== FFTW 3.3.8.7 ============ Release Date: ------------- June 2020 Purpose: -------- This Cray FFTW 3.3.8.7 release is supported on Cray Shasta Systems. FFTW is supported on the host CPU but not on the accelerator of Cray systems. The Cray FFTW 3.3.8.7 release provides the following: - Optimizations for AMD Rome CPUs. See the Product and OS Dependencies section for details [...] The module show command reveals what operations the module actually performs to change your environment when it is loaded. We provide a brief overview of what the significance of these different settings mean below. For example, for the default FFTW module: auser@uan01:~> module show cray-fftw ------------------------------------------------------------------- /opt/cray/pe/modulefiles/cray-fftw/3.3.8.7: conflict cray-fftw conflict fftw setenv FFTW_VERSION 3.3.8.7 setenv CRAY_FFTW_VERSION 3.3.8.7 setenv CRAY_FFTW_PREFIX /opt/cray/pe/fftw/3.3.8.7/x86_rome setenv FFTW_ROOT /opt/cray/pe/fftw/3.3.8.7/x86_rome setenv FFTW_DIR /opt/cray/pe/fftw/3.3.8.7/x86_rome/lib setenv FFTW_INC /opt/cray/pe/fftw/3.3.8.7/x86_rome/include prepend-path PATH /opt/cray/pe/fftw/3.3.8.7/x86_rome/bin prepend-path MANPATH /opt/cray/pe/fftw/3.3.8.7/share/man prepend-path CRAY_LD_LIBRARY_PATH /opt/cray/pe/fftw/3.3.8.7/x86_rome/lib prepend-path PE_PKGCONFIG_PRODUCTS PE_FFTW setenv PE_FFTW_TARGET_x86_skylake x86_skylake setenv PE_FFTW_TARGET_x86_rome x86_rome setenv PE_FFTW_TARGET_x86_cascadelake x86_cascadelake setenv PE_FFTW_TARGET_x86_64 x86_64 setenv PE_FFTW_TARGET_share share setenv PE_FFTW_TARGET_sandybridge sandybridge setenv PE_FFTW_TARGET_mic_knl mic_knl setenv PE_FFTW_TARGET_ivybridge ivybridge setenv PE_FFTW_TARGET_haswell haswell setenv PE_FFTW_TARGET_broadwell broadwell setenv PE_FFTW_VOLATILE_PKGCONFIG_PATH /opt/cray/pe/fftw/3.3.8.7/@PE_FFTW_TARGET@/lib/pkgconfig setenv PE_FFTW_PKGCONFIG_VARIABLES PE_FFTW_OMP_REQUIRES_@openmp@ setenv PE_FFTW_OMP_REQUIRES { } setenv PE_FFTW_OMP_REQUIRES_openmp _mp setenv PE_FFTW_PKGCONFIG_LIBS fftw3_mpi:libfftw3_threads:fftw3:fftw3f_mpi:libfftw3f_threads:fftw3f module-whatis {FFTW 3.3.8.7 - Fastest Fourier Transform in the West} [...] Loading, removing and swapping modules To load a module to use the module load command. For example, to load the default version of HPE Cray FFTW into your environment, use: auser@uan01:~> module load cray-fftw Once you have done this, your environment will be setup to use the HPE Cray FFTW library. The above command will load the default version of HPE Cray FFTW. If you need a specific version of the software, you can add more information: auser@uan01:~> module load cray-fftw/3.3.8.7 will load HPE Cray FFTW version 3.3.8.7 into your environment, regardless of the default. If you want to remove software from your environment, module remove will remove a loaded module: auser@uan01:~> module remove cray-fftw will unload what ever version of cray-fftw (even if it is not the default) you might have loaded. There are many situations in which you might want to change the presently loaded version to a different one, such as trying the latest version which is not yet the default or using a legacy version to keep compatibility with old data. This can be achieved most easily by using module swap oldmodule newmodule . Suppose you have loaded version 3.3.8.7 of cray-fftw , the following command will change to version 3.3.8.5: auser@uan01:~> module swap cray-fftw cray-fftw/3.3.8.5 You did not need to specify the version of the loaded module in your current environment as this can be inferred as it will be the only one you have loaded. Changing Programming Environment The three programming environments PrgEnv-aocc , PrgEnv-cray , PrgEnv-gnu are implemented as module collections. The correct way to change programming environment, that is, change the collection of modules, is therefore via module restore . For example: auser@uan01:~> module restore PrgEnv-gnu !!! note there is only one argument, which is the collection to be restored. The command module restore will output a list of modules in the outgoing collection as they are unloaded, and the modules in the incoming collection as they are loaded. If you prefer not to have messages auser@uan1:~> module -s restore PrgEnv-gnu will suppress the messages. An attempt to restore a collection which is already loaded will result in no operation. Module collections are stored in a user's home directory ${HOME}/.module . However, as the home directory is not available to the back end, module restore may fail for batch jobs. In this case, it is possible to restore one of the three standard programming environments via, e.g., module restore /etc/cray-pe.d/PrgEnv-gnu Capturing your environment for reuse Sometimes it is useful to save the module environment that you are using to compile a piece of code or execute a piece of software. This is saved as a module collection. You can save a collection from your current environment by executing: auser@uan01:~> module save [collection_name] Note If you do not specify the environment name, it is called default . You can find the list of saved module environments by executing: auser@uan01:~> module savelist Named collection list: 1) default 2) PrgEnv-aocc 3) PrgEnv-cray 4) PrgEnv-gnu To list the modules in a collection, you can execute, e.g.,: auser@uan01:~> module saveshow PrgEnv-gnu ------------------------------------------------------------------- /home/t01/t01/auser/.module/default: module use --append /opt/cray/pe/perftools/20.09.0/modulefiles module use --append /opt/cray/pe/craype/2.7.0/modulefiles module use --append /usr/local/Modules/modulefiles module use --append /opt/cray/pe/cpe-prgenv/7.0.0 module use --append /opt/modulefiles module use --append /opt/cray/modulefiles module use --append /opt/cray/pe/modulefiles module use --append /opt/cray/pe/craype-targets/default/modulefiles module load cpe-gnu module load gcc module load craype module load craype-x86-rome module load --notuasked libfabric module load craype-network-ofi module load cray-dsmml module load perftools-base module load xpmem module load cray-mpich module load cray-libsci module load /work/y07/shared/archer2-modules/modulefiles-cse/epcc-setup-env Note again that the details of the collection have been saved to the home directory (the first line of output above). It is possible to save a module collection with a fully qualified path, e.g., auser@uan1:~> module save /work/t01/z01/auser/.module/PrgEnv-gnu which would make it available from the batch system. To delete a module environment, you can execute: auser@uan01:~> module saverm <environment_name> Shell environment overview When you log in to ARCHER2, you are using the bash shell by default. As any other software, the bash shell has loaded a set of environment variables that can be listed by executing printenv or export . The environment variables listed before are useful to define the behaviour of the software you run. For instance, OMP_NUM_THREADS define the number of threads. To define an environment variable, you need to execute: export OMP_NUM_THREADS=4 Please note there are no blanks between the variable name, the assignation symbol, and the value. If the value is a string, enclose the string in double quotation marks. You can show the value of a specific environment variable if you print it: echo $OMP_NUM_THREADS Do not forget the dollar symbol. To remove an environment variable, just execute: unset OMP_NUM_THREADS","title":"Software environment"},{"location":"user-guide/sw-environment/#software-environment","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development. The software environment on ARCHER2 is primarily controlled through the module command. By loading and switching software modules you control which software and versions are available to you. Information A module is a self-contained description of a software package -- it contains the settings required to run a software package and, usually, encodes required dependencies on other software packages. By default, all users on ARCHER2 start with the default software environment loaded. Software modules on ARCHER2 are provided by both HPE Cray (usually known as the Cray Development Environment, CDE ) and by EPCC, who provide the Service Provision, and Computational Science and Engineering services. In this section, we provide: A brief overview of the module command A brief description of how the module command manipulates your environment","title":"Software environment"},{"location":"user-guide/sw-environment/#using-the-module-command","text":"We only cover basic usage of the module command here. For full documentation please see the Linux manual page on modules The module command takes a subcommand to indicate what operation you wish to perform. Common subcommands are: module list [name] - List modules currently loaded in your environment, optionally filtered by [name] module avail [name] - List modules available, optionally filtered by [name] module savelist - List module collections available (usually used for accessing different programming environments) module restore name - Restore the module collection called name (usually used for setting up a programming environment) module load name - Load the module called name into your environment module remove name - Remove the module called name from your environment module swap old new - Swap module new for module old in your environment module help name - Show help information on module name module show name - List what module name actually does to your environment These are described in more detail below.","title":"Using the module command"},{"location":"user-guide/sw-environment/#information-on-the-available-modules","text":"The module list command will give the names of the modules and their versions you have presently loaded in your environment: auser@uan01:~> module list Currently Loaded Modulefiles: 1) cpe-aocc 7) cray-dsmml/0.1.2(default) 2) aocc/2.1.0.3(default) 8) perftools-base/20.09.0(default) 3) craype/2.7.0(default) 9) xpmem/2.2.35-7.0.1.0_1.3__gd50fabf.shasta(default) 4) craype-x86-rome 10) cray-mpich/8.0.15(default) 5) libfabric/1.11.0.0.233(default) 11) cray-libsci/20.08.1.2(default) 6) craype-network-ofi Finding out which software modules are available on the system is performed using the module avail command. To list all software modules available, use: auser@uan01:~> module avail ------------------------------- /opt/cray/pe/perftools/20.09.0/modulefiles -------------------------------- perftools perftools-lite-events perftools-lite-hbm perftools-nwpc perftools-lite perftools-lite-gpu perftools-lite-loops perftools-preload ---------------------------------- /opt/cray/pe/craype/2.7.0/modulefiles ---------------------------------- craype-hugepages1G craype-hugepages8M craype-hugepages128M craype-network-ofi craype-hugepages2G craype-hugepages16M craype-hugepages256M craype-network-slingshot10 craype-hugepages2M craype-hugepages32M craype-hugepages512M craype-x86-rome craype-hugepages4M craype-hugepages64M craype-network-none ------------------------------------- /usr/local/Modules/modulefiles -------------------------------------- dot module-git module-info modules null use.own -------------------------------------- /opt/cray/pe/cpe-prgenv/7.0.0 -------------------------------------- cpe-aocc cpe-cray cpe-gnu -------------------------------------------- /opt/modulefiles --------------------------------------------- aocc/2.1.0.3(default) cray-R/4.0.2.0(default) gcc/8.1.0 gcc/9.3.0 gcc/10.1.0(default) ---------------------------------------- /opt/cray/pe/modulefiles ----------------------------------------- atp/3.7.4(default) cray-mpich-abi/8.0.15 craype-dl-plugin-py3/20.06.1(default) cce/10.0.3(default) cray-mpich-ucx/8.0.15 craype/2.7.0(default) cray-ccdb/4.7.1(default) cray-mpich/8.0.15(default) craypkg-gen/1.3.10(default) cray-cti/2.7.3(default) cray-netcdf-hdf5parallel/4.7.4.0 gdb4hpc/4.7.3(default) cray-dsmml/0.1.2(default) cray-netcdf/4.7.4.0 iobuf/2.0.10(default) cray-fftw/3.3.8.7(default) cray-openshmemx/11.1.1(default) papi/6.0.0.2(default) cray-ga/5.7.0.3 cray-parallel-netcdf/1.12.1.0 perftools-base/20.09.0(default) cray-hdf5-parallel/1.12.0.0 cray-pmi-lib/6.0.6(default) valgrind4hpc/2.7.2(default) cray-hdf5/1.12.0.0 cray-pmi/6.0.6(default) cray-libsci/20.08.1.2(default) cray-python/3.8.5.0(default) This will list all the names and versions of the modules available on the service. Not all of them may work in your account though due to, for example, licencing restrictions. You will notice that for many modules we have more than one version, each of which is identified by a version number. One of these versions is the default. As the service develops the default version will change and old versions of software may be deleted. You can list all the modules of a particular type by providing an argument to the module avail command. For example, to list all available versions of the HPE Cray FFTW library, use: auser@uan01:~> module avail cray-fftw ---------------------------------------- /opt/cray/pe/modulefiles ----------------------------------------- cray-fftw/3.3.8.7(default) If you want more info on any of the modules, you can use the module help command: auser@uan01:~> module help cray-fftw ------------------------------------------------------------------- Module Specific Help for /opt/cray/pe/modulefiles/cray-fftw/3.3.8.7: =================================================================== FFTW 3.3.8.7 ============ Release Date: ------------- June 2020 Purpose: -------- This Cray FFTW 3.3.8.7 release is supported on Cray Shasta Systems. FFTW is supported on the host CPU but not on the accelerator of Cray systems. The Cray FFTW 3.3.8.7 release provides the following: - Optimizations for AMD Rome CPUs. See the Product and OS Dependencies section for details [...] The module show command reveals what operations the module actually performs to change your environment when it is loaded. We provide a brief overview of what the significance of these different settings mean below. For example, for the default FFTW module: auser@uan01:~> module show cray-fftw ------------------------------------------------------------------- /opt/cray/pe/modulefiles/cray-fftw/3.3.8.7: conflict cray-fftw conflict fftw setenv FFTW_VERSION 3.3.8.7 setenv CRAY_FFTW_VERSION 3.3.8.7 setenv CRAY_FFTW_PREFIX /opt/cray/pe/fftw/3.3.8.7/x86_rome setenv FFTW_ROOT /opt/cray/pe/fftw/3.3.8.7/x86_rome setenv FFTW_DIR /opt/cray/pe/fftw/3.3.8.7/x86_rome/lib setenv FFTW_INC /opt/cray/pe/fftw/3.3.8.7/x86_rome/include prepend-path PATH /opt/cray/pe/fftw/3.3.8.7/x86_rome/bin prepend-path MANPATH /opt/cray/pe/fftw/3.3.8.7/share/man prepend-path CRAY_LD_LIBRARY_PATH /opt/cray/pe/fftw/3.3.8.7/x86_rome/lib prepend-path PE_PKGCONFIG_PRODUCTS PE_FFTW setenv PE_FFTW_TARGET_x86_skylake x86_skylake setenv PE_FFTW_TARGET_x86_rome x86_rome setenv PE_FFTW_TARGET_x86_cascadelake x86_cascadelake setenv PE_FFTW_TARGET_x86_64 x86_64 setenv PE_FFTW_TARGET_share share setenv PE_FFTW_TARGET_sandybridge sandybridge setenv PE_FFTW_TARGET_mic_knl mic_knl setenv PE_FFTW_TARGET_ivybridge ivybridge setenv PE_FFTW_TARGET_haswell haswell setenv PE_FFTW_TARGET_broadwell broadwell setenv PE_FFTW_VOLATILE_PKGCONFIG_PATH /opt/cray/pe/fftw/3.3.8.7/@PE_FFTW_TARGET@/lib/pkgconfig setenv PE_FFTW_PKGCONFIG_VARIABLES PE_FFTW_OMP_REQUIRES_@openmp@ setenv PE_FFTW_OMP_REQUIRES { } setenv PE_FFTW_OMP_REQUIRES_openmp _mp setenv PE_FFTW_PKGCONFIG_LIBS fftw3_mpi:libfftw3_threads:fftw3:fftw3f_mpi:libfftw3f_threads:fftw3f module-whatis {FFTW 3.3.8.7 - Fastest Fourier Transform in the West} [...]","title":"Information on the available modules"},{"location":"user-guide/sw-environment/#loading-removing-and-swapping-modules","text":"To load a module to use the module load command. For example, to load the default version of HPE Cray FFTW into your environment, use: auser@uan01:~> module load cray-fftw Once you have done this, your environment will be setup to use the HPE Cray FFTW library. The above command will load the default version of HPE Cray FFTW. If you need a specific version of the software, you can add more information: auser@uan01:~> module load cray-fftw/3.3.8.7 will load HPE Cray FFTW version 3.3.8.7 into your environment, regardless of the default. If you want to remove software from your environment, module remove will remove a loaded module: auser@uan01:~> module remove cray-fftw will unload what ever version of cray-fftw (even if it is not the default) you might have loaded. There are many situations in which you might want to change the presently loaded version to a different one, such as trying the latest version which is not yet the default or using a legacy version to keep compatibility with old data. This can be achieved most easily by using module swap oldmodule newmodule . Suppose you have loaded version 3.3.8.7 of cray-fftw , the following command will change to version 3.3.8.5: auser@uan01:~> module swap cray-fftw cray-fftw/3.3.8.5 You did not need to specify the version of the loaded module in your current environment as this can be inferred as it will be the only one you have loaded.","title":"Loading, removing and swapping modules"},{"location":"user-guide/sw-environment/#changing-programming-environment","text":"The three programming environments PrgEnv-aocc , PrgEnv-cray , PrgEnv-gnu are implemented as module collections. The correct way to change programming environment, that is, change the collection of modules, is therefore via module restore . For example: auser@uan01:~> module restore PrgEnv-gnu !!! note there is only one argument, which is the collection to be restored. The command module restore will output a list of modules in the outgoing collection as they are unloaded, and the modules in the incoming collection as they are loaded. If you prefer not to have messages auser@uan1:~> module -s restore PrgEnv-gnu will suppress the messages. An attempt to restore a collection which is already loaded will result in no operation. Module collections are stored in a user's home directory ${HOME}/.module . However, as the home directory is not available to the back end, module restore may fail for batch jobs. In this case, it is possible to restore one of the three standard programming environments via, e.g., module restore /etc/cray-pe.d/PrgEnv-gnu","title":"Changing Programming Environment"},{"location":"user-guide/sw-environment/#capturing-your-environment-for-reuse","text":"Sometimes it is useful to save the module environment that you are using to compile a piece of code or execute a piece of software. This is saved as a module collection. You can save a collection from your current environment by executing: auser@uan01:~> module save [collection_name] Note If you do not specify the environment name, it is called default . You can find the list of saved module environments by executing: auser@uan01:~> module savelist Named collection list: 1) default 2) PrgEnv-aocc 3) PrgEnv-cray 4) PrgEnv-gnu To list the modules in a collection, you can execute, e.g.,: auser@uan01:~> module saveshow PrgEnv-gnu ------------------------------------------------------------------- /home/t01/t01/auser/.module/default: module use --append /opt/cray/pe/perftools/20.09.0/modulefiles module use --append /opt/cray/pe/craype/2.7.0/modulefiles module use --append /usr/local/Modules/modulefiles module use --append /opt/cray/pe/cpe-prgenv/7.0.0 module use --append /opt/modulefiles module use --append /opt/cray/modulefiles module use --append /opt/cray/pe/modulefiles module use --append /opt/cray/pe/craype-targets/default/modulefiles module load cpe-gnu module load gcc module load craype module load craype-x86-rome module load --notuasked libfabric module load craype-network-ofi module load cray-dsmml module load perftools-base module load xpmem module load cray-mpich module load cray-libsci module load /work/y07/shared/archer2-modules/modulefiles-cse/epcc-setup-env Note again that the details of the collection have been saved to the home directory (the first line of output above). It is possible to save a module collection with a fully qualified path, e.g., auser@uan1:~> module save /work/t01/z01/auser/.module/PrgEnv-gnu which would make it available from the batch system. To delete a module environment, you can execute: auser@uan01:~> module saverm <environment_name>","title":"Capturing your environment for reuse"},{"location":"user-guide/sw-environment/#shell-environment-overview","text":"When you log in to ARCHER2, you are using the bash shell by default. As any other software, the bash shell has loaded a set of environment variables that can be listed by executing printenv or export . The environment variables listed before are useful to define the behaviour of the software you run. For instance, OMP_NUM_THREADS define the number of threads. To define an environment variable, you need to execute: export OMP_NUM_THREADS=4 Please note there are no blanks between the variable name, the assignation symbol, and the value. If the value is a string, enclose the string in double quotation marks. You can show the value of a specific environment variable if you print it: echo $OMP_NUM_THREADS Do not forget the dollar symbol. To remove an environment variable, just execute: unset OMP_NUM_THREADS","title":"Shell environment overview"},{"location":"user-guide/tuning/","text":"Performance tuning Warning The ARCHER2 Service is not yet available. This documentation is in development. MPI The vast majority of parallel scientific software uses the MPI library as the main way to implement parallelism; it is used so universally that the Cray compiler wrappers on ARCHER2 link to the Cray MPI library by default. Unlike other clusters you may have used, there is no choice of MPI library on ARCHER2: regardless of what compiler you are using, your program will use Cray MPI. This is because the Slingshot network on ARCHER2 is Cray-specific and significant effort has been put in by Cray software engineers to optimise the MPI performance on Cray Shasta systems. Here we list a number of suggestions for improving the performance of your MPI programs on ARCHER2. Although MPI programs are capable of scaling very well due to the bespoke communications hardware and software, the details of how a program calls MPI can have significant effects on achieved performance. Note Many of these tips are actually quite generic and should be beneficial to any MPI program; however, they all become much more important when running on very large numbers of processes on a machine the size of ARCHER2. Synchronous vs asynchronous communications MPI_Send A standard way to send data in MPI is using MPI_Send (aptly called standard send ). Somewhat confusingly, MPI is allowed to choose how to implement this in two different ways: Synchronously The sending process waits until a matching receive has been posted, i.e. it operates like MPI_Ssend . This clearly has the risk of deadlock if no receive is ever issued. Asynchronously MPI makes a copy of the message into an internal buffer and returns straight away without waiting for a matching receive; the message may actually be delivered later on. This is like the behaviour of the the buffered send routine MPI_Bsend . The rationale is that MPI, rather than the user, should decide how best to send a message. In practice, what typically happens is that MPI tries to use an asynchronous approach via the eager protocol -- the message is copied directly to a preallocated buffer on the receiver and the routine returns immediately afterward. Clearly there is a limit on how much space can be reserved for this, so: small messages will be sent asynchronously; large messages will be sent synchronously. The threshold is often termed the eager limit which is fixed for the entire run of your program. It will have some default setting which varies from system to system, but might be around 8K bytes. Implications An MPI program will typically run faster if MPI_Send is implemented asynchronously using the eager protocol since synchronisation between sender and receive is much reduced. However, you should never assume that MPI_Send buffers your message, so if you have concerns about deadlock you will need to use the non-blocking variant MPI_Isend to guarantee that the send routine returns control to you immediately even if there is no matching receive. It is not enough to say *deadlock is an issue in principle, but it runs OK on my laptop so there is no problem in practice*. The eager limit is system-dependent so the fact that a message happens to be buffered on your laptop is no guarantee it will be buffered on ARCHER2. To check that you have a correct code, replace all instances of MPI_Send / MPI_Isend with MPI_Ssend / MPI_Issend . A correct MPI program should still run correctly when all references to standard send are replaced by synchronous send (since MPI is allowed to implement standard send as synchronous send). Tuning performance With most MPI libraries you should be able to alter the default value of the eager limit at runtime, perhaps via an environment variable or a command-line argument to mpirun . On ARCHER, the magic incantation is: export MPICH_GNI_MAX_EAGER_MSG_SIZE=16382 which would mean that messages below 16K bytes are sent asynchronously (there will be a similar, but different, incantation on ARCHER2). The advice for tuning the performance of MPI_Send is find out what the distribution of message sizes for MPI_Send is (a profiling tool may be useful here); this applies to MPI_Isend as well: even in the non-blocking form, which can help to weaken synchronisation between sender and receiver, the amount of hand-shaking required is much reduced if the eager protocol is used; find out from the system documentation how to alter the value of the eager limit (there is no standardised way to set it); set the eager limit to a value larger than your typical message size -- you may need to add a small amount, say a few hundred bytes, to allow for any additional header information that is added to each message; measure the performance before and after to check that it has improved. Note It cannot be stressed strongly enough that although the performance may be affected by the value of the eager limit, the functionality of your program should be unaffected. If changing the eager limit affects the correctness of your program (e.g. whether or not it deadlocks) then you have an incorrect MPI program . Collective operations Many of the collective operations that are commonly required by parallel scientific programs, i.e. operations that involve a group of processes, are already implemented in MPI. The canonical operation is perhaps adding up a double precision number across all MPI processes, which is best achieved by a reduction operation : MPI_Allreduce(&x, &xsum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD); This will be implemented using an efficient algorithm, for example based on a binary tree. Using such divide-and-conquer approaches typically results in an algorithm whose execution time on (P) processes scales as (log_2(P)); compare this to a naive approach where every process sends its input to rank 0 where the time will scale as (P). This might not be significant on your laptop, but even on as few as 1000 processes the tree-based algorithm will already be around 100 times faster. So, the basic advice is always use a collective routine to implement your communications pattern if at all possible. In real MPI applications, collective operations are often called on a small amount of data, for example a global reduction of a single variable. In these cases, the time taken will be dominated by message latency and the first port of call when looking at performance optimisation is to call them as infrequently as possible! If you are simply printing diagnostics to the screen in an iterative loop, consider doing this less frequently, e.g every ten iterations, or even not at all (although you should easily be able to turn diagnostics on again for future debugging). If you are computing some termination criterion, it may actually be faster overall to compute it and check for convergence infrequently, e.g. every ten iterations, even although this means that your program could run for up to 9 extra iterations. If possible, group data into a single buffer and call a single reduction with count > 1; two reductions with count = 1 will take almost exactly twice as long as a single reduction with count = 2. For example, if you only need to output a sequence of summed data at the end of the run, store the partial totals in an array and do a single reduction right at the end. Sometimes, the collective routines available may not appear to do exactly what you want. However, they can sometimes be used with a small amount of additional programming work: To operate on a subset of processes, create sub-communicators containing the relevant subset(s) and use these communicators instead of MPI_COMM_WORLD . Useful functions for communicator management include: MPI_Comm_split is the most general routine; MPI_Comm_split_type can be used to create a separate communicator for each shared-memory node with split type = MPI_COMM_TYPE_SHARED ; MPI_Cart_sub can divide a Cartesian communicator into regular slices. If the communication pattern is what you want, but the data on each process is not arranged in the required layout, consider using MPI derived data types for the input and/or output. This can be useful, for example, if you want to communicate non-contiguous data such as a subsection of a multidimensional array although care must be taken in defining these types to ensure they have the correct extents. Another example would be using MPI_Allreduce to add up an integer and a double-precision variable using a single call by putting them together into a C struct and defining a matching MPI datatype using MPI_Type_create_struct . Here you would also have to provide MPI with a custom reduction operation using MPI_Op_create . Many MPI programs call MPI_Barrier to explicitly synchronise all the processes. Although this can be useful for getting reliable performance timings, it is rare in practice to find a program where the call is actually needed for correctness. For example, you may see: // Ensure the input x is available on all processes MPI_Barrier(MPI_COMM_WORLD); // Perform a global reduction operation MPI_Allreduce(&x, &xsum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD); // Ensure the result xsum is available on all processes MPI_Barrier(MPI_COMM_WORLD); Neither of these barriers are needed as the reduction operation performs all the required synchronisation. If removing a barrier from your MPI code makes it run incorrectly, then this should ring alarm bells -- it is often a symptom of an underlying bug that is simply being masked by the barrier. For example, if you use non-blocking calls such as MPI_Irecv then it is the programmer's responsibility to ensure that these are completed at some later point, for example by calling MPI_Wait on the returned request object. A common bug is to forget to do this, in which case you might be reading the contents of the receive buffer before the incoming message has arrived (e.g. if the sender is running late). Calling a barrier may mask this bug as it will make all the processes wait for each other, perhaps allowing the late sender to catch up. However, this is not guaranteed so the real solution is to call the non-blocking communications correctly. One of the few times when a barrier may be required is if processes are communicating with each other via some other non-MPI method, e.g. via the file system. If you want processes to sequentially open, append to, then close the same file then barriers are a simple way to achieve this: for (i=0; i < size; i++) { if (rank == i) append_data_to_file(data, filename); MPI_Barrier(comm); } but this is really something of a special case. Global synchronisation may be required if you are using more advanced techniques such as hybrid MPI/OpenMP or single-sided MPI communication with put and get, but typically you should be using specialised routines such as MPI_Win_fence rather than MPI_Barrier . Tip If you run a performance profiler on your code and it shows a lot of time being spent in a collective operation such as MPI_Allreduce , this is not necessarily a sign that the reduction operation itself is the bottleneck. This is often a symptom of load imbalance : even if a reduction operation is efficiently implemented, it may take a long time to complete if the MPI processes do not all call it at the same time. MPI_Allreduce synchronises across processes so will have to wait for all the processes to call it before it can complete. A single slow process will therefore adversely impact the performance of your entire parallel program.","title":"Performance tuning"},{"location":"user-guide/tuning/#performance-tuning","text":"Warning The ARCHER2 Service is not yet available. This documentation is in development.","title":"Performance tuning"},{"location":"user-guide/tuning/#mpi","text":"The vast majority of parallel scientific software uses the MPI library as the main way to implement parallelism; it is used so universally that the Cray compiler wrappers on ARCHER2 link to the Cray MPI library by default. Unlike other clusters you may have used, there is no choice of MPI library on ARCHER2: regardless of what compiler you are using, your program will use Cray MPI. This is because the Slingshot network on ARCHER2 is Cray-specific and significant effort has been put in by Cray software engineers to optimise the MPI performance on Cray Shasta systems. Here we list a number of suggestions for improving the performance of your MPI programs on ARCHER2. Although MPI programs are capable of scaling very well due to the bespoke communications hardware and software, the details of how a program calls MPI can have significant effects on achieved performance. Note Many of these tips are actually quite generic and should be beneficial to any MPI program; however, they all become much more important when running on very large numbers of processes on a machine the size of ARCHER2.","title":"MPI"},{"location":"user-guide/tuning/#synchronous-vs-asynchronous-communications","text":"","title":"Synchronous vs asynchronous communications"},{"location":"user-guide/tuning/#mpi_send","text":"A standard way to send data in MPI is using MPI_Send (aptly called standard send ). Somewhat confusingly, MPI is allowed to choose how to implement this in two different ways: Synchronously The sending process waits until a matching receive has been posted, i.e. it operates like MPI_Ssend . This clearly has the risk of deadlock if no receive is ever issued. Asynchronously MPI makes a copy of the message into an internal buffer and returns straight away without waiting for a matching receive; the message may actually be delivered later on. This is like the behaviour of the the buffered send routine MPI_Bsend . The rationale is that MPI, rather than the user, should decide how best to send a message. In practice, what typically happens is that MPI tries to use an asynchronous approach via the eager protocol -- the message is copied directly to a preallocated buffer on the receiver and the routine returns immediately afterward. Clearly there is a limit on how much space can be reserved for this, so: small messages will be sent asynchronously; large messages will be sent synchronously. The threshold is often termed the eager limit which is fixed for the entire run of your program. It will have some default setting which varies from system to system, but might be around 8K bytes.","title":"MPI_Send"},{"location":"user-guide/tuning/#implications","text":"An MPI program will typically run faster if MPI_Send is implemented asynchronously using the eager protocol since synchronisation between sender and receive is much reduced. However, you should never assume that MPI_Send buffers your message, so if you have concerns about deadlock you will need to use the non-blocking variant MPI_Isend to guarantee that the send routine returns control to you immediately even if there is no matching receive. It is not enough to say *deadlock is an issue in principle, but it runs OK on my laptop so there is no problem in practice*. The eager limit is system-dependent so the fact that a message happens to be buffered on your laptop is no guarantee it will be buffered on ARCHER2. To check that you have a correct code, replace all instances of MPI_Send / MPI_Isend with MPI_Ssend / MPI_Issend . A correct MPI program should still run correctly when all references to standard send are replaced by synchronous send (since MPI is allowed to implement standard send as synchronous send).","title":"Implications"},{"location":"user-guide/tuning/#tuning-performance","text":"With most MPI libraries you should be able to alter the default value of the eager limit at runtime, perhaps via an environment variable or a command-line argument to mpirun . On ARCHER, the magic incantation is: export MPICH_GNI_MAX_EAGER_MSG_SIZE=16382 which would mean that messages below 16K bytes are sent asynchronously (there will be a similar, but different, incantation on ARCHER2). The advice for tuning the performance of MPI_Send is find out what the distribution of message sizes for MPI_Send is (a profiling tool may be useful here); this applies to MPI_Isend as well: even in the non-blocking form, which can help to weaken synchronisation between sender and receiver, the amount of hand-shaking required is much reduced if the eager protocol is used; find out from the system documentation how to alter the value of the eager limit (there is no standardised way to set it); set the eager limit to a value larger than your typical message size -- you may need to add a small amount, say a few hundred bytes, to allow for any additional header information that is added to each message; measure the performance before and after to check that it has improved. Note It cannot be stressed strongly enough that although the performance may be affected by the value of the eager limit, the functionality of your program should be unaffected. If changing the eager limit affects the correctness of your program (e.g. whether or not it deadlocks) then you have an incorrect MPI program .","title":"Tuning performance"},{"location":"user-guide/tuning/#collective-operations","text":"Many of the collective operations that are commonly required by parallel scientific programs, i.e. operations that involve a group of processes, are already implemented in MPI. The canonical operation is perhaps adding up a double precision number across all MPI processes, which is best achieved by a reduction operation : MPI_Allreduce(&x, &xsum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD); This will be implemented using an efficient algorithm, for example based on a binary tree. Using such divide-and-conquer approaches typically results in an algorithm whose execution time on (P) processes scales as (log_2(P)); compare this to a naive approach where every process sends its input to rank 0 where the time will scale as (P). This might not be significant on your laptop, but even on as few as 1000 processes the tree-based algorithm will already be around 100 times faster. So, the basic advice is always use a collective routine to implement your communications pattern if at all possible. In real MPI applications, collective operations are often called on a small amount of data, for example a global reduction of a single variable. In these cases, the time taken will be dominated by message latency and the first port of call when looking at performance optimisation is to call them as infrequently as possible! If you are simply printing diagnostics to the screen in an iterative loop, consider doing this less frequently, e.g every ten iterations, or even not at all (although you should easily be able to turn diagnostics on again for future debugging). If you are computing some termination criterion, it may actually be faster overall to compute it and check for convergence infrequently, e.g. every ten iterations, even although this means that your program could run for up to 9 extra iterations. If possible, group data into a single buffer and call a single reduction with count > 1; two reductions with count = 1 will take almost exactly twice as long as a single reduction with count = 2. For example, if you only need to output a sequence of summed data at the end of the run, store the partial totals in an array and do a single reduction right at the end. Sometimes, the collective routines available may not appear to do exactly what you want. However, they can sometimes be used with a small amount of additional programming work: To operate on a subset of processes, create sub-communicators containing the relevant subset(s) and use these communicators instead of MPI_COMM_WORLD . Useful functions for communicator management include: MPI_Comm_split is the most general routine; MPI_Comm_split_type can be used to create a separate communicator for each shared-memory node with split type = MPI_COMM_TYPE_SHARED ; MPI_Cart_sub can divide a Cartesian communicator into regular slices. If the communication pattern is what you want, but the data on each process is not arranged in the required layout, consider using MPI derived data types for the input and/or output. This can be useful, for example, if you want to communicate non-contiguous data such as a subsection of a multidimensional array although care must be taken in defining these types to ensure they have the correct extents. Another example would be using MPI_Allreduce to add up an integer and a double-precision variable using a single call by putting them together into a C struct and defining a matching MPI datatype using MPI_Type_create_struct . Here you would also have to provide MPI with a custom reduction operation using MPI_Op_create . Many MPI programs call MPI_Barrier to explicitly synchronise all the processes. Although this can be useful for getting reliable performance timings, it is rare in practice to find a program where the call is actually needed for correctness. For example, you may see: // Ensure the input x is available on all processes MPI_Barrier(MPI_COMM_WORLD); // Perform a global reduction operation MPI_Allreduce(&x, &xsum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD); // Ensure the result xsum is available on all processes MPI_Barrier(MPI_COMM_WORLD); Neither of these barriers are needed as the reduction operation performs all the required synchronisation. If removing a barrier from your MPI code makes it run incorrectly, then this should ring alarm bells -- it is often a symptom of an underlying bug that is simply being masked by the barrier. For example, if you use non-blocking calls such as MPI_Irecv then it is the programmer's responsibility to ensure that these are completed at some later point, for example by calling MPI_Wait on the returned request object. A common bug is to forget to do this, in which case you might be reading the contents of the receive buffer before the incoming message has arrived (e.g. if the sender is running late). Calling a barrier may mask this bug as it will make all the processes wait for each other, perhaps allowing the late sender to catch up. However, this is not guaranteed so the real solution is to call the non-blocking communications correctly. One of the few times when a barrier may be required is if processes are communicating with each other via some other non-MPI method, e.g. via the file system. If you want processes to sequentially open, append to, then close the same file then barriers are a simple way to achieve this: for (i=0; i < size; i++) { if (rank == i) append_data_to_file(data, filename); MPI_Barrier(comm); } but this is really something of a special case. Global synchronisation may be required if you are using more advanced techniques such as hybrid MPI/OpenMP or single-sided MPI communication with put and get, but typically you should be using specialised routines such as MPI_Win_fence rather than MPI_Barrier . Tip If you run a performance profiler on your code and it shows a lot of time being spent in a collective operation such as MPI_Allreduce , this is not necessarily a sign that the reduction operation itself is the bottleneck. This is often a symptom of load imbalance : even if a reduction operation is efficiently implemented, it may take a long time to complete if the MPI processes do not all call it at the same time. MPI_Allreduce synchronises across processes so will have to wait for all the processes to call it before it can complete. A single slow process will therefore adversely impact the performance of your entire parallel program.","title":"Collective operations"}]}

<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="shortcut icon" href="../../favicon.ico">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-6.1.4">
    
    
      
        <title>Running jobs on ARCHER2 - ARCHER2 User Documentation</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.358818c7.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.f0267088.min.css">
        
      
    
    
    
      
        
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
      <link rel="stylesheet" href="../../stylesheets/archer2.css">
    
    
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="" data-md-color-accent="">
      
  
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#running-jobs-on-archer2" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="Header">
    <a href="../.." title="ARCHER2 User Documentation" class="md-header-nav__button md-logo" aria-label="ARCHER2 User Documentation">
      
  <img src="../../images/archer2_white_transparent.png" alt="logo">

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      
        <div class="md-header-nav__ellipsis">
          <span class="md-header-nav__topic md-ellipsis">
            ARCHER2 User Documentation
          </span>
          <span class="md-header-nav__topic md-ellipsis">
            
              Running jobs on ARCHER2
            
          </span>
        </div>
      
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active">
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header-nav__source">
        
<a href="https://github.com/ARCHER2-HPC/archer2-docs/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    ARCHER2-HPC/archer2-docs
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="ARCHER2 User Documentation" class="md-nav__button md-logo" aria-label="ARCHER2 User Documentation">
      
  <img src="../../images/archer2_white_transparent.png" alt="logo">

    </a>
    ARCHER2 User Documentation
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/ARCHER2-HPC/archer2-docs/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    ARCHER2-HPC/archer2-docs
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../.." class="md-nav__link">
      Documentation overview
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2">
    
    <label class="md-nav__link" for="nav-2">
      Quickstart
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="Quickstart" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        <span class="md-nav__icon md-icon"></span>
        Quickstart
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../quick-start/overview/" class="md-nav__link">
      Overview
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../quick-start/quickstart-users/" class="md-nav__link">
      Quickstart for users
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../quick-start/quickstart-developers/" class="md-nav__link">
      Quickstart for developers
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3" checked>
    
    <label class="md-nav__link" for="nav-3">
      User and Best Practice Guide
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="User and Best Practice Guide" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        <span class="md-nav__icon md-icon"></span>
        User and Best Practice Guide
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../" class="md-nav__link">
      Overview
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../connecting/" class="md-nav__link">
      Connecting to ARCHER2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../data/" class="md-nav__link">
      Data management and transfer
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../sw-environment/" class="md-nav__link">
      Software environment
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Running jobs on ARCHER2
        <span class="md-nav__icon md-icon"></span>
      </label>
    
    <a href="./" class="md-nav__link md-nav__link--active">
      Running jobs on ARCHER2
    </a>
    
      
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#basic-slurm-commands" class="md-nav__link">
    Basic Slurm commands
  </a>
  
    <nav class="md-nav" aria-label="Basic Slurm commands">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sinfo-information-on-resources" class="md-nav__link">
    sinfo: information on resources
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbatch-submitting-jobs" class="md-nav__link">
    sbatch: submitting jobs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#squeue-monitoring-jobs" class="md-nav__link">
    squeue: monitoring jobs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scancel-deleting-jobs" class="md-nav__link">
    scancel: deleting jobs
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resource-limits" class="md-nav__link">
    Resource Limits
  </a>
  
    <nav class="md-nav" aria-label="Resource Limits">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#primary-resource" class="md-nav__link">
    Primary resource
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#partitions" class="md-nav__link">
    Partitions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quality-of-service-qos" class="md-nav__link">
    Quality of Service (QoS)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#troubleshooting" class="md-nav__link">
    Troubleshooting
  </a>
  
    <nav class="md-nav" aria-label="Troubleshooting">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slurm-error-messages" class="md-nav__link">
    Slurm error messages
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slurm-queued-reasons" class="md-nav__link">
    Slurm queued reasons
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#output-from-slurm-jobs" class="md-nav__link">
    Output from Slurm jobs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#specifying-resources-in-job-scripts" class="md-nav__link">
    Specifying resources in job scripts
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#using-modules-in-the-batch-system-the-epcc-job-env-module" class="md-nav__link">
    Using modules in the batch system: the epcc-job-env module
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#srun-launching-parallel-jobs" class="md-nav__link">
    srun: Launching parallel jobs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bolt-job-submission-script-creation-tool" class="md-nav__link">
    bolt: Job submission script creation tool
  </a>
  
    <nav class="md-nav" aria-label="bolt: Job submission script creation tool">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#basic-usage" class="md-nav__link">
    Basic Usage
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#further-help" class="md-nav__link">
    Further help
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#checkscript-job-submission-script-validation-tool" class="md-nav__link">
    checkScript job submission script validation tool
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#example-job-submission-scripts" class="md-nav__link">
    Example job submission scripts
  </a>
  
    <nav class="md-nav" aria-label="Example job submission scripts">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-job-submission-script-for-mpi-parallel-job" class="md-nav__link">
    Example: job submission script for MPI parallel job
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-job-submission-script-for-mpiopenmp-mixed-mode-parallel-job" class="md-nav__link">
    Example: job submission script for MPI+OpenMP (mixed mode) parallel job
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#job-arrays" class="md-nav__link">
    Job arrays
  </a>
  
    <nav class="md-nav" aria-label="Job arrays">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#job-script-for-a-job-array" class="md-nav__link">
    Job script for a job array
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#submitting-a-job-array" class="md-nav__link">
    Submitting a job array
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#job-chaining" class="md-nav__link">
    Job chaining
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#interactive-jobs-salloc" class="md-nav__link">
    Interactive Jobs: salloc
  </a>
  
    <nav class="md-nav" aria-label="Interactive Jobs: salloc">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#using-srun-directly" class="md-nav__link">
    Using srun directly
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reservations" class="md-nav__link">
    Reservations
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#best-practices-for-job-submission" class="md-nav__link">
    Best practices for job submission
  </a>
  
    <nav class="md-nav" aria-label="Best practices for job submission">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#time-limits" class="md-nav__link">
    Time Limits
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#long-running-jobs" class="md-nav__link">
    Long Running Jobs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#io-performance" class="md-nav__link">
    I/O performance
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#large-jobs" class="md-nav__link">
    Large Jobs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#process-placement" class="md-nav__link">
    Process Placement
  </a>
  
    <nav class="md-nav" aria-label="Process Placement">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#default" class="md-nav__link">
    Default
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mpich_rank_reorder_method" class="md-nav__link">
    MPICH_RANK_REORDER_METHOD
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#huge-pages" class="md-nav__link">
    Huge pages
  </a>
  
    <nav class="md-nav" aria-label="Huge pages">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#when-to-use-huge-pages" class="md-nav__link">
    When to Use Huge Pages
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#when-to-avoid-huge-pages" class="md-nav__link">
    When to Avoid Huge Pages
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../io/" class="md-nav__link">
      I/O and file systems
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../dev-environment/" class="md-nav__link">
      Application development environment
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../containers/" class="md-nav__link">
      Containers
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../python/" class="md-nav__link">
      Using Python
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../analysis/" class="md-nav__link">
      Data analysis
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../debug/" class="md-nav__link">
      Debugging
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../profile/" class="md-nav__link">
      Profiling
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../tuning/" class="md-nav__link">
      Performance tuning
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4">
    
    <label class="md-nav__link" for="nav-4">
      Research Software
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="Research Software" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        <span class="md-nav__icon md-icon"></span>
        Research Software
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../research-software/" class="md-nav__link">
      Overview
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../research-software/castep/castep/" class="md-nav__link">
      CASTEP
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../research-software/chemshell/chemshell/" class="md-nav__link">
      Chemshell
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../research-software/code-saturne/code-saturne/" class="md-nav__link">
      Code_Saturne
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../research-software/cp2k/cp2k/" class="md-nav__link">
      CP2K
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../research-software/elk/elk/" class="md-nav__link">
      ELK
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../research-software/fenics/fenics/" class="md-nav__link">
      FEniCS
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../research-software/gromacs/gromacs/" class="md-nav__link">
      GROMACS
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../research-software/lammps/lammps/" class="md-nav__link">
      LAMMPS
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../research-software/mitgcm/mitgcm/" class="md-nav__link">
      MITgcm
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../research-software/mo-unified-model/mo-unified-model/" class="md-nav__link">
      Met Office Unified Model
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../research-software/namd/namd/" class="md-nav__link">
      NAMD
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../research-software/nektarplusplus/nektarplusplus/" class="md-nav__link">
      Nektar++
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../research-software/nemo/nemo/" class="md-nav__link">
      NEMO
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../research-software/nwchem/nwchem/" class="md-nav__link">
      NWChem
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../research-software/onetep/onetep/" class="md-nav__link">
      ONETEP
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../research-software/openfoam/openfoam/" class="md-nav__link">
      OpenFOAM
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../research-software/qe/qe/" class="md-nav__link">
      Quantum Espresso
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../research-software/vasp/vasp/" class="md-nav__link">
      VASP
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5">
    
    <label class="md-nav__link" for="nav-5">
      Software Libraries
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="Software Libraries" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        <span class="md-nav__icon md-icon"></span>
        Software Libraries
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../software-libraries/" class="md-nav__link">
      Overview
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../software-libraries/libsci/" class="md-nav__link">
      HPE Cray LibSci: BLAS, LAPACK, ScaLAPACK
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../software-libraries/fftw/" class="md-nav__link">
      FFTW
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../software-libraries/hdf5/" class="md-nav__link">
      HDF5
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../software-libraries/netcdf/" class="md-nav__link">
      NetCDF
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../data-tools/" class="md-nav__link">
      Data Analysis and Tools
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../essentials/" class="md-nav__link">
      Essential Skills
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#basic-slurm-commands" class="md-nav__link">
    Basic Slurm commands
  </a>
  
    <nav class="md-nav" aria-label="Basic Slurm commands">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sinfo-information-on-resources" class="md-nav__link">
    sinfo: information on resources
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbatch-submitting-jobs" class="md-nav__link">
    sbatch: submitting jobs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#squeue-monitoring-jobs" class="md-nav__link">
    squeue: monitoring jobs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scancel-deleting-jobs" class="md-nav__link">
    scancel: deleting jobs
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resource-limits" class="md-nav__link">
    Resource Limits
  </a>
  
    <nav class="md-nav" aria-label="Resource Limits">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#primary-resource" class="md-nav__link">
    Primary resource
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#partitions" class="md-nav__link">
    Partitions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quality-of-service-qos" class="md-nav__link">
    Quality of Service (QoS)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#troubleshooting" class="md-nav__link">
    Troubleshooting
  </a>
  
    <nav class="md-nav" aria-label="Troubleshooting">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slurm-error-messages" class="md-nav__link">
    Slurm error messages
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slurm-queued-reasons" class="md-nav__link">
    Slurm queued reasons
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#output-from-slurm-jobs" class="md-nav__link">
    Output from Slurm jobs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#specifying-resources-in-job-scripts" class="md-nav__link">
    Specifying resources in job scripts
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#using-modules-in-the-batch-system-the-epcc-job-env-module" class="md-nav__link">
    Using modules in the batch system: the epcc-job-env module
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#srun-launching-parallel-jobs" class="md-nav__link">
    srun: Launching parallel jobs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bolt-job-submission-script-creation-tool" class="md-nav__link">
    bolt: Job submission script creation tool
  </a>
  
    <nav class="md-nav" aria-label="bolt: Job submission script creation tool">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#basic-usage" class="md-nav__link">
    Basic Usage
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#further-help" class="md-nav__link">
    Further help
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#checkscript-job-submission-script-validation-tool" class="md-nav__link">
    checkScript job submission script validation tool
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#example-job-submission-scripts" class="md-nav__link">
    Example job submission scripts
  </a>
  
    <nav class="md-nav" aria-label="Example job submission scripts">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-job-submission-script-for-mpi-parallel-job" class="md-nav__link">
    Example: job submission script for MPI parallel job
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-job-submission-script-for-mpiopenmp-mixed-mode-parallel-job" class="md-nav__link">
    Example: job submission script for MPI+OpenMP (mixed mode) parallel job
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#job-arrays" class="md-nav__link">
    Job arrays
  </a>
  
    <nav class="md-nav" aria-label="Job arrays">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#job-script-for-a-job-array" class="md-nav__link">
    Job script for a job array
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#submitting-a-job-array" class="md-nav__link">
    Submitting a job array
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#job-chaining" class="md-nav__link">
    Job chaining
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#interactive-jobs-salloc" class="md-nav__link">
    Interactive Jobs: salloc
  </a>
  
    <nav class="md-nav" aria-label="Interactive Jobs: salloc">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#using-srun-directly" class="md-nav__link">
    Using srun directly
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reservations" class="md-nav__link">
    Reservations
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#best-practices-for-job-submission" class="md-nav__link">
    Best practices for job submission
  </a>
  
    <nav class="md-nav" aria-label="Best practices for job submission">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#time-limits" class="md-nav__link">
    Time Limits
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#long-running-jobs" class="md-nav__link">
    Long Running Jobs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#io-performance" class="md-nav__link">
    I/O performance
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#large-jobs" class="md-nav__link">
    Large Jobs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#process-placement" class="md-nav__link">
    Process Placement
  </a>
  
    <nav class="md-nav" aria-label="Process Placement">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#default" class="md-nav__link">
    Default
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mpich_rank_reorder_method" class="md-nav__link">
    MPICH_RANK_REORDER_METHOD
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#huge-pages" class="md-nav__link">
    Huge pages
  </a>
  
    <nav class="md-nav" aria-label="Huge pages">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#when-to-use-huge-pages" class="md-nav__link">
    When to Use Huge Pages
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#when-to-avoid-huge-pages" class="md-nav__link">
    When to Avoid Huge Pages
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/ARCHER2-HPC/archer2-docs/edit/master/docs/user-guide/scheduler.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                <h1 id="running-jobs-on-archer2">Running jobs on ARCHER2</h1>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The ARCHER2 Service is not yet available. This documentation is in
development.</p>
</div>
<p>As with most HPC services, ARCHER2 uses a scheduler to manage access to
resources and ensure that the thousands of different users of system are
able to share the system and all get access to the resources they
require. ARCHER2 uses the Slurm software to schedule jobs.</p>
<p>Writing a submission script is typically the most convenient way to
submit your job to the scheduler. Example submission scripts (with
explanations) for the most common job types are provided below.</p>
<p>Interactive jobs are also available and can be particularly useful for
developing and debugging applications. More details are available below.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>If you have any questions on how to run jobs on ARCHER2 do not hesitate
to contact the <a href="mailto:support@archer2.ac.uk">ARCHER2 Service Desk</a>.</p>
</div>
<p>You typically interact with Slurm by issuing Slurm commands from the
login nodes (to submit, check and cancel jobs), and by specifying Slurm
directives that describe the resources required for your jobs in job
submission scripts.</p>
<h2 id="basic-slurm-commands">Basic Slurm commands</h2>
<p>There are three key commands used to interact with the Slurm on the
command line:</p>
<ul>
<li><code>sinfo</code> - Get information on the partitions and resources available</li>
<li><code>sbatch jobscript.slurm</code> - Submit a job submission script (in this
    case called: <code>jobscript.slurm</code>) to the scheduler</li>
<li><code>squeue</code> - Get the current status of jobs submitted to the scheduler</li>
<li><code>scancel 12345</code> - Cancel a job (in this case with the job ID
    <code>12345</code>)</li>
</ul>
<p>We cover each of these commands in more detail below.</p>
<h3 id="sinfo-information-on-resources"><code>sinfo</code>: information on resources</h3>
<p><code>sinfo</code> is used to query information about available resources and
partitions. Without any options, <code>sinfo</code> lists the status of all
resources and partitions, e.g.</p>
<pre><code>sinfo

PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST 
standard     up 1-00:00:00    105  down* nid[001006,...,002014]
standard     up 1-00:00:00     12  drain nid[001016,...,001969]
standard     up 1-00:00:00      5   resv nid[001000,001002-001004,001114] 
standard     up 1-00:00:00    683  alloc nid[001001,...,001970-001991] 
standard     up 1-00:00:00    214   idle nid[001022-001023,...,002015-002023]
standard     up 1-00:00:00      2   down nid[001021,001050]
</code></pre>
<p>Here we see the number of nodes in different states. For example, 683
nodes are allocated (running jobs), and 214 are idle (available to run
jobs). !!! note that long lists of node IDs have been abbreviated with
<code>...</code>.</p>
<h3 id="sbatch-submitting-jobs"><code>sbatch</code>: submitting jobs</h3>
<p><code>sbatch</code> is used to submit a job script to the job submission system.
The script will typically contain one or more <code>srun</code> commands to launch
parallel tasks.</p>
<p>When you submit the job, the scheduler provides the job ID, which is
used to identify this job in other Slurm commands and when looking at
resource usage in SAFE.</p>
<pre><code>sbatch test-job.slurm
Submitted batch job 12345
</code></pre>
<h3 id="squeue-monitoring-jobs"><code>squeue</code>: monitoring jobs</h3>
<p><code>squeue</code> without any options or arguments shows the current status of
all jobs known to the scheduler. For example:</p>
<pre><code>squeue
</code></pre>
<p>will list all jobs on ARCHER2.</p>
<p>The output of this is often overwhelmingly large. You can restrict the
output to just your jobs by adding the <code>-u $USER</code> option:</p>
<pre><code>squeue -u $USER
</code></pre>
<h3 id="scancel-deleting-jobs"><code>scancel</code>: deleting jobs</h3>
<p><code>scancel</code> is used to delete a jobs from the scheduler. If the job is
waiting to run it is simply cancelled, if it is a running job then it is
stopped immediately. You need to provide the job ID of the job you wish
to cancel/stop. For example:</p>
<pre><code>scancel 12345
</code></pre>
<p>will cancel (if waiting) or stop (if running) the job with ID <code>12345</code>.</p>
<h2 id="resource-limits">Resource Limits</h2>
<p>The ARCHER2 resource limits for any given job are covered by three
separate attributes.</p>
<ul>
<li>The amount of <em>primary resource</em> you require, i.e., number of
    compute nodes.</li>
<li>The <em>partition</em> that you want to use - this specifies the nodes that
    are eligible to run your job.</li>
<li>The <em>Quality of Service (QoS)</em> that you want to use - this specifies
    the job limits that apply.</li>
</ul>
<h3 id="primary-resource">Primary resource</h3>
<p>The <em>primary resource</em> you can request for your job is the compute node.</p>
<div class="admonition information">
<p class="admonition-title">Information</p>
<p>The <code>--exclusive</code> option is enforced on ARCHER2 which means you will
always have access to all of the memory on the compute node regardless
of how many processes are actually running on the node.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You will not generally have access to the full amount of memory resource
on the the node as some is retained for running the operating system and
other system processes.</p>
</div>
<h3 id="partitions">Partitions</h3>
<p>On ARCHER2, compute nodes are grouped into partitions. You will have to
specify a partition using the <code>--partition</code> option in your Slurm
submission script. The following table has a list of active partitions
on ARCHER2.</p>
<table>
<thead>
<tr>
<th>Partition</th>
<th>Description</th>
<th>Max nodes available</th>
</tr>
</thead>
<tbody>
<tr>
<td>standard</td>
<td>CPU nodes with AMD EPYC 7742 64-core processor &times; 2</td>
<td>1024</td>
</tr>
</tbody>
</table>
<p>ARCHER2 Partitions</p>
<p>You can list the active partitions by running <code>sinfo</code>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>You may not have access to all the available partitions.</p>
</div>
<h3 id="quality-of-service-qos">Quality of Service (QoS)</h3>
<p>On ARCHER2, job limits are defined by the requested Quality of Service
(QoS), as specified by the <code>--qos</code> Slurm directive. The following table
lists the active QoS on
ARCHER2.</p>
<table>
<thead>
<tr>
<th>QoS</th>
<th>Max Nodes Per Job</th>
<th>Max Walltime</th>
<th>Jobs Queued</th>
<th>Jobs Running</th>
<th>Partition(s)</th>
</tr>
</thead>
<tbody>
<tr>
<td>standard</td>
<td>940</td>
<td>24 hrs</td>
<td>64</td>
<td>16</td>
<td>standard</td>
</tr>
<tr>
<td>short</td>
<td>8</td>
<td>20 mins</td>
<td>2</td>
<td>2</td>
<td>short</td>
</tr>
<tr>
<td>long</td>
<td>16</td>
<td>48 hrs</td>
<td>16</td>
<td>16</td>
<td>standard</td>
</tr>
</tbody>
</table>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you want to use the <code>short</code> QoS then you also need to add the
<code>--reservation=shortqos</code> to your job submission command.</p>
</div>
<p>Please note, there are two other limits not covered by the above table.</p>
<ul>
<li>The short QoS has restricted hours of service, 08:00-20:00 Mon-Fri.</li>
<li>Long jobs must have a minimum walltime of 24 hrs.</li>
</ul>
<p>You can find out the QoS that you can use by running the following
command:</p>
<pre><code>sacctmgr show assoc user=$USER cluster=archer2-es format=cluster,account,user,qos%50
</code></pre>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>If you have needs which do not fit within the current QoS, please
<a href="https://www.archer2.ac.uk/support-access/servicedesk.html">contact the Service
Desk</a> and we
can discuss how to accommodate your requirements.</p>
</div>
<h2 id="troubleshooting">Troubleshooting</h2>
<h3 id="slurm-error-messages">Slurm error messages</h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>More information on common error messages will be added when the ARCHER2
system is available.</p>
</div>
<h3 id="slurm-queued-reasons">Slurm queued reasons</h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Explanations of the reasons for jobs being queued and not running will
be added when the ARCHER2 system is available.</p>
</div>
<h2 id="output-from-slurm-jobs">Output from Slurm jobs</h2>
<p>Slurm places standard output (STDOUT) and standard error (STDERR) for
each job in the file <code>slurm_&lt;JobID&gt;.out</code>. This file appears in the job's
working directory once your job starts running.</p>
<h2 id="specifying-resources-in-job-scripts">Specifying resources in job scripts</h2>
<p>You specify the resources you require for your job using directives at
the top of your job submission script using lines that start with the
directive <code>#SBATCH</code>.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>Options provided using <code>#SBATCH</code> directives can also be specified as
command line options to <code>srun</code>.</p>
</div>
<p>If you do not specify any options, then the default for each option will
be applied. As a minimum, all job submissions must specify the budget
that they wish to charge the job too with the option:</p>
<ul>
<li><code>--account=&lt;budgetID&gt;</code> your budget ID is usually something like
     <code>t01</code> or <code>t01-test</code>. You can see which budget codes you can charge
     to in SAFE.</li>
</ul>
<p>Other common options that are used are:</p>
<ul>
<li><code>--time=&lt;hh:mm:ss&gt;</code> the maximum walltime for your job. <em>e.g.</em> For
     a 6.5 hour walltime, you would use <code>--time=6:30:0</code>.</li>
<li><code>--job-name=&lt;jobname&gt;</code> set a name for the job to help identify it
     in</li>
</ul>
<p>In addition, parallel jobs will also need to specify how many nodes,
parallel processes and threads they require.</p>
<ul>
<li><code>--nodes=&lt;nodes&gt;</code> the number of nodes to use for the job.</li>
<li><code>--tasks-per-node=&lt;processes per node&gt;</code> the number of parallel
     processes (e.g. MPI ranks) per node.</li>
<li><code>--cpus-per-task=1</code> if you are using parallel processes only with
     no threading then you should set the number of CPUs (cores) per
     parallel process to 1. <strong>!!! note:</strong> if you are using threading (e.g.
     with OpenMP) then you will need to change this option as described
     below.</li>
</ul>
<p>For parallel jobs that use threading (e.g. OpenMP), you will also need
to change the <code>--cpus-per-task</code> option.</p>
<ul>
<li><code>--cpus-per-task=&lt;threads per task&gt;</code> the number of threads per
     parallel process (e.g. number of OpenMP threads per MPI task for
     hybrid MPI/OpenMP jobs). <strong>!!! note:</strong> you must also set the
     <code>OMP_NUM_THREADS</code> environment variable if using OpenMP in your
     job.</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For parallel jobs, ARCHER2 operates in a <em>node exclusive</em> way. This
means that you are assigned resources in the units of full compute nodes
for your jobs (<em>i.e.</em> 128 cores) and that no other user can share those
compute nodes with you. Hence, the minimum amount of resource you can
request for a parallel job is 1 node (or 128 cores).</p>
</div>
<p>To prevent the behaviour of batch scripts being dependent on the user
environment at the point of submission, the option</p>
<ul>
<li><code>--export=none</code> prevents the user environment from being exported
     to the batch system.</li>
</ul>
<p>Using the <code>--export=none</code> means that the behaviour of batch submissions
should be repeatable. We strongly recommend its use.</p>
<h2 id="using-modules-in-the-batch-system-the-epcc-job-env-module">Using modules in the batch system: the <code>epcc-job-env</code> module</h2>
<p>Batch jobs must be submitted in the work file system <code>/work</code> as the
compute nodes do not have access to the <code>/home</code> file system. This has a
knock-on effect on the behaviour of module collections, which the module
system expects to find in a user's home directory. In order that the
module system work correctly, batch scripts should contain</p>
<pre><code>module load epcc-job-env
</code></pre>
<p><strong>as the first module command in the script</strong> to ensure that the
environment is set correctly for the job. This will also ensure all
relevant library paths are set correctly at run time.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><code>module -s</code> can be used to suppress the associated messages if desired.</p>
</div>
<h2 id="srun-launching-parallel-jobs"><code>srun</code>: Launching parallel jobs</h2>
<p>If you are running parallel jobs, your job submission script should
contain one or more <code>srun</code> commands to launch the parallel executable
across the compute nodes.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>To ensure that processes and threads are correctly mapped (or <em>pinned</em>)
to cores, you should always specify
<code>--cpu-bind=cores</code> option to <code>srun</code>.</p>
</div>
<h2 id="bolt-job-submission-script-creation-tool">bolt: Job submission script creation tool</h2>
<p>The bolt job submission script creation tool has been written by EPCC to
simplify the process of writing job submission scripts for modern
multicore architectures. Based on the options you supply, bolt will
generate a job submission script that uses ARCHER2 in a reasonable way.</p>
<p>MPI, OpenMP and hybrid MPI/OpenMP jobs are supported.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The tool will allow you to generate scripts for jobs that use the <code>long</code>
QoS but you will need to manually modify the resulting script to change
the QoS to <code>long</code>.</p>
</div>
<p>If there are problems or errors in your job parameter specifications
then bolt will print warnings or errors. However, bolt cannot detect all
problems.</p>
<h3 id="basic-usage">Basic Usage</h3>
<p>The basic syntax for using bolt is:</p>
<pre><code>bolt -n [parallel tasks] -N [parallel tasks per node] -d [number of threads per task] \
     -t [wallclock time (h:m:s)] -o [script name] -j [job name] -A [project code]  [arguments...]
</code></pre>
<p>Example 1: to generate a job script to run an executable called
<code>my_prog.x</code> for 24 hours using 8192 parallel (MPI) processes and 128
(MPI) processes per compute node you would use something like:</p>
<pre><code>bolt -n 8192 -N 128 -t 24:0:0 -o my_job.bolt -j my_job -A z01-budget my_prog.x arg1 arg2
</code></pre>
<p>(remember to substitute <code>z01-budget</code> for your actual budget code.)</p>
<p>Example 2: to generate a job script to run an executable called
<code>my_prog.x</code> for 3 hours using 2048 parallel (MPI) processes and 64 (MPI)
processes per compute node (i.e. using half of the cores on a compute
node), you would use:</p>
<pre><code>bolt -n 2048 -N 64 -t 3:0:0 -o my_job.bolt -j my_job -A z01-budget my_prog.x arg1 arg2
</code></pre>
<p>These examples generate the job script <code>my_job.bolt</code> with the correct
options to run <code>my_prog.x</code> with command line arguments <code>arg1</code> and
<code>arg2</code>. The project code against which the job will be charged is
specified with the ' -A ' option. As usual, the job script is submitted
as follows:</p>
<pre><code>sbatch my_job.bolt
</code></pre>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>If you do not specify the script name with the '-o' option then your
script will be a file called <code>a.bolt</code>.</p>
</div>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>If you do not specify the number of parallel tasks then bolt will try to
generate a serial job submission script (and throw an error on the
ARCHER2 4 cabinet system as serial jobs are not supported).</p>
</div>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>If you do not specify a project code, bolt will use your default project
code (set by your login account).</p>
</div>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>If you do not specify a job name, bolt will use either <code>bolt_ser_job</code>
(for serial jobs) or <code>bolt_par_job</code> (for parallel jobs).</p>
</div>
<h3 id="further-help">Further help</h3>
<p>You can access further help on using bolt on ARCHER2 with the ' -h '
option:</p>
<pre><code>bolt -h
</code></pre>
<p>A selection of other useful options are:</p>
<ul>
<li><code>-s</code> Write and submit the job script rather than just writing the
     job script.</li>
<li><code>-p</code> Force the job to be parallel even if it only uses a single
     parallel task.</li>
</ul>
<h2 id="checkscript-job-submission-script-validation-tool">checkScript job submission script validation tool</h2>
<p>The checkScript tool has been written to allow users to validate their
job submission scripts before submitting their jobs. The tool will read
your job submission script and try to identify errors, problems or
inconsistencies.</p>
<p>An example of the sort of output the tool can give would be:</p>
<pre><code>auser@uan01:/work/t01/t01/auser&gt; checkScript submit.slurm

===========================================================================
checkScript
---------------------------------------------------------------------------
Copyright 2011-2020  EPCC, The University of Edinburgh
This program comes with ABSOLUTELY NO WARRANTY.
This is free software, and you are welcome to redistribute it
under certain conditions.
===========================================================================

Script details
---------------
       User: auser
Script file: submit.slurm
  Directory: /work/t01/t01/auser (ok)
   Job name: test (ok)
  Partition: standard (ok)
        QoS: standard (ok)
Combination:          (ok)

Requested resources
-------------------
         nodes =              3                     (ok)
tasks per node =             16
 cpus per task =              8
cores per node =            128                     (ok)
OpenMP defined =           True                     (ok)
      walltime =          1:0:0                     (ok)

CU Usage Estimate (if full job time used)
------------------------------------------
                      CU =          3.000



checkScript finished: 0 warning(s) and 0 error(s).
</code></pre>
<h2 id="example-job-submission-scripts">Example job submission scripts</h2>
<p>A subset of example job submission scripts are included in full below.
You can also download these examples at:</p>
<h3 id="example-job-submission-script-for-mpi-parallel-job">Example: job submission script for MPI parallel job</h3>
<p>A simple MPI job submission script to submit a job using 4 compute nodes
and 128 MPI ranks per node for 20 minutes would look like:</p>
<pre><code>#!/bin/bash

# Slurm job options (job-name, compute nodes, job time)
#SBATCH --job-name=Example_MPI_Job
#SBATCH --time=0:20:0
#SBATCH --nodes=4
#SBATCH --tasks-per-node=128
#SBATCH --cpus-per-task=1

# Replace [budget code] below with your budget code (e.g. t01)
#SBATCH --account=[budget code]             
#SBATCH --partition=standard
#SBATCH --qos=standard

# Setup the job environment (this module needs to be loaded before any other modules)
module load epcc-job-env

# Set the number of threads to 1
#   This prevents any threaded system libraries from automatically 
#   using threading.
export OMP_NUM_THREADS=1

# Launch the parallel job
#   Using 512 MPI processes and 128 MPI processes per node
#   srun picks up the distribution from the sbatch options

srun --cpu-bind=cores ./my_mpi_executable.x
</code></pre>
<p>This will run your executable "my_mpi_executable.x" in parallel on 512
MPI processes using 4 nodes (128 cores per node, i.e. not using
hyper-threading). Slurm will allocate 4 nodes to your job and srun will
place 128 MPI processes on each node (one per physical core).</p>
<p>See above for a more detailed discussion of the different <code>sbatch</code>
options</p>
<h3 id="example-job-submission-script-for-mpiopenmp-mixed-mode-parallel-job">Example: job submission script for MPI+OpenMP (mixed mode) parallel job</h3>
<p>Mixed mode codes that use both MPI (or another distributed memory
parallel model) and OpenMP should take care to ensure that the shared
memory portion of the process/thread placement does not span more than
one NUMA region. Nodes on ARCHER2 are made up of two sockets each
containing 4 NUMA regions of 16 cores, i.e. there are 8 NUMA regions in
total. Therefore the total number of threads should ideally not be
greater than 16, and also needs to be a factor of 16. Sensible choices
for the number of threads are therefore 1 (single-threaded), 2, 4, 8,
and 16. More information about using OpenMP and MPI+OpenMP can be found
in the Tuning chapter.</p>
<p>To ensure correct placement of MPI processes the number of cpus-per-task
needs to match the number of OpenMP threads, and the number of
tasks-per-node should be set to ensure the entire node is filled with
MPI tasks.</p>
<p>In the example below, we are using 4 nodes for 6 hours. There are 32 MPI
processes in total (8 MPI processes per node) and 16 OpenMP threads per
MPI process. This results in all 128 physical cores per node being used.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>Note the use of the <code>export OMP_PLACES=cores</code> environment option and the
<code>--hint=nomultithread</code> and <code>--distribution=block:block</code> options to
<code>srun</code> to generate the correct pinning.</p>
</div>
<div class="highlight"><pre><span></span><code>#!/bin/bash

# Slurm job options (job-name, compute nodes, job time)
#SBATCH --job-name=Example_MPI_Job
#SBATCH --time=0:20:0
#SBATCH --nodes=4
#SBATCH --ntasks=32
#SBATCH --tasks-per-node=8
#SBATCH --cpus-per-task=16

# Replace [budget code] below with your project code (e.g. t01)
#SBATCH --account=[budget code] 
#SBATCH --partition=standard
#SBATCH --qos=standard

# Setup the job environment (this module needs to be loaded before any other modules)
module load epcc-job-env

# Set the number of threads to 16 and specify placement
#   There are 16 OpenMP threads per MPI process
#   We want one thread per physical core
export OMP_NUM_THREADS=16
export OMP_PLACES=cores

# Launch the parallel job
#   Using 32 MPI processes
#   8 MPI processes per node
#   16 OpenMP threads per MPI process
#   Additional srun options to pin one thread per physical core
srun --hint=nomultithread --distribution=block:block ./my_mixed_executable.x arg1 arg2
</code></pre></div>
<h2 id="job-arrays">Job arrays</h2>
<p>The Slurm job scheduling system offers the <em>job array</em> concept, for
running collections of almost-identical jobs. For example, running the
same program several times with different arguments or input data.</p>
<p>Each job in a job array is called a <em>subjob</em>. The subjobs of a job array
can be submitted and queried as a unit, making it easier and cleaner to
handle the full set, compared to individual jobs.</p>
<p>All subjobs in a job array are started by running the same job script.
The job script also contains information on the number of jobs to be
started, and Slurm provides a subjob index which can be passed to the
individual subjobs or used to select the input data per subjob.</p>
<h3 id="job-script-for-a-job-array">Job script for a job array</h3>
<p>As an example, the following script runs 56 subjobs, with the subjob
index as the only argument to the executable. Each subjob requests a
single node and uses all 128 cores on the node by placing 1 MPI process
per core and specifies 4 hours maximum runtime per subjob:</p>
<pre><code>#!/bin/bash
# Slurm job options (job-name, compute nodes, job time)
#SBATCH --job-name=Example_Array_Job
#SBATCH --time=04:00:00
#SBATCH --nodes=1
#SBATCH --tasks-per-node=128
#SBATCH --cpus-per-task=1
#SBATCH --array=0-55

# Replace [budget code] below with your budget code (e.g. t01)
#SBATCH --account=[budget code]  
#SBATCH --partition=standard
#SBATCH --qos=standard

# Setup the job environment (this module needs to be loaded before any other modules)
module load epcc-job-env

# Set the number of threads to 1
#   This prevents any threaded system libraries from automatically 
#   using threading.
export OMP_NUM_THREADS=1

srun --cpu-bind=cores /path/to/exe $SLURM_ARRAY_TASK_ID
</code></pre>
<h3 id="submitting-a-job-array">Submitting a job array</h3>
<p>Job arrays are submitted using <code>sbatch</code> in the same way as for standard
jobs:</p>
<pre><code>sbatch job_script.pbs
</code></pre>
<h2 id="job-chaining">Job chaining</h2>
<p>Job dependencies can be used to construct complex pipelines or chain
together long simulations requiring multiple steps.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>The <code>--parsable</code> option to <code>sbatch</code> can simplify working with job
dependencies. It returns the job ID in a format that can be used as the
input to other commands.</p>
</div>
<p>For example:</p>
<pre><code>jobid=$(sbatch --parsable first_job.sh)
sbatch --dependency=afterok:$jobid second_job.sh
</code></pre>
<p>or for a longer chain:</p>
<pre><code>jobid1=$(sbatch --parsable first_job.sh)
jobid2=$(sbatch --parsable --dependency=afterok:$jobid1 second_job.sh)
jobid3=$(sbatch --parsable --dependency=afterok:$jobid1 third_job.sh)
sbatch --dependency=afterok:$jobid2,afterok:$jobid3 last_job.sh
</code></pre>
<h2 id="interactive-jobs-salloc">Interactive Jobs: <code>salloc</code></h2>
<p>When you are developing or debugging code you often want to run many
short jobs with a small amount of editing the code between runs. This
can be achieved by using the login nodes to run MPI but you may want to
test on the compute nodes (e.g. you may want to test running on multiple
nodes across the high performance interconnect). One of the best ways to
achieve this on ARCHER2 is to use interactive jobs.</p>
<p>An interactive job allows you to issue <code>srun</code> commands directly from the
command line without using a job submission script, and to see the
output from your program directly in the terminal.</p>
<p>You use the <code>salloc</code> command to reserve compute nodes for interactive
jobs.</p>
<p>To submit a request for an interactive job reserving 8 nodes (1024
physical cores) for 1 hour you would issue the following qsub command
from the command line:</p>
<pre><code>auser@uan01:&gt; salloc --nodes=8 --tasks-per-node=128 --cpus-per-task=1 \
              --time=01:00:00 --partition=standard --qos=standard \
              --account=[budget code]
</code></pre>
<p>When you submit this job your terminal will display something like:</p>
<pre><code>salloc: Granted job allocation 24236
salloc: Waiting for resource configuration
salloc: Nodes nid000002 are ready for job
auser@uan01:&gt;
</code></pre>
<p>It may take some time for your interactive job to start. Once it runs
you will enter a standard interactive terminal session (a new shell).
Note that this shell is still on the front end (the prompt has not
change). Whilst the interactive session lasts you will be able to run
parallel jobs on the compute nodes by issuing the <code>srun
--cpu-bind=cores</code> command directly at your command prompt using the same
syntax as you would inside a job script. The maximum number of nodes you
can use is limited by resources requested in the <code>salloc</code> command.</p>
<p>If you know you will be doing a lot of intensive debugging you may find
it useful to request an interactive session lasting the expected length
of your working session, say a full day.</p>
<p>Your session will end when you hit the requested walltime. If you wish
to finish before this you should use the <code>exit</code> command - this will
return you to your prompt before you issued the <code>salloc</code> command.</p>
<h3 id="using-srun-directly">Using <code>srun</code> directly</h3>
<p>A second way to run an interactive job is to use <code>srun</code> directly in the
following way:</p>
<pre><code>auser@uan01:/work/t01/t01/auser&gt; srun --nodes=1 --exclusive --time=00:20:00 --account=[] \
               --partition=standard --qos=standard --pty /bin/bash
auser@uan01:/work/t01/t01/auser&gt; hostname
nid001261
</code></pre>
<p>The <code>--pty /bin/bash</code> will cause a new shell to be started on the first
node of a new allocation (note that while the shell prompt has not
changed, we are now on the compute node). This is perhaps closer to what
many people consider an 'interactive' job than the method using <code>salloc</code>
appears.</p>
<p>One can now issue shell commands in the usual way. A further invocation
of <code>srun</code> is required to launch a parallel job in the allocation.</p>
<p>When finished, type <code>exit</code> to relinquish the allocation and control will
be returned to the front end.</p>
<p>By default, the interactive shell will retain the environment of the
parent. If you want a clean shell, remember to specify <code>--export=none</code>.</p>
<h2 id="reservations">Reservations</h2>
<p>The mechanism for submitting reservations on ARCHER2 has yet to be
specified.</p>
<h2 id="best-practices-for-job-submission">Best practices for job submission</h2>
<p>This guidance is adapted from <a href="https://docs.nersc.gov/jobs/best-practices/">the advice provided by
NERSC</a></p>
<h3 id="time-limits">Time Limits</h3>
<p>Due to backfill scheduling, short and variable-length jobs generally
start quickly resulting in much better job throughput. You can specify a
minimum time for your job with the <code>--time-min</code> option to SBATCH:</p>
<pre><code>#SBATCH --time-min=&lt;lower_bound&gt;
#SBATCH --time=&lt;upper_bound&gt;
</code></pre>
<p>Within your job script, you can get the time remaining in the job with
<code>squeue -h -j ${Slurm_JOBID} -o %L</code> to allow you to deal with
potentially varying runtimes when using this option.</p>
<h3 id="long-running-jobs">Long Running Jobs</h3>
<p>Simulations which must run for a long period of time achieve the best
throughput when composed of many small jobs using a checkpoint and
restart method chained together (see above for how to chain jobs
together). However, this method does occur a startup and shutdown
overhead for each job as the state is saved and loaded so you should
experiment to find the best balance between runtime (long runtimes
minimise the checkpoint/restart overheads) and throughput (short
runtimes maximise throughput).</p>
<h3 id="io-performance">I/O performance</h3>
<h3 id="large-jobs">Large Jobs</h3>
<p>Large jobs may take longer to start up. The <code>sbcast</code> command is
recommended for large jobs requesting over 1500 MPI tasks. By default,
Slurm reads the executable on the allocated compute nodes from the
location where it is installed; this may take long time when the file
system (where the executable resides) is slow or busy. The <code>sbcast</code>
command, the executable can be copied to the <code>/tmp</code> directory on each of
the compute nodes. Since <code>/tmp</code> is part of the memory on the compute
nodes, it can speed up the job startup time.</p>
<div class="highlight"><pre><span></span><code>sbcast --compress=lz4 /path/to/exe /tmp/exe
srun /tmp/exe
</code></pre></div>
<h3 id="process-placement">Process Placement</h3>
<p>Several mechanisms exist to control process placement on ARCHER2.
Application performance can depend strongly on placement depending on
the communication pattern and other computational characteristics.</p>
<h4 id="default">Default</h4>
<p>The default is to place MPI tasks sequentially on nodes until the
maximum number of tasks is reached:</p>
<pre><code>salloc --nodes=8 --tasks-per-node=2 --cpus-per-task=1 --time=0:10:0 \
       --account=[account code] --partition=partition code] --qos=standard

salloc: Granted job allocation 24236
salloc: Waiting for resource configuration
salloc: Nodes cn13 are ready for job

module load xthi
export OMP_NUM_THREADS=1
srun --cpu-bind=cores xthi

Hello from rank 0, thread 0, on nid000001. (core affinity = 0,128)
Hello from rank 1, thread 0, on nid000001. (core affinity = 16,144)
Hello from rank 2, thread 0, on nid000002. (core affinity = 0,128)
Hello from rank 3, thread 0, on nid000002. (core affinity = 16,144)
Hello from rank 4, thread 0, on nid000003. (core affinity = 0,128)
Hello from rank 5, thread 0, on nid000003. (core affinity = 16,144)
Hello from rank 6, thread 0, on nid000004. (core affinity = 0,128)
Hello from rank 7, thread 0, on nid000004. (core affinity = 16,144)
Hello from rank 8, thread 0, on nid000005. (core affinity = 0,128)
Hello from rank 9, thread 0, on nid000005. (core affinity = 16,144)
Hello from rank 10, thread 0, on nid000006. (core affinity = 0,128)
Hello from rank 11, thread 0, on nid000006. (core affinity = 16,144)
Hello from rank 12, thread 0, on nid000007. (core affinity = 0,128)
Hello from rank 13, thread 0, on nid000007. (core affinity = 16,144)
Hello from rank 14, thread 0, on nid000008. (core affinity = 0,128)
Hello from rank 15, thread 0, on nid000008. (core affinity = 16,144)
</code></pre>
<h4 id="mpich_rank_reorder_method"><code>MPICH_RANK_REORDER_METHOD</code></h4>
<p>The <code>MPICH_RANK_REORDER_METHOD</code> environment variable is used to specify
other types of MPI task placement. For example, setting it to 0 results
in a round-robin placement:</p>
<pre><code>salloc --nodes=8 --tasks-per-node=2 --cpus-per-task=1 --time=0:10:0 --account=t01

salloc: Granted job allocation 24236
salloc: Waiting for resource configuration
salloc: Nodes cn13 are ready for job

module load xthi
export OMP_NUM_THREADS=1
export MPICH_RANK_REORDER_METHOD=0
srun --cpu-bind=cores xthi

Hello from rank 0, thread 0, on nid000001. (core affinity = 0,128)
Hello from rank 1, thread 0, on nid000002. (core affinity = 0,128)
Hello from rank 2, thread 0, on nid000003. (core affinity = 0,128)
Hello from rank 3, thread 0, on nid000004. (core affinity = 0,128)
Hello from rank 4, thread 0, on nid000005. (core affinity = 0,128)
Hello from rank 5, thread 0, on nid000006. (core affinity = 0,128)
Hello from rank 6, thread 0, on nid000007. (core affinity = 0,128)
Hello from rank 7, thread 0, on nid000008. (core affinity = 0,128)
Hello from rank 8, thread 0, on nid000001. (core affinity = 16,144)
Hello from rank 9, thread 0, on nid000002. (core affinity = 16,144)
Hello from rank 10, thread 0, on nid000003. (core affinity = 16,144)
Hello from rank 11, thread 0, on nid000004. (core affinity = 16,144)
Hello from rank 12, thread 0, on nid000005. (core affinity = 16,144)
Hello from rank 13, thread 0, on nid000006. (core affinity = 16,144)
Hello from rank 14, thread 0, on nid000007. (core affinity = 16,144)
Hello from rank 15, thread 0, on nid000008. (core affinity = 16,144)
</code></pre>
<p>There are other modes available with the <code>MPICH_RANK_REORDER_METHOD</code>
environment variable, including one which lets the user provide a file
called <code>MPICH_RANK_ORDER</code> which contains a list of each task's placement
on each node. These options are described in detail in the <code>intro_mpi</code>
man page.</p>
<p><strong>grid_order</strong></p>
<p>For MPI applications which perform a large amount of nearest-neighbor
communication, e.g., stencil-based applications on structured grids,
Cray provides a tool in the <code>perftools-base</code> module called <code>grid_order</code>
which can generate a <code>MPICH_RANK_ORDER</code> file automatically by taking as
parameters the dimensions of the grid, core count, etc. For example, to
place MPI tasks in row-major order on a Cartesian grid of size $(4, 4,
4)$, using 32 tasks per node:</p>
<pre><code>module load perftools-base
grid_order -R -c 32 -g 4,4,4

# grid_order -R -Z -c 32 -g 4,4,4
# Region 3: 0,0,1 (0..63)
0,1,2,3,16,17,18,19,32,33,34,35,48,49,50,51,4,5,6,7,20,21,22,23,36,37,38,39,52,53,54,55
8,9,10,11,24,25,26,27,40,41,42,43,56,57,58,59,12,13,14,15,28,29,30,31,44,45,46,47,60,61,62,63
</code></pre>
<p>One can then save this output to a file called <code>MPICH_RANK_ORDER</code> and
then set <code>MPICH_RANK_REORDER_METHOD=3</code> before running the job, which
tells Cray MPI to read the <code>MPICH_RANK_ORDER</code> file to set the MPI task
placement. For more information, please see the man page <code>man
grid_order</code> (available when the <code>perftools-base</code> module is loaded).</p>
<h3 id="huge-pages">Huge pages</h3>
<p>Huge pages are virtual memory pages which are bigger than the default
page size of 4K bytes. Huge pages can improve memory performance for
common access patterns on large data sets since it helps to reduce the
number of virtual to physical address translations when compared to
using the default 4KB.</p>
<p>To use huge pages for an application (with the 2 MB huge pages as an
example):</p>
<pre><code>module load craype-hugepages2M
cc -o mycode.exe mycode.c
</code></pre>
<p>And also load the same huge pages module at runtime.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Due to the huge pages memory fragmentation issue, applications may get
<em>Cannot allocate memory</em> warnings or errors when there are not enough
hugepages on the compute node, such as:</p>
<p>libhugetlbfs [nid0000xx:xxxxx]: WARNING: New heap segment map at 0x10000000 failed: Cannot allocate memory``</p>
</div>
<p>By default, The verbosity level of libhugetlbfs <code>HUGETLB_VERBOSE</code> is set
to <code>0</code> on ARCHER2 to surpress debugging messages. Users can adjust this
value to obtain more information on huge pages use.</p>
<h4 id="when-to-use-huge-pages">When to Use Huge Pages</h4>
<ul>
<li>For MPI applications, map the static data and/or heap onto huge
    pages.</li>
<li>For an application which uses shared memory, which needs to be
    concurrently registered with the high speed network drivers for
    remote communication.</li>
<li>For SHMEM applications, map the static data and/or private heap onto
    huge pages.</li>
<li>For applications written in Unified Parallel C, Coarray Fortran, and
    other languages based on the PGAS programming model, map the static
    data and/or private heap onto huge pages.</li>
<li>For an application doing heavy I/O.</li>
<li>To improve memory performance for common access patterns on large
    data sets.</li>
</ul>
<h4 id="when-to-avoid-huge-pages">When to Avoid Huge Pages</h4>
<ul>
<li>Applications sometimes consist of many steering programs in addition
    to the core application. Applying huge page behavior to all
    processes would not provide any benefit and would consume huge pages
    that would otherwise benefit the core application. The runtime
    environment variable <code>HUGETLB_RESTRICT_EXE</code> can be used to specify
    the susbset of the programs to use hugepages.</li>
<li>For certain applications if using hugepages either causes issues or
    slows down performance. One such example is that when an application
    forks more subprocesses (such as pthreads) and these threads
    allocate memory, the newly allocated memory are the default 4 KB
    pages.</li>
</ul>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid" aria-label="Footer">
        
          <a href="../sw-environment/" class="md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
            </div>
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Software environment
              </div>
            </div>
          </a>
        
        
          <a href="../io/" class="md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                I/O and file systems
              </div>
            </div>
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
      </div>
      
  <div class="md-footer-social">
    
      
      
        
        
      
      <a href="https://github.com/ARCHER2-HPC" target="_blank" rel="noopener" title="github.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
      </a>
    
      
      
        
        
      
      <a href="https://twitter.com/ARCHER2_HPC" target="_blank" rel="noopener" title="twitter.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
      </a>
    
      
      
        
        
      
      <a href="https://www.linkedin.com/groups/13848871/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
      </a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../assets/javascripts/vendor.7e0ee788.min.js"></script>
      <script src="../../assets/javascripts/bundle.b3a72adc.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}</script>
      
      <script>
        app = initialize({
          base: "../..",
          features: ['tabs'],
          search: Object.assign({
            worker: "../../assets/javascripts/worker/search.4ac00218.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
    
  </body>
</html>